{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import torch.utils.data as Data  # to make Loader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import numpy as np \n",
    "import time\n",
    "import csv\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_data_loader,optimizer,epoch,device):\n",
    "    net.train()\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    total_train_step=0\n",
    "    for data in train_data_loader:\n",
    "        imgs,targets=data\n",
    "        imgs,targets=imgs.to(device),targets.to(device)\n",
    "        #imgs=imgs.permute(0,3,1,2)\n",
    "        #imgs=compose(imgs)\n",
    "        outputs=net(imgs)\n",
    "        optimizer.zero_grad()\n",
    "        loss=criterion(outputs,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_step+=1\n",
    "        if total_train_step %160 ==0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, total_train_step*32, len(train_data_loader)*32,\n",
    "                    100. * total_train_step*32 / (len(train_data_loader)*32), loss.item()))\n",
    "        \n",
    "def test(net,test_data_loader,epoch,device):\n",
    "    net.eval()\n",
    "    acc=0\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_data_loader):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs=inputs.to(device)\n",
    "        #inputs=compose(inputs)\n",
    "        outputs = net(inputs)\n",
    "        outputs=outputs.to(\"cpu\")\n",
    "        pred = np.argmax(outputs.data, axis=1)\n",
    "        for i in range(len(targets.data)):\n",
    "            if pred.data[i]==targets.data[i]:\n",
    "                acc+=1\n",
    "        total_acc += acc\n",
    "        total_num += batch_size\n",
    "        acc = acc / batch_size\n",
    "        if batch_idx % 64 == 0:\n",
    "            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tAcc: {:.6f}'.format(epoch, \\\n",
    "                    batch_idx*batch_size, len(test_data_loader)*batch_size,100. * float(batch_idx)*batch_size / (batch_size*len(test_data_loader)), acc))\n",
    "            \n",
    "    print ('Total test acc =', total_acc / total_num)\n",
    "    return total_acc/total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "'''The target and the full set of images have completed'''\n",
    "device = torch.device(\"cuda\")\n",
    "net=ResNet18().to(device)\n",
    "learning_rate=1e-3\n",
    "optimizer=optim.SGD(net.parameters(),lr=learning_rate,momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "# 1. Get the train data\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),transforms.RandomResizedCrop((32,32),antialias=True)])\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "train_data_loader = Data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    # num_workers=2 # ready to be commented(windows)\n",
    ")\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "test_data_loader = Data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    # num_workers=2\n",
    ")\n",
    "epochs=100\n",
    "epoch=0\n",
    "train_loss=[]\n",
    "test_acc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "Train Epoch: 1 [5120/50016 (10%)]\tLoss: 1.684928\n",
      "Train Epoch: 1 [10240/50016 (20%)]\tLoss: 1.694371\n",
      "Train Epoch: 1 [15360/50016 (31%)]\tLoss: 1.723184\n",
      "Train Epoch: 1 [20480/50016 (41%)]\tLoss: 1.786580\n",
      "Train Epoch: 1 [25600/50016 (51%)]\tLoss: 1.169143\n",
      "Train Epoch: 1 [30720/50016 (61%)]\tLoss: 1.480228\n",
      "Train Epoch: 1 [35840/50016 (72%)]\tLoss: 1.556011\n",
      "Train Epoch: 1 [40960/50016 (82%)]\tLoss: 1.291971\n",
      "Train Epoch: 1 [46080/50016 (92%)]\tLoss: 1.333292\n",
      "Test Epoch: 1 [0/10016 (0%)]\tAcc: 0.437500\n",
      "Test Epoch: 1 [2048/10016 (20%)]\tAcc: 0.456525\n",
      "Test Epoch: 1 [4096/10016 (41%)]\tAcc: 0.333449\n",
      "Test Epoch: 1 [6144/10016 (61%)]\tAcc: 0.324775\n",
      "Test Epoch: 1 [8192/10016 (82%)]\tAcc: 0.456589\n",
      "Total test acc = 0.4642827197243808\n",
      "epoch:  2\n",
      "Train Epoch: 2 [5120/50016 (10%)]\tLoss: 1.227190\n",
      "Train Epoch: 2 [10240/50016 (20%)]\tLoss: 1.398421\n",
      "Train Epoch: 2 [15360/50016 (31%)]\tLoss: 1.431101\n",
      "Train Epoch: 2 [20480/50016 (41%)]\tLoss: 1.193825\n",
      "Train Epoch: 2 [25600/50016 (51%)]\tLoss: 1.186788\n",
      "Train Epoch: 2 [30720/50016 (61%)]\tLoss: 1.571050\n",
      "Train Epoch: 2 [35840/50016 (72%)]\tLoss: 1.135710\n",
      "Train Epoch: 2 [40960/50016 (82%)]\tLoss: 1.026758\n",
      "Train Epoch: 2 [46080/50016 (92%)]\tLoss: 1.350967\n",
      "Test Epoch: 2 [0/10016 (0%)]\tAcc: 0.718750\n",
      "Test Epoch: 2 [2048/10016 (20%)]\tAcc: 0.486039\n",
      "Test Epoch: 2 [4096/10016 (41%)]\tAcc: 0.425282\n",
      "Test Epoch: 2 [6144/10016 (61%)]\tAcc: 0.481249\n",
      "Test Epoch: 2 [8192/10016 (82%)]\tAcc: 0.452622\n",
      "Total test acc = 0.5652307977936433\n",
      "epoch:  3\n",
      "Train Epoch: 3 [5120/50016 (10%)]\tLoss: 1.283630\n",
      "Train Epoch: 3 [10240/50016 (20%)]\tLoss: 1.202877\n",
      "Train Epoch: 3 [15360/50016 (31%)]\tLoss: 1.185059\n",
      "Train Epoch: 3 [20480/50016 (41%)]\tLoss: 1.437438\n",
      "Train Epoch: 3 [25600/50016 (51%)]\tLoss: 0.976150\n",
      "Train Epoch: 3 [30720/50016 (61%)]\tLoss: 1.234064\n",
      "Train Epoch: 3 [35840/50016 (72%)]\tLoss: 1.454191\n",
      "Train Epoch: 3 [40960/50016 (82%)]\tLoss: 1.519487\n",
      "Train Epoch: 3 [46080/50016 (92%)]\tLoss: 0.926086\n",
      "Test Epoch: 3 [0/10016 (0%)]\tAcc: 0.687500\n",
      "Test Epoch: 3 [2048/10016 (20%)]\tAcc: 0.549490\n",
      "Test Epoch: 3 [4096/10016 (41%)]\tAcc: 0.520129\n",
      "Test Epoch: 3 [6144/10016 (61%)]\tAcc: 0.703695\n",
      "Test Epoch: 3 [8192/10016 (82%)]\tAcc: 0.578661\n",
      "Total test acc = 0.6054981201675087\n",
      "epoch:  4\n",
      "Train Epoch: 4 [5120/50016 (10%)]\tLoss: 1.188442\n",
      "Train Epoch: 4 [10240/50016 (20%)]\tLoss: 1.260318\n",
      "Train Epoch: 4 [15360/50016 (31%)]\tLoss: 1.516560\n",
      "Train Epoch: 4 [20480/50016 (41%)]\tLoss: 1.124829\n",
      "Train Epoch: 4 [25600/50016 (51%)]\tLoss: 0.922944\n",
      "Train Epoch: 4 [30720/50016 (61%)]\tLoss: 0.935480\n",
      "Train Epoch: 4 [35840/50016 (72%)]\tLoss: 0.896829\n",
      "Train Epoch: 4 [40960/50016 (82%)]\tLoss: 1.246764\n",
      "Train Epoch: 4 [46080/50016 (92%)]\tLoss: 0.748245\n",
      "Test Epoch: 4 [0/10016 (0%)]\tAcc: 0.625000\n",
      "Test Epoch: 4 [2048/10016 (20%)]\tAcc: 0.581835\n",
      "Test Epoch: 4 [4096/10016 (41%)]\tAcc: 0.588492\n",
      "Test Epoch: 4 [6144/10016 (61%)]\tAcc: 0.456654\n",
      "Test Epoch: 4 [8192/10016 (82%)]\tAcc: 0.616898\n",
      "Total test acc = 0.6316071962926182\n",
      "epoch:  5\n",
      "Train Epoch: 5 [5120/50016 (10%)]\tLoss: 0.842623\n",
      "Train Epoch: 5 [10240/50016 (20%)]\tLoss: 0.877316\n",
      "Train Epoch: 5 [15360/50016 (31%)]\tLoss: 1.006206\n",
      "Train Epoch: 5 [20480/50016 (41%)]\tLoss: 0.989925\n",
      "Train Epoch: 5 [25600/50016 (51%)]\tLoss: 0.988645\n",
      "Train Epoch: 5 [30720/50016 (61%)]\tLoss: 1.262429\n",
      "Train Epoch: 5 [35840/50016 (72%)]\tLoss: 1.178306\n",
      "Train Epoch: 5 [40960/50016 (82%)]\tLoss: 1.311793\n",
      "Train Epoch: 5 [46080/50016 (92%)]\tLoss: 0.895020\n",
      "Test Epoch: 5 [0/10016 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 5 [2048/10016 (20%)]\tAcc: 0.551412\n",
      "Test Epoch: 5 [4096/10016 (41%)]\tAcc: 0.650108\n",
      "Test Epoch: 5 [6144/10016 (61%)]\tAcc: 0.645158\n",
      "Test Epoch: 5 [8192/10016 (82%)]\tAcc: 0.711662\n",
      "Total test acc = 0.6752621320733452\n",
      "epoch:  6\n",
      "Train Epoch: 6 [5120/50016 (10%)]\tLoss: 1.202965\n",
      "Train Epoch: 6 [10240/50016 (20%)]\tLoss: 0.950150\n",
      "Train Epoch: 6 [15360/50016 (31%)]\tLoss: 1.111992\n",
      "Train Epoch: 6 [20480/50016 (41%)]\tLoss: 1.241419\n",
      "Train Epoch: 6 [25600/50016 (51%)]\tLoss: 1.249399\n",
      "Train Epoch: 6 [30720/50016 (61%)]\tLoss: 0.726478\n",
      "Train Epoch: 6 [35840/50016 (72%)]\tLoss: 0.991207\n",
      "Train Epoch: 6 [40960/50016 (82%)]\tLoss: 0.801816\n",
      "Train Epoch: 6 [46080/50016 (92%)]\tLoss: 1.002531\n",
      "Test Epoch: 6 [0/10016 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 6 [2048/10016 (20%)]\tAcc: 0.646078\n",
      "Test Epoch: 6 [4096/10016 (41%)]\tAcc: 0.679315\n",
      "Test Epoch: 6 [6144/10016 (61%)]\tAcc: 0.610065\n",
      "Test Epoch: 6 [8192/10016 (82%)]\tAcc: 0.681298\n",
      "Total test acc = 0.6916882286266318\n",
      "epoch:  7\n",
      "Train Epoch: 7 [5120/50016 (10%)]\tLoss: 0.931406\n",
      "Train Epoch: 7 [10240/50016 (20%)]\tLoss: 0.852896\n",
      "Train Epoch: 7 [15360/50016 (31%)]\tLoss: 0.926015\n",
      "Train Epoch: 7 [20480/50016 (41%)]\tLoss: 0.873699\n",
      "Train Epoch: 7 [25600/50016 (51%)]\tLoss: 0.719626\n",
      "Train Epoch: 7 [30720/50016 (61%)]\tLoss: 0.973447\n",
      "Train Epoch: 7 [35840/50016 (72%)]\tLoss: 0.749322\n",
      "Train Epoch: 7 [40960/50016 (82%)]\tLoss: 0.859110\n",
      "Train Epoch: 7 [46080/50016 (92%)]\tLoss: 0.992848\n",
      "Test Epoch: 7 [0/10016 (0%)]\tAcc: 0.656250\n",
      "Test Epoch: 7 [2048/10016 (20%)]\tAcc: 0.705741\n",
      "Test Epoch: 7 [4096/10016 (41%)]\tAcc: 0.651115\n",
      "Test Epoch: 7 [6144/10016 (61%)]\tAcc: 0.770282\n",
      "Test Epoch: 7 [8192/10016 (82%)]\tAcc: 0.523119\n",
      "Total test acc = 0.6943619272920915\n",
      "epoch:  8\n",
      "Train Epoch: 8 [5120/50016 (10%)]\tLoss: 1.036258\n",
      "Train Epoch: 8 [10240/50016 (20%)]\tLoss: 0.710889\n",
      "Train Epoch: 8 [15360/50016 (31%)]\tLoss: 0.690753\n",
      "Train Epoch: 8 [20480/50016 (41%)]\tLoss: 0.764796\n",
      "Train Epoch: 8 [25600/50016 (51%)]\tLoss: 0.738090\n",
      "Train Epoch: 8 [30720/50016 (61%)]\tLoss: 0.724694\n",
      "Train Epoch: 8 [35840/50016 (72%)]\tLoss: 1.321508\n",
      "Train Epoch: 8 [40960/50016 (82%)]\tLoss: 0.520356\n",
      "Train Epoch: 8 [46080/50016 (92%)]\tLoss: 0.855722\n",
      "Test Epoch: 8 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 8 [2048/10016 (20%)]\tAcc: 0.709679\n",
      "Test Epoch: 8 [4096/10016 (41%)]\tAcc: 0.681330\n",
      "Test Epoch: 8 [6144/10016 (61%)]\tAcc: 0.616901\n",
      "Test Epoch: 8 [8192/10016 (82%)]\tAcc: 0.683405\n",
      "Total test acc = 0.7207911484667799\n",
      "epoch:  9\n",
      "Train Epoch: 9 [5120/50016 (10%)]\tLoss: 0.696066\n",
      "Train Epoch: 9 [10240/50016 (20%)]\tLoss: 0.934841\n",
      "Train Epoch: 9 [15360/50016 (31%)]\tLoss: 0.651537\n",
      "Train Epoch: 9 [20480/50016 (41%)]\tLoss: 1.647733\n",
      "Train Epoch: 9 [25600/50016 (51%)]\tLoss: 0.612308\n",
      "Train Epoch: 9 [30720/50016 (61%)]\tLoss: 0.954858\n",
      "Train Epoch: 9 [35840/50016 (72%)]\tLoss: 0.972162\n",
      "Train Epoch: 9 [40960/50016 (82%)]\tLoss: 0.986417\n",
      "Train Epoch: 9 [46080/50016 (92%)]\tLoss: 0.666239\n",
      "Test Epoch: 9 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 9 [2048/10016 (20%)]\tAcc: 0.705010\n",
      "Test Epoch: 9 [4096/10016 (41%)]\tAcc: 0.741968\n",
      "Test Epoch: 9 [6144/10016 (61%)]\tAcc: 0.519368\n",
      "Test Epoch: 9 [8192/10016 (82%)]\tAcc: 0.683218\n",
      "Total test acc = 0.7322396430640795\n",
      "epoch:  10\n",
      "Train Epoch: 10 [5120/50016 (10%)]\tLoss: 0.784354\n",
      "Train Epoch: 10 [10240/50016 (20%)]\tLoss: 1.193465\n",
      "Train Epoch: 10 [15360/50016 (31%)]\tLoss: 1.020196\n",
      "Train Epoch: 10 [20480/50016 (41%)]\tLoss: 1.294614\n",
      "Train Epoch: 10 [25600/50016 (51%)]\tLoss: 0.696348\n",
      "Train Epoch: 10 [30720/50016 (61%)]\tLoss: 0.900753\n",
      "Train Epoch: 10 [35840/50016 (72%)]\tLoss: 1.092925\n",
      "Train Epoch: 10 [40960/50016 (82%)]\tLoss: 0.345922\n",
      "Train Epoch: 10 [46080/50016 (92%)]\tLoss: 0.569606\n",
      "Test Epoch: 10 [0/10016 (0%)]\tAcc: 0.718750\n",
      "Test Epoch: 10 [2048/10016 (20%)]\tAcc: 0.709799\n",
      "Test Epoch: 10 [4096/10016 (41%)]\tAcc: 0.715689\n",
      "Test Epoch: 10 [6144/10016 (61%)]\tAcc: 0.679492\n",
      "Test Epoch: 10 [8192/10016 (82%)]\tAcc: 0.742913\n",
      "Total test acc = 0.731313729191341\n",
      "epoch:  11\n",
      "Train Epoch: 11 [5120/50016 (10%)]\tLoss: 0.906628\n",
      "Train Epoch: 11 [10240/50016 (20%)]\tLoss: 0.861585\n",
      "Train Epoch: 11 [15360/50016 (31%)]\tLoss: 1.169211\n",
      "Train Epoch: 11 [20480/50016 (41%)]\tLoss: 0.668955\n",
      "Train Epoch: 11 [25600/50016 (51%)]\tLoss: 0.531715\n",
      "Train Epoch: 11 [30720/50016 (61%)]\tLoss: 0.780056\n",
      "Train Epoch: 11 [35840/50016 (72%)]\tLoss: 0.769613\n",
      "Train Epoch: 11 [40960/50016 (82%)]\tLoss: 1.073542\n",
      "Train Epoch: 11 [46080/50016 (92%)]\tLoss: 0.900791\n",
      "Test Epoch: 11 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 11 [2048/10016 (20%)]\tAcc: 0.586696\n",
      "Test Epoch: 11 [4096/10016 (41%)]\tAcc: 0.684318\n",
      "Test Epoch: 11 [6144/10016 (61%)]\tAcc: 0.649314\n",
      "Test Epoch: 11 [8192/10016 (82%)]\tAcc: 0.806541\n",
      "Total test acc = 0.7425685681045394\n",
      "epoch:  12\n",
      "Train Epoch: 12 [5120/50016 (10%)]\tLoss: 0.775078\n",
      "Train Epoch: 12 [10240/50016 (20%)]\tLoss: 0.326286\n",
      "Train Epoch: 12 [15360/50016 (31%)]\tLoss: 0.726018\n",
      "Train Epoch: 12 [20480/50016 (41%)]\tLoss: 0.845098\n",
      "Train Epoch: 12 [25600/50016 (51%)]\tLoss: 0.993759\n",
      "Train Epoch: 12 [30720/50016 (61%)]\tLoss: 0.817874\n",
      "Train Epoch: 12 [35840/50016 (72%)]\tLoss: 0.747116\n",
      "Train Epoch: 12 [40960/50016 (82%)]\tLoss: 0.943610\n",
      "Train Epoch: 12 [46080/50016 (92%)]\tLoss: 0.714685\n",
      "Test Epoch: 12 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 12 [2048/10016 (20%)]\tAcc: 0.833670\n",
      "Test Epoch: 12 [4096/10016 (41%)]\tAcc: 0.714621\n",
      "Test Epoch: 12 [6144/10016 (61%)]\tAcc: 0.648277\n",
      "Test Epoch: 12 [8192/10016 (82%)]\tAcc: 0.683342\n",
      "Total test acc = 0.7415269415249097\n",
      "epoch:  13\n",
      "Train Epoch: 13 [5120/50016 (10%)]\tLoss: 0.666417\n",
      "Train Epoch: 13 [10240/50016 (20%)]\tLoss: 0.694591\n",
      "Train Epoch: 13 [15360/50016 (31%)]\tLoss: 0.654711\n",
      "Train Epoch: 13 [20480/50016 (41%)]\tLoss: 1.025933\n",
      "Train Epoch: 13 [25600/50016 (51%)]\tLoss: 0.880399\n",
      "Train Epoch: 13 [30720/50016 (61%)]\tLoss: 0.515040\n",
      "Train Epoch: 13 [35840/50016 (72%)]\tLoss: 0.686015\n",
      "Train Epoch: 13 [40960/50016 (82%)]\tLoss: 0.604613\n",
      "Train Epoch: 13 [46080/50016 (92%)]\tLoss: 0.746249\n",
      "Test Epoch: 13 [0/10016 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 13 [2048/10016 (20%)]\tAcc: 0.710713\n",
      "Test Epoch: 13 [4096/10016 (41%)]\tAcc: 0.711573\n",
      "Test Epoch: 13 [6144/10016 (61%)]\tAcc: 0.646326\n",
      "Test Epoch: 13 [8192/10016 (82%)]\tAcc: 0.589562\n",
      "Total test acc = 0.7458688767291224\n",
      "epoch:  14\n",
      "Train Epoch: 14 [5120/50016 (10%)]\tLoss: 1.319694\n",
      "Train Epoch: 14 [10240/50016 (20%)]\tLoss: 0.759301\n",
      "Train Epoch: 14 [15360/50016 (31%)]\tLoss: 0.783146\n",
      "Train Epoch: 14 [20480/50016 (41%)]\tLoss: 0.846319\n",
      "Train Epoch: 14 [25600/50016 (51%)]\tLoss: 0.740415\n",
      "Train Epoch: 14 [30720/50016 (61%)]\tLoss: 0.954426\n",
      "Train Epoch: 14 [35840/50016 (72%)]\tLoss: 1.130816\n",
      "Train Epoch: 14 [40960/50016 (82%)]\tLoss: 0.430093\n",
      "Train Epoch: 14 [46080/50016 (92%)]\tLoss: 0.686360\n",
      "Test Epoch: 14 [0/10016 (0%)]\tAcc: 0.656250\n",
      "Test Epoch: 14 [2048/10016 (20%)]\tAcc: 0.771201\n",
      "Test Epoch: 14 [4096/10016 (41%)]\tAcc: 0.678458\n",
      "Test Epoch: 14 [6144/10016 (61%)]\tAcc: 0.645346\n",
      "Test Epoch: 14 [8192/10016 (82%)]\tAcc: 0.775138\n",
      "Total test acc = 0.7576299558831266\n",
      "epoch:  15\n",
      "Train Epoch: 15 [5120/50016 (10%)]\tLoss: 0.732282\n",
      "Train Epoch: 15 [10240/50016 (20%)]\tLoss: 0.673262\n",
      "Train Epoch: 15 [15360/50016 (31%)]\tLoss: 0.724376\n",
      "Train Epoch: 15 [20480/50016 (41%)]\tLoss: 1.055489\n",
      "Train Epoch: 15 [25600/50016 (51%)]\tLoss: 0.711302\n",
      "Train Epoch: 15 [30720/50016 (61%)]\tLoss: 0.658463\n",
      "Train Epoch: 15 [35840/50016 (72%)]\tLoss: 0.745631\n",
      "Train Epoch: 15 [40960/50016 (82%)]\tLoss: 0.411965\n",
      "Train Epoch: 15 [46080/50016 (92%)]\tLoss: 0.456709\n",
      "Test Epoch: 15 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 15 [2048/10016 (20%)]\tAcc: 0.707786\n",
      "Test Epoch: 15 [4096/10016 (41%)]\tAcc: 0.588708\n",
      "Test Epoch: 15 [6144/10016 (61%)]\tAcc: 0.834710\n",
      "Test Epoch: 15 [8192/10016 (82%)]\tAcc: 0.588674\n",
      "Total test acc = 0.7649620155801976\n",
      "epoch:  16\n",
      "Train Epoch: 16 [5120/50016 (10%)]\tLoss: 0.545411\n",
      "Train Epoch: 16 [10240/50016 (20%)]\tLoss: 0.717025\n",
      "Train Epoch: 16 [15360/50016 (31%)]\tLoss: 0.557078\n",
      "Train Epoch: 16 [20480/50016 (41%)]\tLoss: 0.573445\n",
      "Train Epoch: 16 [25600/50016 (51%)]\tLoss: 0.451970\n",
      "Train Epoch: 16 [30720/50016 (61%)]\tLoss: 0.704824\n",
      "Train Epoch: 16 [35840/50016 (72%)]\tLoss: 0.590850\n",
      "Train Epoch: 16 [40960/50016 (82%)]\tLoss: 0.820804\n",
      "Train Epoch: 16 [46080/50016 (92%)]\tLoss: 0.668263\n",
      "Test Epoch: 16 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 16 [2048/10016 (20%)]\tAcc: 0.767480\n",
      "Test Epoch: 16 [4096/10016 (41%)]\tAcc: 0.743828\n",
      "Test Epoch: 16 [6144/10016 (61%)]\tAcc: 0.644278\n",
      "Test Epoch: 16 [8192/10016 (82%)]\tAcc: 0.683430\n",
      "Total test acc = 0.7536136344059166\n",
      "epoch:  17\n",
      "Train Epoch: 17 [5120/50016 (10%)]\tLoss: 0.762369\n",
      "Train Epoch: 17 [10240/50016 (20%)]\tLoss: 0.439171\n",
      "Train Epoch: 17 [15360/50016 (31%)]\tLoss: 0.863756\n",
      "Train Epoch: 17 [20480/50016 (41%)]\tLoss: 0.544908\n",
      "Train Epoch: 17 [25600/50016 (51%)]\tLoss: 0.751896\n",
      "Train Epoch: 17 [30720/50016 (61%)]\tLoss: 0.821443\n",
      "Train Epoch: 17 [35840/50016 (72%)]\tLoss: 0.864653\n",
      "Train Epoch: 17 [40960/50016 (82%)]\tLoss: 0.727215\n",
      "Train Epoch: 17 [46080/50016 (92%)]\tLoss: 0.506059\n",
      "Test Epoch: 17 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 17 [2048/10016 (20%)]\tAcc: 0.617978\n",
      "Test Epoch: 17 [4096/10016 (41%)]\tAcc: 0.745872\n",
      "Test Epoch: 17 [6144/10016 (61%)]\tAcc: 0.707846\n",
      "Test Epoch: 17 [8192/10016 (82%)]\tAcc: 0.838676\n",
      "Total test acc = 0.7677527506433526\n",
      "epoch:  18\n",
      "Train Epoch: 18 [5120/50016 (10%)]\tLoss: 0.639940\n",
      "Train Epoch: 18 [10240/50016 (20%)]\tLoss: 0.710131\n",
      "Train Epoch: 18 [15360/50016 (31%)]\tLoss: 0.787956\n",
      "Train Epoch: 18 [20480/50016 (41%)]\tLoss: 0.761222\n",
      "Train Epoch: 18 [25600/50016 (51%)]\tLoss: 0.689644\n",
      "Train Epoch: 18 [30720/50016 (61%)]\tLoss: 0.389090\n",
      "Train Epoch: 18 [35840/50016 (72%)]\tLoss: 0.595160\n",
      "Train Epoch: 18 [40960/50016 (82%)]\tLoss: 0.483971\n",
      "Train Epoch: 18 [46080/50016 (92%)]\tLoss: 0.714476\n",
      "Test Epoch: 18 [0/10016 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 18 [2048/10016 (20%)]\tAcc: 0.769221\n",
      "Test Epoch: 18 [4096/10016 (41%)]\tAcc: 0.743829\n",
      "Test Epoch: 18 [6144/10016 (61%)]\tAcc: 0.807338\n",
      "Test Epoch: 18 [8192/10016 (82%)]\tAcc: 0.682394\n",
      "Total test acc = 0.7663138396487458\n",
      "epoch:  19\n",
      "Train Epoch: 19 [5120/50016 (10%)]\tLoss: 0.631852\n",
      "Train Epoch: 19 [10240/50016 (20%)]\tLoss: 0.786268\n",
      "Train Epoch: 19 [15360/50016 (31%)]\tLoss: 0.952253\n",
      "Train Epoch: 19 [20480/50016 (41%)]\tLoss: 0.723511\n",
      "Train Epoch: 19 [25600/50016 (51%)]\tLoss: 0.569336\n",
      "Train Epoch: 19 [30720/50016 (61%)]\tLoss: 0.721231\n",
      "Train Epoch: 19 [35840/50016 (72%)]\tLoss: 0.766805\n",
      "Train Epoch: 19 [40960/50016 (82%)]\tLoss: 0.505463\n",
      "Train Epoch: 19 [46080/50016 (92%)]\tLoss: 0.577993\n",
      "Test Epoch: 19 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 19 [2048/10016 (20%)]\tAcc: 0.808494\n",
      "Test Epoch: 19 [4096/10016 (41%)]\tAcc: 0.681511\n",
      "Test Epoch: 19 [6144/10016 (61%)]\tAcc: 0.739130\n",
      "Test Epoch: 19 [8192/10016 (82%)]\tAcc: 0.899344\n",
      "Total test acc = 0.7889201809027728\n",
      "epoch:  20\n",
      "Train Epoch: 20 [5120/50016 (10%)]\tLoss: 1.249727\n",
      "Train Epoch: 20 [10240/50016 (20%)]\tLoss: 0.319102\n",
      "Train Epoch: 20 [15360/50016 (31%)]\tLoss: 0.483402\n",
      "Train Epoch: 20 [20480/50016 (41%)]\tLoss: 0.792789\n",
      "Train Epoch: 20 [25600/50016 (51%)]\tLoss: 0.556761\n",
      "Train Epoch: 20 [30720/50016 (61%)]\tLoss: 0.478903\n",
      "Train Epoch: 20 [35840/50016 (72%)]\tLoss: 0.944002\n",
      "Train Epoch: 20 [40960/50016 (82%)]\tLoss: 0.372607\n",
      "Train Epoch: 20 [46080/50016 (92%)]\tLoss: 0.552202\n",
      "Test Epoch: 20 [0/10016 (0%)]\tAcc: 0.718750\n",
      "Test Epoch: 20 [2048/10016 (20%)]\tAcc: 0.772244\n",
      "Test Epoch: 20 [4096/10016 (41%)]\tAcc: 0.680411\n",
      "Test Epoch: 20 [6144/10016 (61%)]\tAcc: 0.742088\n",
      "Test Epoch: 20 [8192/10016 (82%)]\tAcc: 0.715629\n",
      "Total test acc = 0.7879784528399978\n",
      "epoch:  21\n",
      "Train Epoch: 21 [5120/50016 (10%)]\tLoss: 0.435957\n",
      "Train Epoch: 21 [10240/50016 (20%)]\tLoss: 0.475448\n",
      "Train Epoch: 21 [15360/50016 (31%)]\tLoss: 0.666208\n",
      "Train Epoch: 21 [20480/50016 (41%)]\tLoss: 0.288283\n",
      "Train Epoch: 21 [25600/50016 (51%)]\tLoss: 0.483933\n",
      "Train Epoch: 21 [30720/50016 (61%)]\tLoss: 0.314495\n",
      "Train Epoch: 21 [35840/50016 (72%)]\tLoss: 0.942973\n",
      "Train Epoch: 21 [40960/50016 (82%)]\tLoss: 1.018870\n",
      "Train Epoch: 21 [46080/50016 (92%)]\tLoss: 0.621326\n",
      "Test Epoch: 21 [0/10016 (0%)]\tAcc: 0.687500\n",
      "Test Epoch: 21 [2048/10016 (20%)]\tAcc: 0.677698\n",
      "Test Epoch: 21 [4096/10016 (41%)]\tAcc: 0.840631\n",
      "Test Epoch: 21 [6144/10016 (61%)]\tAcc: 0.618006\n",
      "Test Epoch: 21 [8192/10016 (82%)]\tAcc: 0.776174\n",
      "Total test acc = 0.7938685621950241\n",
      "epoch:  22\n",
      "Train Epoch: 22 [5120/50016 (10%)]\tLoss: 0.617242\n",
      "Train Epoch: 22 [10240/50016 (20%)]\tLoss: 0.665133\n",
      "Train Epoch: 22 [15360/50016 (31%)]\tLoss: 0.559613\n",
      "Train Epoch: 22 [20480/50016 (41%)]\tLoss: 0.479232\n",
      "Train Epoch: 22 [25600/50016 (51%)]\tLoss: 0.577559\n",
      "Train Epoch: 22 [30720/50016 (61%)]\tLoss: 0.384402\n",
      "Train Epoch: 22 [35840/50016 (72%)]\tLoss: 0.669382\n",
      "Train Epoch: 22 [40960/50016 (82%)]\tLoss: 0.426643\n",
      "Train Epoch: 22 [46080/50016 (92%)]\tLoss: 0.732842\n",
      "Test Epoch: 22 [0/10016 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 22 [2048/10016 (20%)]\tAcc: 0.770346\n",
      "Test Epoch: 22 [4096/10016 (41%)]\tAcc: 0.746910\n",
      "Test Epoch: 22 [6144/10016 (61%)]\tAcc: 0.712822\n",
      "Test Epoch: 22 [8192/10016 (82%)]\tAcc: 0.807425\n",
      "Total test acc = 0.7939684764498879\n",
      "epoch:  23\n",
      "Train Epoch: 23 [5120/50016 (10%)]\tLoss: 0.635470\n",
      "Train Epoch: 23 [10240/50016 (20%)]\tLoss: 0.370849\n",
      "Train Epoch: 23 [15360/50016 (31%)]\tLoss: 0.849955\n",
      "Train Epoch: 23 [20480/50016 (41%)]\tLoss: 0.436305\n",
      "Train Epoch: 23 [25600/50016 (51%)]\tLoss: 0.449131\n",
      "Train Epoch: 23 [30720/50016 (61%)]\tLoss: 0.775665\n",
      "Train Epoch: 23 [35840/50016 (72%)]\tLoss: 0.536894\n",
      "Train Epoch: 23 [40960/50016 (82%)]\tLoss: 0.577638\n",
      "Train Epoch: 23 [46080/50016 (92%)]\tLoss: 0.694954\n",
      "Test Epoch: 23 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 23 [2048/10016 (20%)]\tAcc: 0.773310\n",
      "Test Epoch: 23 [4096/10016 (41%)]\tAcc: 0.898189\n",
      "Test Epoch: 23 [6144/10016 (61%)]\tAcc: 0.745877\n",
      "Test Epoch: 23 [8192/10016 (82%)]\tAcc: 0.775260\n",
      "Total test acc = 0.8018167469862317\n",
      "epoch:  24\n",
      "Train Epoch: 24 [5120/50016 (10%)]\tLoss: 0.470008\n",
      "Train Epoch: 24 [10240/50016 (20%)]\tLoss: 0.386124\n",
      "Train Epoch: 24 [15360/50016 (31%)]\tLoss: 0.432455\n",
      "Train Epoch: 24 [20480/50016 (41%)]\tLoss: 0.484157\n",
      "Train Epoch: 24 [25600/50016 (51%)]\tLoss: 0.719544\n",
      "Train Epoch: 24 [30720/50016 (61%)]\tLoss: 0.604940\n",
      "Train Epoch: 24 [35840/50016 (72%)]\tLoss: 1.015227\n",
      "Train Epoch: 24 [40960/50016 (82%)]\tLoss: 0.640740\n",
      "Train Epoch: 24 [46080/50016 (92%)]\tLoss: 0.472923\n",
      "Test Epoch: 24 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 24 [2048/10016 (20%)]\tAcc: 0.771387\n",
      "Test Epoch: 24 [4096/10016 (41%)]\tAcc: 0.835716\n",
      "Test Epoch: 24 [6144/10016 (61%)]\tAcc: 0.681601\n",
      "Test Epoch: 24 [8192/10016 (82%)]\tAcc: 0.775354\n",
      "Total test acc = 0.7997525430286433\n",
      "epoch:  25\n",
      "Train Epoch: 25 [5120/50016 (10%)]\tLoss: 0.524710\n",
      "Train Epoch: 25 [10240/50016 (20%)]\tLoss: 0.389349\n",
      "Train Epoch: 25 [15360/50016 (31%)]\tLoss: 0.520898\n",
      "Train Epoch: 25 [20480/50016 (41%)]\tLoss: 0.541710\n",
      "Train Epoch: 25 [25600/50016 (51%)]\tLoss: 1.072470\n",
      "Train Epoch: 25 [30720/50016 (61%)]\tLoss: 0.619885\n",
      "Train Epoch: 25 [35840/50016 (72%)]\tLoss: 0.435834\n",
      "Train Epoch: 25 [40960/50016 (82%)]\tLoss: 0.443223\n",
      "Train Epoch: 25 [46080/50016 (92%)]\tLoss: 0.467839\n",
      "Test Epoch: 25 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 25 [2048/10016 (20%)]\tAcc: 0.708887\n",
      "Test Epoch: 25 [4096/10016 (41%)]\tAcc: 0.839593\n",
      "Test Epoch: 25 [6144/10016 (61%)]\tAcc: 0.712819\n",
      "Test Epoch: 25 [8192/10016 (82%)]\tAcc: 0.808438\n",
      "Total test acc = 0.7930396301550502\n",
      "epoch:  26\n",
      "Train Epoch: 26 [5120/50016 (10%)]\tLoss: 0.902180\n",
      "Train Epoch: 26 [10240/50016 (20%)]\tLoss: 0.458984\n",
      "Train Epoch: 26 [15360/50016 (31%)]\tLoss: 0.341844\n",
      "Train Epoch: 26 [20480/50016 (41%)]\tLoss: 0.688903\n",
      "Train Epoch: 26 [25600/50016 (51%)]\tLoss: 0.406432\n",
      "Train Epoch: 26 [30720/50016 (61%)]\tLoss: 0.668623\n",
      "Train Epoch: 26 [35840/50016 (72%)]\tLoss: 0.381607\n",
      "Train Epoch: 26 [40960/50016 (82%)]\tLoss: 0.651220\n",
      "Train Epoch: 26 [46080/50016 (92%)]\tLoss: 0.879473\n",
      "Test Epoch: 26 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 26 [2048/10016 (20%)]\tAcc: 0.834798\n",
      "Test Epoch: 26 [4096/10016 (41%)]\tAcc: 0.872920\n",
      "Test Epoch: 26 [6144/10016 (61%)]\tAcc: 0.744041\n",
      "Test Epoch: 26 [8192/10016 (82%)]\tAcc: 0.716638\n",
      "Total test acc = 0.7958265251104276\n",
      "epoch:  27\n",
      "Train Epoch: 27 [5120/50016 (10%)]\tLoss: 0.488341\n",
      "Train Epoch: 27 [10240/50016 (20%)]\tLoss: 0.713543\n",
      "Train Epoch: 27 [15360/50016 (31%)]\tLoss: 0.409293\n",
      "Train Epoch: 27 [20480/50016 (41%)]\tLoss: 0.658912\n",
      "Train Epoch: 27 [25600/50016 (51%)]\tLoss: 0.439762\n",
      "Train Epoch: 27 [30720/50016 (61%)]\tLoss: 0.550769\n",
      "Train Epoch: 27 [35840/50016 (72%)]\tLoss: 0.450206\n",
      "Train Epoch: 27 [40960/50016 (82%)]\tLoss: 0.790509\n",
      "Train Epoch: 27 [46080/50016 (92%)]\tLoss: 0.647674\n",
      "Test Epoch: 27 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 27 [2048/10016 (20%)]\tAcc: 0.927607\n",
      "Test Epoch: 27 [4096/10016 (41%)]\tAcc: 0.684443\n",
      "Test Epoch: 27 [6144/10016 (61%)]\tAcc: 0.710773\n",
      "Test Epoch: 27 [8192/10016 (82%)]\tAcc: 0.806513\n",
      "Total test acc = 0.8091427580068407\n",
      "epoch:  28\n",
      "Train Epoch: 28 [5120/50016 (10%)]\tLoss: 0.795175\n",
      "Train Epoch: 28 [10240/50016 (20%)]\tLoss: 0.447308\n",
      "Train Epoch: 28 [15360/50016 (31%)]\tLoss: 0.356151\n",
      "Train Epoch: 28 [20480/50016 (41%)]\tLoss: 0.678199\n",
      "Train Epoch: 28 [25600/50016 (51%)]\tLoss: 0.423120\n",
      "Train Epoch: 28 [30720/50016 (61%)]\tLoss: 0.647036\n",
      "Train Epoch: 28 [35840/50016 (72%)]\tLoss: 0.668963\n",
      "Train Epoch: 28 [40960/50016 (82%)]\tLoss: 0.528342\n",
      "Train Epoch: 28 [46080/50016 (92%)]\tLoss: 0.896924\n",
      "Test Epoch: 28 [0/10016 (0%)]\tAcc: 0.968750\n",
      "Test Epoch: 28 [2048/10016 (20%)]\tAcc: 0.805535\n",
      "Test Epoch: 28 [4096/10016 (41%)]\tAcc: 0.717613\n",
      "Test Epoch: 28 [6144/10016 (61%)]\tAcc: 0.806392\n",
      "Test Epoch: 28 [8192/10016 (82%)]\tAcc: 0.741113\n",
      "Total test acc = 0.8164685583491815\n",
      "epoch:  29\n",
      "Train Epoch: 29 [5120/50016 (10%)]\tLoss: 0.417442\n",
      "Train Epoch: 29 [10240/50016 (20%)]\tLoss: 0.656119\n",
      "Train Epoch: 29 [15360/50016 (31%)]\tLoss: 0.666043\n",
      "Train Epoch: 29 [20480/50016 (41%)]\tLoss: 0.224539\n",
      "Train Epoch: 29 [25600/50016 (51%)]\tLoss: 0.375100\n",
      "Train Epoch: 29 [30720/50016 (61%)]\tLoss: 0.556719\n",
      "Train Epoch: 29 [35840/50016 (72%)]\tLoss: 0.589177\n",
      "Train Epoch: 29 [40960/50016 (82%)]\tLoss: 0.688271\n",
      "Train Epoch: 29 [46080/50016 (92%)]\tLoss: 0.651599\n",
      "Test Epoch: 29 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 29 [2048/10016 (20%)]\tAcc: 0.868005\n",
      "Test Epoch: 29 [4096/10016 (41%)]\tAcc: 0.841546\n",
      "Test Epoch: 29 [6144/10016 (61%)]\tAcc: 0.838648\n",
      "Test Epoch: 29 [8192/10016 (82%)]\tAcc: 0.683403\n",
      "Total test acc = 0.8019200767465391\n",
      "epoch:  30\n",
      "Train Epoch: 30 [5120/50016 (10%)]\tLoss: 0.997748\n",
      "Train Epoch: 30 [10240/50016 (20%)]\tLoss: 0.794766\n",
      "Train Epoch: 30 [15360/50016 (31%)]\tLoss: 0.442132\n",
      "Train Epoch: 30 [20480/50016 (41%)]\tLoss: 0.553462\n",
      "Train Epoch: 30 [25600/50016 (51%)]\tLoss: 0.450012\n",
      "Train Epoch: 30 [30720/50016 (61%)]\tLoss: 0.707538\n",
      "Train Epoch: 30 [35840/50016 (72%)]\tLoss: 0.419028\n",
      "Train Epoch: 30 [40960/50016 (82%)]\tLoss: 0.338920\n",
      "Train Epoch: 30 [46080/50016 (92%)]\tLoss: 0.283547\n",
      "Test Epoch: 30 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 30 [2048/10016 (20%)]\tAcc: 0.834710\n",
      "Test Epoch: 30 [4096/10016 (41%)]\tAcc: 0.715570\n",
      "Test Epoch: 30 [6144/10016 (61%)]\tAcc: 0.773340\n",
      "Test Epoch: 30 [8192/10016 (82%)]\tAcc: 0.933342\n",
      "Total test acc = 0.8003718880999812\n",
      "epoch:  31\n",
      "Train Epoch: 31 [5120/50016 (10%)]\tLoss: 0.601572\n",
      "Train Epoch: 31 [10240/50016 (20%)]\tLoss: 0.568380\n",
      "Train Epoch: 31 [15360/50016 (31%)]\tLoss: 0.588403\n",
      "Train Epoch: 31 [20480/50016 (41%)]\tLoss: 0.405271\n",
      "Train Epoch: 31 [25600/50016 (51%)]\tLoss: 0.551916\n",
      "Train Epoch: 31 [30720/50016 (61%)]\tLoss: 0.775353\n",
      "Train Epoch: 31 [35840/50016 (72%)]\tLoss: 0.714757\n",
      "Train Epoch: 31 [40960/50016 (82%)]\tLoss: 0.901747\n",
      "Train Epoch: 31 [46080/50016 (92%)]\tLoss: 0.471770\n",
      "Test Epoch: 31 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 31 [2048/10016 (20%)]\tAcc: 0.832849\n",
      "Test Epoch: 31 [4096/10016 (41%)]\tAcc: 0.806541\n",
      "Test Epoch: 31 [6144/10016 (61%)]\tAcc: 0.773339\n",
      "Test Epoch: 31 [8192/10016 (82%)]\tAcc: 0.743981\n",
      "Total test acc = 0.8066652474964074\n",
      "epoch:  32\n",
      "Train Epoch: 32 [5120/50016 (10%)]\tLoss: 0.519087\n",
      "Train Epoch: 32 [10240/50016 (20%)]\tLoss: 0.370855\n",
      "Train Epoch: 32 [15360/50016 (31%)]\tLoss: 0.613564\n",
      "Train Epoch: 32 [20480/50016 (41%)]\tLoss: 0.429764\n",
      "Train Epoch: 32 [25600/50016 (51%)]\tLoss: 0.229992\n",
      "Train Epoch: 32 [30720/50016 (61%)]\tLoss: 0.440567\n",
      "Train Epoch: 32 [35840/50016 (72%)]\tLoss: 0.458662\n",
      "Train Epoch: 32 [40960/50016 (82%)]\tLoss: 0.574067\n",
      "Train Epoch: 32 [46080/50016 (92%)]\tLoss: 0.460901\n",
      "Test Epoch: 32 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 32 [2048/10016 (20%)]\tAcc: 0.868950\n",
      "Test Epoch: 32 [4096/10016 (41%)]\tAcc: 0.872951\n",
      "Test Epoch: 32 [6144/10016 (61%)]\tAcc: 0.834832\n",
      "Test Epoch: 32 [8192/10016 (82%)]\tAcc: 0.834007\n",
      "Total test acc = 0.8260748115112887\n",
      "epoch:  33\n",
      "Train Epoch: 33 [5120/50016 (10%)]\tLoss: 0.292933\n",
      "Train Epoch: 33 [10240/50016 (20%)]\tLoss: 0.558576\n",
      "Train Epoch: 33 [15360/50016 (31%)]\tLoss: 0.542125\n",
      "Train Epoch: 33 [20480/50016 (41%)]\tLoss: 0.794176\n",
      "Train Epoch: 33 [25600/50016 (51%)]\tLoss: 0.335387\n",
      "Train Epoch: 33 [30720/50016 (61%)]\tLoss: 0.459412\n",
      "Train Epoch: 33 [35840/50016 (72%)]\tLoss: 0.585508\n",
      "Train Epoch: 33 [40960/50016 (82%)]\tLoss: 0.554407\n",
      "Train Epoch: 33 [46080/50016 (92%)]\tLoss: 0.446350\n",
      "Test Epoch: 33 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 33 [2048/10016 (20%)]\tAcc: 0.800654\n",
      "Test Epoch: 33 [4096/10016 (41%)]\tAcc: 0.841698\n",
      "Test Epoch: 33 [6144/10016 (61%)]\tAcc: 0.648279\n",
      "Test Epoch: 33 [8192/10016 (82%)]\tAcc: 0.777183\n",
      "Total test acc = 0.821423100841172\n",
      "epoch:  34\n",
      "Train Epoch: 34 [5120/50016 (10%)]\tLoss: 0.684746\n",
      "Train Epoch: 34 [10240/50016 (20%)]\tLoss: 0.236345\n",
      "Train Epoch: 34 [15360/50016 (31%)]\tLoss: 0.296465\n",
      "Train Epoch: 34 [20480/50016 (41%)]\tLoss: 0.825498\n",
      "Train Epoch: 34 [25600/50016 (51%)]\tLoss: 0.530369\n",
      "Train Epoch: 34 [30720/50016 (61%)]\tLoss: 0.665020\n",
      "Train Epoch: 34 [35840/50016 (72%)]\tLoss: 0.663587\n",
      "Train Epoch: 34 [40960/50016 (82%)]\tLoss: 0.433365\n",
      "Train Epoch: 34 [46080/50016 (92%)]\tLoss: 0.359789\n",
      "Test Epoch: 34 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 34 [2048/10016 (20%)]\tAcc: 0.866020\n",
      "Test Epoch: 34 [4096/10016 (41%)]\tAcc: 0.714594\n",
      "Test Epoch: 34 [6144/10016 (61%)]\tAcc: 0.739038\n",
      "Test Epoch: 34 [8192/10016 (82%)]\tAcc: 0.837794\n",
      "Total test acc = 0.8128587863191729\n",
      "epoch:  35\n",
      "Train Epoch: 35 [5120/50016 (10%)]\tLoss: 0.478386\n",
      "Train Epoch: 35 [10240/50016 (20%)]\tLoss: 0.612727\n",
      "Train Epoch: 35 [15360/50016 (31%)]\tLoss: 0.507557\n",
      "Train Epoch: 35 [20480/50016 (41%)]\tLoss: 0.412071\n",
      "Train Epoch: 35 [25600/50016 (51%)]\tLoss: 0.335802\n",
      "Train Epoch: 35 [30720/50016 (61%)]\tLoss: 0.546741\n",
      "Train Epoch: 35 [35840/50016 (72%)]\tLoss: 0.452434\n",
      "Train Epoch: 35 [40960/50016 (82%)]\tLoss: 0.476910\n",
      "Train Epoch: 35 [46080/50016 (92%)]\tLoss: 0.394229\n",
      "Test Epoch: 35 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 35 [2048/10016 (20%)]\tAcc: 0.900294\n",
      "Test Epoch: 35 [4096/10016 (41%)]\tAcc: 0.775231\n",
      "Test Epoch: 35 [6144/10016 (61%)]\tAcc: 0.710748\n",
      "Test Epoch: 35 [8192/10016 (82%)]\tAcc: 0.713708\n",
      "Total test acc = 0.8246296503075881\n",
      "epoch:  36\n",
      "Train Epoch: 36 [5120/50016 (10%)]\tLoss: 0.490854\n",
      "Train Epoch: 36 [10240/50016 (20%)]\tLoss: 0.473432\n",
      "Train Epoch: 36 [15360/50016 (31%)]\tLoss: 0.241124\n",
      "Train Epoch: 36 [20480/50016 (41%)]\tLoss: 0.541355\n",
      "Train Epoch: 36 [25600/50016 (51%)]\tLoss: 0.738001\n",
      "Train Epoch: 36 [30720/50016 (61%)]\tLoss: 0.261174\n",
      "Train Epoch: 36 [35840/50016 (72%)]\tLoss: 0.598781\n",
      "Train Epoch: 36 [40960/50016 (82%)]\tLoss: 0.445070\n",
      "Train Epoch: 36 [46080/50016 (92%)]\tLoss: 0.772099\n",
      "Test Epoch: 36 [0/10016 (0%)]\tAcc: 0.968750\n",
      "Test Epoch: 36 [2048/10016 (20%)]\tAcc: 0.711790\n",
      "Test Epoch: 36 [4096/10016 (41%)]\tAcc: 0.872858\n",
      "Test Epoch: 36 [6144/10016 (61%)]\tAcc: 0.712790\n",
      "Test Epoch: 36 [8192/10016 (82%)]\tAcc: 0.837732\n",
      "Total test acc = 0.8284459774114584\n",
      "epoch:  37\n",
      "Train Epoch: 37 [5120/50016 (10%)]\tLoss: 0.526893\n",
      "Train Epoch: 37 [10240/50016 (20%)]\tLoss: 0.629368\n",
      "Train Epoch: 37 [15360/50016 (31%)]\tLoss: 0.564758\n",
      "Train Epoch: 37 [20480/50016 (41%)]\tLoss: 0.373531\n",
      "Train Epoch: 37 [25600/50016 (51%)]\tLoss: 0.251869\n",
      "Train Epoch: 37 [30720/50016 (61%)]\tLoss: 0.236671\n",
      "Train Epoch: 37 [35840/50016 (72%)]\tLoss: 0.684622\n",
      "Train Epoch: 37 [40960/50016 (82%)]\tLoss: 0.459835\n",
      "Train Epoch: 37 [46080/50016 (92%)]\tLoss: 0.238106\n",
      "Test Epoch: 37 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 37 [2048/10016 (20%)]\tAcc: 0.870049\n",
      "Test Epoch: 37 [4096/10016 (41%)]\tAcc: 0.712669\n",
      "Test Epoch: 37 [6144/10016 (61%)]\tAcc: 0.866111\n",
      "Test Epoch: 37 [8192/10016 (82%)]\tAcc: 0.931542\n",
      "Total test acc = 0.819881263115363\n",
      "epoch:  38\n",
      "Train Epoch: 38 [5120/50016 (10%)]\tLoss: 0.375054\n",
      "Train Epoch: 38 [10240/50016 (20%)]\tLoss: 0.517422\n",
      "Train Epoch: 38 [15360/50016 (31%)]\tLoss: 0.299807\n",
      "Train Epoch: 38 [20480/50016 (41%)]\tLoss: 0.369325\n",
      "Train Epoch: 38 [25600/50016 (51%)]\tLoss: 0.626943\n",
      "Train Epoch: 38 [30720/50016 (61%)]\tLoss: 0.798488\n",
      "Train Epoch: 38 [35840/50016 (72%)]\tLoss: 0.257880\n",
      "Train Epoch: 38 [40960/50016 (82%)]\tLoss: 0.331143\n",
      "Train Epoch: 38 [46080/50016 (92%)]\tLoss: 0.498682\n",
      "Test Epoch: 38 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 38 [2048/10016 (20%)]\tAcc: 0.771355\n",
      "Test Epoch: 38 [4096/10016 (41%)]\tAcc: 0.869987\n",
      "Test Epoch: 38 [6144/10016 (61%)]\tAcc: 0.837670\n",
      "Test Epoch: 38 [8192/10016 (82%)]\tAcc: 0.871055\n",
      "Total test acc = 0.8221457635852812\n",
      "epoch:  39\n",
      "Train Epoch: 39 [5120/50016 (10%)]\tLoss: 0.465087\n",
      "Train Epoch: 39 [10240/50016 (20%)]\tLoss: 0.474203\n",
      "Train Epoch: 39 [15360/50016 (31%)]\tLoss: 0.461570\n",
      "Train Epoch: 39 [20480/50016 (41%)]\tLoss: 0.443537\n",
      "Train Epoch: 39 [25600/50016 (51%)]\tLoss: 0.649702\n",
      "Train Epoch: 39 [30720/50016 (61%)]\tLoss: 0.548738\n",
      "Train Epoch: 39 [35840/50016 (72%)]\tLoss: 0.340564\n",
      "Train Epoch: 39 [40960/50016 (82%)]\tLoss: 0.316093\n",
      "Train Epoch: 39 [46080/50016 (92%)]\tLoss: 0.536650\n",
      "Test Epoch: 39 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 39 [2048/10016 (20%)]\tAcc: 0.770258\n",
      "Test Epoch: 39 [4096/10016 (41%)]\tAcc: 0.745846\n",
      "Test Epoch: 39 [6144/10016 (61%)]\tAcc: 0.741110\n",
      "Test Epoch: 39 [8192/10016 (82%)]\tAcc: 0.743949\n",
      "Total test acc = 0.8227683599849118\n",
      "epoch:  40\n",
      "Train Epoch: 40 [5120/50016 (10%)]\tLoss: 0.382309\n",
      "Train Epoch: 40 [10240/50016 (20%)]\tLoss: 0.223299\n",
      "Train Epoch: 40 [15360/50016 (31%)]\tLoss: 0.729182\n",
      "Train Epoch: 40 [20480/50016 (41%)]\tLoss: 0.333205\n",
      "Train Epoch: 40 [25600/50016 (51%)]\tLoss: 0.286022\n",
      "Train Epoch: 40 [30720/50016 (61%)]\tLoss: 0.223631\n",
      "Train Epoch: 40 [35840/50016 (72%)]\tLoss: 0.545254\n",
      "Train Epoch: 40 [40960/50016 (82%)]\tLoss: 0.473476\n",
      "Train Epoch: 40 [46080/50016 (92%)]\tLoss: 0.717437\n",
      "Test Epoch: 40 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 40 [2048/10016 (20%)]\tAcc: 0.805507\n",
      "Test Epoch: 40 [4096/10016 (41%)]\tAcc: 0.775261\n",
      "Test Epoch: 40 [6144/10016 (61%)]\tAcc: 0.648246\n",
      "Test Epoch: 40 [8192/10016 (82%)]\tAcc: 0.741080\n",
      "Total test acc = 0.8264814739785167\n",
      "epoch:  41\n",
      "Train Epoch: 41 [5120/50016 (10%)]\tLoss: 0.301535\n",
      "Train Epoch: 41 [10240/50016 (20%)]\tLoss: 0.374238\n",
      "Train Epoch: 41 [15360/50016 (31%)]\tLoss: 0.366734\n",
      "Train Epoch: 41 [20480/50016 (41%)]\tLoss: 0.193036\n",
      "Train Epoch: 41 [25600/50016 (51%)]\tLoss: 0.562892\n",
      "Train Epoch: 41 [30720/50016 (61%)]\tLoss: 0.728525\n",
      "Train Epoch: 41 [35840/50016 (72%)]\tLoss: 0.282554\n",
      "Train Epoch: 41 [40960/50016 (82%)]\tLoss: 0.489314\n",
      "Train Epoch: 41 [46080/50016 (92%)]\tLoss: 0.470214\n",
      "Test Epoch: 41 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 41 [2048/10016 (20%)]\tAcc: 0.866020\n",
      "Test Epoch: 41 [4096/10016 (41%)]\tAcc: 0.716699\n",
      "Test Epoch: 41 [6144/10016 (61%)]\tAcc: 0.775291\n",
      "Test Epoch: 41 [8192/10016 (82%)]\tAcc: 0.747887\n",
      "Total test acc = 0.8246296437054096\n",
      "epoch:  42\n",
      "Train Epoch: 42 [5120/50016 (10%)]\tLoss: 0.879987\n",
      "Train Epoch: 42 [10240/50016 (20%)]\tLoss: 0.434092\n",
      "Train Epoch: 42 [15360/50016 (31%)]\tLoss: 0.439306\n",
      "Train Epoch: 42 [20480/50016 (41%)]\tLoss: 0.496943\n",
      "Train Epoch: 42 [25600/50016 (51%)]\tLoss: 0.521211\n",
      "Train Epoch: 42 [30720/50016 (61%)]\tLoss: 0.754410\n",
      "Train Epoch: 42 [35840/50016 (72%)]\tLoss: 0.630615\n",
      "Train Epoch: 42 [40960/50016 (82%)]\tLoss: 0.392047\n",
      "Train Epoch: 42 [46080/50016 (92%)]\tLoss: 0.468286\n",
      "Test Epoch: 42 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 42 [2048/10016 (20%)]\tAcc: 0.929590\n",
      "Test Epoch: 42 [4096/10016 (41%)]\tAcc: 0.806391\n",
      "Test Epoch: 42 [6144/10016 (61%)]\tAcc: 0.743061\n",
      "Test Epoch: 42 [8192/10016 (82%)]\tAcc: 0.775264\n",
      "Total test acc = 0.8243136250320289\n",
      "epoch:  43\n",
      "Train Epoch: 43 [5120/50016 (10%)]\tLoss: 0.543654\n",
      "Train Epoch: 43 [10240/50016 (20%)]\tLoss: 0.477938\n",
      "Train Epoch: 43 [15360/50016 (31%)]\tLoss: 0.382104\n",
      "Train Epoch: 43 [20480/50016 (41%)]\tLoss: 0.476217\n",
      "Train Epoch: 43 [25600/50016 (51%)]\tLoss: 0.511602\n",
      "Train Epoch: 43 [30720/50016 (61%)]\tLoss: 0.563432\n",
      "Train Epoch: 43 [35840/50016 (72%)]\tLoss: 0.743788\n",
      "Train Epoch: 43 [40960/50016 (82%)]\tLoss: 0.700976\n",
      "Train Epoch: 43 [46080/50016 (92%)]\tLoss: 0.401344\n",
      "Test Epoch: 43 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 43 [2048/10016 (20%)]\tAcc: 0.838682\n",
      "Test Epoch: 43 [4096/10016 (41%)]\tAcc: 0.841639\n",
      "Test Epoch: 43 [6144/10016 (61%)]\tAcc: 0.775322\n",
      "Test Epoch: 43 [8192/10016 (82%)]\tAcc: 0.772304\n",
      "Total test acc = 0.8223585785165285\n",
      "epoch:  44\n",
      "Train Epoch: 44 [5120/50016 (10%)]\tLoss: 0.215402\n",
      "Train Epoch: 44 [10240/50016 (20%)]\tLoss: 0.478608\n",
      "Train Epoch: 44 [15360/50016 (31%)]\tLoss: 0.536371\n",
      "Train Epoch: 44 [20480/50016 (41%)]\tLoss: 0.264442\n",
      "Train Epoch: 44 [25600/50016 (51%)]\tLoss: 0.430459\n",
      "Train Epoch: 44 [30720/50016 (61%)]\tLoss: 0.313319\n",
      "Train Epoch: 44 [35840/50016 (72%)]\tLoss: 0.520949\n",
      "Train Epoch: 44 [40960/50016 (82%)]\tLoss: 0.396468\n",
      "Train Epoch: 44 [46080/50016 (92%)]\tLoss: 0.466352\n",
      "Test Epoch: 44 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 44 [2048/10016 (20%)]\tAcc: 0.770412\n",
      "Test Epoch: 44 [4096/10016 (41%)]\tAcc: 0.809415\n",
      "Test Epoch: 44 [6144/10016 (61%)]\tAcc: 0.682429\n",
      "Test Epoch: 44 [8192/10016 (82%)]\tAcc: 0.811427\n",
      "Total test acc = 0.828965237932265\n",
      "epoch:  45\n",
      "Train Epoch: 45 [5120/50016 (10%)]\tLoss: 0.331467\n",
      "Train Epoch: 45 [10240/50016 (20%)]\tLoss: 0.286083\n",
      "Train Epoch: 45 [15360/50016 (31%)]\tLoss: 0.359360\n",
      "Train Epoch: 45 [20480/50016 (41%)]\tLoss: 0.456304\n",
      "Train Epoch: 45 [25600/50016 (51%)]\tLoss: 0.518825\n",
      "Train Epoch: 45 [30720/50016 (61%)]\tLoss: 0.538532\n",
      "Train Epoch: 45 [35840/50016 (72%)]\tLoss: 0.224617\n",
      "Train Epoch: 45 [40960/50016 (82%)]\tLoss: 0.306544\n",
      "Train Epoch: 45 [46080/50016 (92%)]\tLoss: 0.603862\n",
      "Test Epoch: 45 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 45 [2048/10016 (20%)]\tAcc: 0.829916\n",
      "Test Epoch: 45 [4096/10016 (41%)]\tAcc: 0.807486\n",
      "Test Epoch: 45 [6144/10016 (61%)]\tAcc: 0.710773\n",
      "Test Epoch: 45 [8192/10016 (82%)]\tAcc: 0.837730\n",
      "Total test acc = 0.8243135115227578\n",
      "epoch:  46\n",
      "Train Epoch: 46 [5120/50016 (10%)]\tLoss: 0.397325\n",
      "Train Epoch: 46 [10240/50016 (20%)]\tLoss: 0.340673\n",
      "Train Epoch: 46 [15360/50016 (31%)]\tLoss: 0.364084\n",
      "Train Epoch: 46 [20480/50016 (41%)]\tLoss: 0.408036\n",
      "Train Epoch: 46 [25600/50016 (51%)]\tLoss: 0.186549\n",
      "Train Epoch: 46 [30720/50016 (61%)]\tLoss: 0.766219\n",
      "Train Epoch: 46 [35840/50016 (72%)]\tLoss: 0.279151\n",
      "Train Epoch: 46 [40960/50016 (82%)]\tLoss: 0.383719\n",
      "Train Epoch: 46 [46080/50016 (92%)]\tLoss: 0.497679\n",
      "Test Epoch: 46 [0/10016 (0%)]\tAcc: 0.968750\n",
      "Test Epoch: 46 [2048/10016 (20%)]\tAcc: 0.899346\n",
      "Test Epoch: 46 [4096/10016 (41%)]\tAcc: 0.838740\n",
      "Test Epoch: 46 [6144/10016 (61%)]\tAcc: 0.805536\n",
      "Test Epoch: 46 [8192/10016 (82%)]\tAcc: 0.774348\n",
      "Total test acc = 0.8311264243964682\n",
      "epoch:  47\n",
      "Train Epoch: 47 [5120/50016 (10%)]\tLoss: 0.407487\n",
      "Train Epoch: 47 [10240/50016 (20%)]\tLoss: 0.596603\n",
      "Train Epoch: 47 [15360/50016 (31%)]\tLoss: 0.324140\n",
      "Train Epoch: 47 [20480/50016 (41%)]\tLoss: 0.554774\n",
      "Train Epoch: 47 [25600/50016 (51%)]\tLoss: 0.349786\n",
      "Train Epoch: 47 [30720/50016 (61%)]\tLoss: 0.349916\n",
      "Train Epoch: 47 [35840/50016 (72%)]\tLoss: 0.374844\n",
      "Train Epoch: 47 [40960/50016 (82%)]\tLoss: 0.661680\n",
      "Train Epoch: 47 [46080/50016 (92%)]\tLoss: 0.341045\n",
      "Test Epoch: 47 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 47 [2048/10016 (20%)]\tAcc: 0.679591\n",
      "Test Epoch: 47 [4096/10016 (41%)]\tAcc: 0.811395\n",
      "Test Epoch: 47 [6144/10016 (61%)]\tAcc: 0.835778\n",
      "Test Epoch: 47 [8192/10016 (82%)]\tAcc: 0.777246\n",
      "Total test acc = 0.8348397214092796\n",
      "epoch:  48\n",
      "Train Epoch: 48 [5120/50016 (10%)]\tLoss: 0.363649\n",
      "Train Epoch: 48 [10240/50016 (20%)]\tLoss: 0.220169\n",
      "Train Epoch: 48 [15360/50016 (31%)]\tLoss: 0.332716\n",
      "Train Epoch: 48 [20480/50016 (41%)]\tLoss: 0.271777\n",
      "Train Epoch: 48 [25600/50016 (51%)]\tLoss: 0.268987\n",
      "Train Epoch: 48 [30720/50016 (61%)]\tLoss: 0.534771\n",
      "Train Epoch: 48 [35840/50016 (72%)]\tLoss: 0.344181\n",
      "Train Epoch: 48 [40960/50016 (82%)]\tLoss: 0.232138\n",
      "Train Epoch: 48 [46080/50016 (92%)]\tLoss: 0.437006\n",
      "Test Epoch: 48 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 48 [2048/10016 (20%)]\tAcc: 0.805535\n",
      "Test Epoch: 48 [4096/10016 (41%)]\tAcc: 0.744931\n",
      "Test Epoch: 48 [6144/10016 (61%)]\tAcc: 0.779104\n",
      "Test Epoch: 48 [8192/10016 (82%)]\tAcc: 0.715727\n",
      "Total test acc = 0.8295844071149049\n",
      "epoch:  49\n",
      "Train Epoch: 49 [5120/50016 (10%)]\tLoss: 0.501634\n",
      "Train Epoch: 49 [10240/50016 (20%)]\tLoss: 0.259214\n",
      "Train Epoch: 49 [15360/50016 (31%)]\tLoss: 0.434150\n",
      "Train Epoch: 49 [20480/50016 (41%)]\tLoss: 0.484279\n",
      "Train Epoch: 49 [25600/50016 (51%)]\tLoss: 0.539916\n",
      "Train Epoch: 49 [30720/50016 (61%)]\tLoss: 0.372894\n",
      "Train Epoch: 49 [35840/50016 (72%)]\tLoss: 0.236843\n",
      "Train Epoch: 49 [40960/50016 (82%)]\tLoss: 0.293284\n",
      "Train Epoch: 49 [46080/50016 (92%)]\tLoss: 0.432256\n",
      "Test Epoch: 49 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 49 [2048/10016 (20%)]\tAcc: 0.803645\n",
      "Test Epoch: 49 [4096/10016 (41%)]\tAcc: 0.808375\n",
      "Test Epoch: 49 [6144/10016 (61%)]\tAcc: 0.804468\n",
      "Test Epoch: 49 [8192/10016 (82%)]\tAcc: 0.778222\n",
      "Total test acc = 0.8405198780865619\n",
      "epoch:  50\n",
      "Train Epoch: 50 [5120/50016 (10%)]\tLoss: 0.542937\n",
      "Train Epoch: 50 [10240/50016 (20%)]\tLoss: 0.222256\n",
      "Train Epoch: 50 [15360/50016 (31%)]\tLoss: 0.328864\n",
      "Train Epoch: 50 [20480/50016 (41%)]\tLoss: 0.570169\n",
      "Train Epoch: 50 [25600/50016 (51%)]\tLoss: 0.263323\n",
      "Train Epoch: 50 [30720/50016 (61%)]\tLoss: 0.359653\n",
      "Train Epoch: 50 [35840/50016 (72%)]\tLoss: 0.227960\n",
      "Train Epoch: 50 [40960/50016 (82%)]\tLoss: 0.406974\n",
      "Train Epoch: 50 [46080/50016 (92%)]\tLoss: 0.456057\n",
      "Test Epoch: 50 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 50 [2048/10016 (20%)]\tAcc: 0.837796\n",
      "Test Epoch: 50 [4096/10016 (41%)]\tAcc: 0.838769\n",
      "Test Epoch: 50 [6144/10016 (61%)]\tAcc: 0.742027\n",
      "Test Epoch: 50 [8192/10016 (82%)]\tAcc: 0.774257\n",
      "Total test acc = 0.830923091292213\n",
      "epoch:  51\n",
      "Train Epoch: 51 [5120/50016 (10%)]\tLoss: 0.345194\n",
      "Train Epoch: 51 [10240/50016 (20%)]\tLoss: 0.280354\n",
      "Train Epoch: 51 [15360/50016 (31%)]\tLoss: 0.392262\n",
      "Train Epoch: 51 [20480/50016 (41%)]\tLoss: 0.659113\n",
      "Train Epoch: 51 [25600/50016 (51%)]\tLoss: 0.425103\n",
      "Train Epoch: 51 [30720/50016 (61%)]\tLoss: 0.533523\n",
      "Train Epoch: 51 [35840/50016 (72%)]\tLoss: 0.306731\n",
      "Train Epoch: 51 [40960/50016 (82%)]\tLoss: 0.636948\n",
      "Train Epoch: 51 [46080/50016 (92%)]\tLoss: 0.478415\n",
      "Test Epoch: 51 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 51 [2048/10016 (20%)]\tAcc: 0.898312\n",
      "Test Epoch: 51 [4096/10016 (41%)]\tAcc: 0.808404\n",
      "Test Epoch: 51 [6144/10016 (61%)]\tAcc: 0.743127\n",
      "Test Epoch: 51 [8192/10016 (82%)]\tAcc: 0.903192\n",
      "Total test acc = 0.8383517362689057\n",
      "epoch:  52\n",
      "Train Epoch: 52 [5120/50016 (10%)]\tLoss: 0.657731\n",
      "Train Epoch: 52 [10240/50016 (20%)]\tLoss: 0.163508\n",
      "Train Epoch: 52 [15360/50016 (31%)]\tLoss: 0.599381\n",
      "Train Epoch: 52 [20480/50016 (41%)]\tLoss: 0.412731\n",
      "Train Epoch: 52 [25600/50016 (51%)]\tLoss: 0.263917\n",
      "Train Epoch: 52 [30720/50016 (61%)]\tLoss: 0.330835\n",
      "Train Epoch: 52 [35840/50016 (72%)]\tLoss: 0.159425\n",
      "Train Epoch: 52 [40960/50016 (82%)]\tLoss: 0.382068\n",
      "Train Epoch: 52 [46080/50016 (92%)]\tLoss: 0.504510\n",
      "Test Epoch: 52 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 52 [2048/10016 (20%)]\tAcc: 0.834927\n",
      "Test Epoch: 52 [4096/10016 (41%)]\tAcc: 0.746911\n",
      "Test Epoch: 52 [6144/10016 (61%)]\tAcc: 0.839807\n",
      "Test Epoch: 52 [8192/10016 (82%)]\tAcc: 0.808556\n",
      "Total test acc = 0.8482621003433729\n",
      "epoch:  53\n",
      "Train Epoch: 53 [5120/50016 (10%)]\tLoss: 0.363681\n",
      "Train Epoch: 53 [10240/50016 (20%)]\tLoss: 0.583887\n",
      "Train Epoch: 53 [15360/50016 (31%)]\tLoss: 0.245213\n",
      "Train Epoch: 53 [20480/50016 (41%)]\tLoss: 0.213108\n",
      "Train Epoch: 53 [25600/50016 (51%)]\tLoss: 0.121311\n",
      "Train Epoch: 53 [30720/50016 (61%)]\tLoss: 0.377560\n",
      "Train Epoch: 53 [35840/50016 (72%)]\tLoss: 0.200273\n",
      "Train Epoch: 53 [40960/50016 (82%)]\tLoss: 0.392524\n",
      "Train Epoch: 53 [46080/50016 (92%)]\tLoss: 0.348255\n",
      "Test Epoch: 53 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 53 [2048/10016 (20%)]\tAcc: 0.803646\n",
      "Test Epoch: 53 [4096/10016 (41%)]\tAcc: 0.747951\n",
      "Test Epoch: 53 [6144/10016 (61%)]\tAcc: 0.712793\n",
      "Test Epoch: 53 [8192/10016 (82%)]\tAcc: 0.870016\n",
      "Total test acc = 0.8328810614188146\n",
      "epoch:  54\n",
      "Train Epoch: 54 [5120/50016 (10%)]\tLoss: 0.454430\n",
      "Train Epoch: 54 [10240/50016 (20%)]\tLoss: 0.395410\n",
      "Train Epoch: 54 [15360/50016 (31%)]\tLoss: 0.294249\n",
      "Train Epoch: 54 [20480/50016 (41%)]\tLoss: 0.315939\n",
      "Train Epoch: 54 [25600/50016 (51%)]\tLoss: 0.489126\n",
      "Train Epoch: 54 [30720/50016 (61%)]\tLoss: 0.555922\n",
      "Train Epoch: 54 [35840/50016 (72%)]\tLoss: 0.394911\n",
      "Train Epoch: 54 [40960/50016 (82%)]\tLoss: 0.356234\n",
      "Train Epoch: 54 [46080/50016 (92%)]\tLoss: 0.297526\n",
      "Test Epoch: 54 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 54 [2048/10016 (20%)]\tAcc: 0.832757\n",
      "Test Epoch: 54 [4096/10016 (41%)]\tAcc: 0.776300\n",
      "Test Epoch: 54 [6144/10016 (61%)]\tAcc: 0.776205\n",
      "Test Epoch: 54 [8192/10016 (82%)]\tAcc: 0.868007\n",
      "Total test acc = 0.8327781411638425\n",
      "epoch:  55\n",
      "Train Epoch: 55 [5120/50016 (10%)]\tLoss: 0.337201\n",
      "Train Epoch: 55 [10240/50016 (20%)]\tLoss: 0.433703\n",
      "Train Epoch: 55 [15360/50016 (31%)]\tLoss: 0.335972\n",
      "Train Epoch: 55 [20480/50016 (41%)]\tLoss: 0.250414\n",
      "Train Epoch: 55 [25600/50016 (51%)]\tLoss: 0.333045\n",
      "Train Epoch: 55 [30720/50016 (61%)]\tLoss: 0.555518\n",
      "Train Epoch: 55 [35840/50016 (72%)]\tLoss: 0.404393\n",
      "Train Epoch: 55 [40960/50016 (82%)]\tLoss: 0.089769\n",
      "Train Epoch: 55 [46080/50016 (92%)]\tLoss: 0.599427\n",
      "Test Epoch: 55 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 55 [2048/10016 (20%)]\tAcc: 0.838708\n",
      "Test Epoch: 55 [4096/10016 (41%)]\tAcc: 0.745997\n",
      "Test Epoch: 55 [6144/10016 (61%)]\tAcc: 0.711721\n",
      "Test Epoch: 55 [8192/10016 (82%)]\tAcc: 0.714682\n",
      "Total test acc = 0.8352619139861827\n",
      "epoch:  56\n",
      "Train Epoch: 56 [5120/50016 (10%)]\tLoss: 0.264388\n",
      "Train Epoch: 56 [10240/50016 (20%)]\tLoss: 0.423480\n",
      "Train Epoch: 56 [15360/50016 (31%)]\tLoss: 0.387674\n",
      "Train Epoch: 56 [20480/50016 (41%)]\tLoss: 0.354977\n",
      "Train Epoch: 56 [25600/50016 (51%)]\tLoss: 0.265106\n",
      "Train Epoch: 56 [30720/50016 (61%)]\tLoss: 0.327771\n",
      "Train Epoch: 56 [35840/50016 (72%)]\tLoss: 0.333968\n",
      "Train Epoch: 56 [40960/50016 (82%)]\tLoss: 0.699449\n",
      "Train Epoch: 56 [46080/50016 (92%)]\tLoss: 0.562535\n",
      "Test Epoch: 56 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 56 [2048/10016 (20%)]\tAcc: 0.897334\n",
      "Test Epoch: 56 [4096/10016 (41%)]\tAcc: 0.933496\n",
      "Test Epoch: 56 [6144/10016 (61%)]\tAcc: 0.681573\n",
      "Test Epoch: 56 [8192/10016 (82%)]\tAcc: 0.836785\n",
      "Total test acc = 0.8339132218953464\n",
      "epoch:  57\n",
      "Train Epoch: 57 [5120/50016 (10%)]\tLoss: 0.507819\n",
      "Train Epoch: 57 [10240/50016 (20%)]\tLoss: 0.413285\n",
      "Train Epoch: 57 [15360/50016 (31%)]\tLoss: 0.374359\n",
      "Train Epoch: 57 [20480/50016 (41%)]\tLoss: 0.399804\n",
      "Train Epoch: 57 [25600/50016 (51%)]\tLoss: 0.119597\n",
      "Train Epoch: 57 [30720/50016 (61%)]\tLoss: 0.288813\n",
      "Train Epoch: 57 [35840/50016 (72%)]\tLoss: 0.402791\n",
      "Train Epoch: 57 [40960/50016 (82%)]\tLoss: 0.502799\n",
      "Train Epoch: 57 [46080/50016 (92%)]\tLoss: 0.411707\n",
      "Test Epoch: 57 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 57 [2048/10016 (20%)]\tAcc: 0.835658\n",
      "Test Epoch: 57 [4096/10016 (41%)]\tAcc: 0.804559\n",
      "Test Epoch: 57 [6144/10016 (61%)]\tAcc: 0.839718\n",
      "Test Epoch: 57 [8192/10016 (82%)]\tAcc: 0.868098\n",
      "Total test acc = 0.8348520223438923\n",
      "epoch:  58\n",
      "Train Epoch: 58 [5120/50016 (10%)]\tLoss: 0.164097\n",
      "Train Epoch: 58 [10240/50016 (20%)]\tLoss: 0.356139\n",
      "Train Epoch: 58 [15360/50016 (31%)]\tLoss: 0.253874\n",
      "Train Epoch: 58 [20480/50016 (41%)]\tLoss: 0.407103\n",
      "Train Epoch: 58 [25600/50016 (51%)]\tLoss: 0.217808\n",
      "Train Epoch: 58 [30720/50016 (61%)]\tLoss: 0.425443\n",
      "Train Epoch: 58 [35840/50016 (72%)]\tLoss: 0.286724\n",
      "Train Epoch: 58 [40960/50016 (82%)]\tLoss: 0.238461\n",
      "Train Epoch: 58 [46080/50016 (92%)]\tLoss: 0.295184\n",
      "Test Epoch: 58 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 58 [2048/10016 (20%)]\tAcc: 0.838804\n",
      "Test Epoch: 58 [4096/10016 (41%)]\tAcc: 0.745904\n",
      "Test Epoch: 58 [6144/10016 (61%)]\tAcc: 0.870995\n",
      "Test Epoch: 58 [8192/10016 (82%)]\tAcc: 0.810447\n",
      "Total test acc = 0.8327814582279943\n",
      "epoch:  59\n",
      "Train Epoch: 59 [5120/50016 (10%)]\tLoss: 0.256685\n",
      "Train Epoch: 59 [10240/50016 (20%)]\tLoss: 0.481919\n",
      "Train Epoch: 59 [15360/50016 (31%)]\tLoss: 0.370168\n",
      "Train Epoch: 59 [20480/50016 (41%)]\tLoss: 0.443105\n",
      "Train Epoch: 59 [25600/50016 (51%)]\tLoss: 0.224964\n",
      "Train Epoch: 59 [30720/50016 (61%)]\tLoss: 0.412443\n",
      "Train Epoch: 59 [35840/50016 (72%)]\tLoss: 0.365084\n",
      "Train Epoch: 59 [40960/50016 (82%)]\tLoss: 0.174838\n",
      "Train Epoch: 59 [46080/50016 (92%)]\tLoss: 0.314891\n",
      "Test Epoch: 59 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 59 [2048/10016 (20%)]\tAcc: 0.811336\n",
      "Test Epoch: 59 [4096/10016 (41%)]\tAcc: 0.778164\n",
      "Test Epoch: 59 [6144/10016 (61%)]\tAcc: 0.773187\n",
      "Test Epoch: 59 [8192/10016 (82%)]\tAcc: 0.899286\n",
      "Total test acc = 0.8389684573364053\n",
      "epoch:  60\n",
      "Train Epoch: 60 [5120/50016 (10%)]\tLoss: 0.239665\n",
      "Train Epoch: 60 [10240/50016 (20%)]\tLoss: 0.313151\n",
      "Train Epoch: 60 [15360/50016 (31%)]\tLoss: 0.222912\n",
      "Train Epoch: 60 [20480/50016 (41%)]\tLoss: 0.150513\n",
      "Train Epoch: 60 [25600/50016 (51%)]\tLoss: 0.402814\n",
      "Train Epoch: 60 [30720/50016 (61%)]\tLoss: 0.297837\n",
      "Train Epoch: 60 [35840/50016 (72%)]\tLoss: 0.207827\n",
      "Train Epoch: 60 [40960/50016 (82%)]\tLoss: 0.177059\n",
      "Train Epoch: 60 [46080/50016 (92%)]\tLoss: 0.184708\n",
      "Test Epoch: 60 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 60 [2048/10016 (20%)]\tAcc: 0.772331\n",
      "Test Epoch: 60 [4096/10016 (41%)]\tAcc: 0.838741\n",
      "Test Epoch: 60 [6144/10016 (61%)]\tAcc: 0.683491\n",
      "Test Epoch: 60 [8192/10016 (82%)]\tAcc: 0.900385\n",
      "Total test acc = 0.8383585785102728\n",
      "epoch:  61\n",
      "Train Epoch: 61 [5120/50016 (10%)]\tLoss: 0.459517\n",
      "Train Epoch: 61 [10240/50016 (20%)]\tLoss: 0.598931\n",
      "Train Epoch: 61 [15360/50016 (31%)]\tLoss: 0.216929\n",
      "Train Epoch: 61 [20480/50016 (41%)]\tLoss: 0.456051\n",
      "Train Epoch: 61 [25600/50016 (51%)]\tLoss: 0.313810\n",
      "Train Epoch: 61 [30720/50016 (61%)]\tLoss: 0.375250\n",
      "Train Epoch: 61 [35840/50016 (72%)]\tLoss: 0.207074\n",
      "Train Epoch: 61 [40960/50016 (82%)]\tLoss: 0.286767\n",
      "Train Epoch: 61 [46080/50016 (92%)]\tLoss: 0.419377\n",
      "Test Epoch: 61 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 61 [2048/10016 (20%)]\tAcc: 0.835809\n",
      "Test Epoch: 61 [4096/10016 (41%)]\tAcc: 0.558374\n",
      "Test Epoch: 61 [6144/10016 (61%)]\tAcc: 0.776178\n",
      "Test Epoch: 61 [8192/10016 (82%)]\tAcc: 0.837789\n",
      "Total test acc = 0.8403100935573807\n",
      "epoch:  62\n",
      "Train Epoch: 62 [5120/50016 (10%)]\tLoss: 0.295064\n",
      "Train Epoch: 62 [10240/50016 (20%)]\tLoss: 0.492354\n",
      "Train Epoch: 62 [15360/50016 (31%)]\tLoss: 0.256045\n",
      "Train Epoch: 62 [20480/50016 (41%)]\tLoss: 0.359201\n",
      "Train Epoch: 62 [25600/50016 (51%)]\tLoss: 0.432691\n",
      "Train Epoch: 62 [30720/50016 (61%)]\tLoss: 0.335308\n",
      "Train Epoch: 62 [35840/50016 (72%)]\tLoss: 0.195939\n",
      "Train Epoch: 62 [40960/50016 (82%)]\tLoss: 0.655307\n",
      "Train Epoch: 62 [46080/50016 (92%)]\tLoss: 0.299276\n",
      "Test Epoch: 62 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 62 [2048/10016 (20%)]\tAcc: 0.933345\n",
      "Test Epoch: 62 [4096/10016 (41%)]\tAcc: 0.810419\n",
      "Test Epoch: 62 [6144/10016 (61%)]\tAcc: 0.651296\n",
      "Test Epoch: 62 [8192/10016 (82%)]\tAcc: 0.807427\n",
      "Total test acc = 0.8394908398170806\n",
      "epoch:  63\n",
      "Train Epoch: 63 [5120/50016 (10%)]\tLoss: 0.291246\n",
      "Train Epoch: 63 [10240/50016 (20%)]\tLoss: 0.241445\n",
      "Train Epoch: 63 [15360/50016 (31%)]\tLoss: 0.399948\n",
      "Train Epoch: 63 [20480/50016 (41%)]\tLoss: 0.238353\n",
      "Train Epoch: 63 [25600/50016 (51%)]\tLoss: 0.626350\n",
      "Train Epoch: 63 [30720/50016 (61%)]\tLoss: 0.495352\n",
      "Train Epoch: 63 [35840/50016 (72%)]\tLoss: 0.374911\n",
      "Train Epoch: 63 [40960/50016 (82%)]\tLoss: 0.298567\n",
      "Train Epoch: 63 [46080/50016 (92%)]\tLoss: 0.663575\n",
      "Test Epoch: 63 [0/10016 (0%)]\tAcc: 0.968750\n",
      "Test Epoch: 63 [2048/10016 (20%)]\tAcc: 0.838737\n",
      "Test Epoch: 63 [4096/10016 (41%)]\tAcc: 0.777887\n",
      "Test Epoch: 63 [6144/10016 (61%)]\tAcc: 0.778222\n",
      "Test Epoch: 63 [8192/10016 (82%)]\tAcc: 0.810298\n",
      "Total test acc = 0.8374263265430122\n",
      "epoch:  64\n",
      "Train Epoch: 64 [5120/50016 (10%)]\tLoss: 0.333167\n",
      "Train Epoch: 64 [10240/50016 (20%)]\tLoss: 0.313819\n",
      "Train Epoch: 64 [15360/50016 (31%)]\tLoss: 0.322657\n",
      "Train Epoch: 64 [20480/50016 (41%)]\tLoss: 0.545763\n",
      "Train Epoch: 64 [25600/50016 (51%)]\tLoss: 0.736096\n",
      "Train Epoch: 64 [30720/50016 (61%)]\tLoss: 0.221219\n",
      "Train Epoch: 64 [35840/50016 (72%)]\tLoss: 0.304822\n",
      "Train Epoch: 64 [40960/50016 (82%)]\tLoss: 0.314591\n",
      "Train Epoch: 64 [46080/50016 (92%)]\tLoss: 0.305188\n",
      "Test Epoch: 64 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 64 [2048/10016 (20%)]\tAcc: 0.900326\n",
      "Test Epoch: 64 [4096/10016 (41%)]\tAcc: 0.714719\n",
      "Test Epoch: 64 [6144/10016 (61%)]\tAcc: 0.744041\n",
      "Test Epoch: 64 [8192/10016 (82%)]\tAcc: 0.872946\n",
      "Total test acc = 0.8371100871619015\n",
      "epoch:  65\n",
      "Train Epoch: 65 [5120/50016 (10%)]\tLoss: 0.257114\n",
      "Train Epoch: 65 [10240/50016 (20%)]\tLoss: 0.297650\n",
      "Train Epoch: 65 [15360/50016 (31%)]\tLoss: 0.347278\n",
      "Train Epoch: 65 [20480/50016 (41%)]\tLoss: 0.363218\n",
      "Train Epoch: 65 [25600/50016 (51%)]\tLoss: 0.180728\n",
      "Train Epoch: 65 [30720/50016 (61%)]\tLoss: 0.259753\n",
      "Train Epoch: 65 [35840/50016 (72%)]\tLoss: 0.416048\n",
      "Train Epoch: 65 [40960/50016 (82%)]\tLoss: 0.392392\n",
      "Train Epoch: 65 [46080/50016 (92%)]\tLoss: 0.494735\n",
      "Test Epoch: 65 [0/10016 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 65 [2048/10016 (20%)]\tAcc: 0.932246\n",
      "Test Epoch: 65 [4096/10016 (41%)]\tAcc: 0.900233\n",
      "Test Epoch: 65 [6144/10016 (61%)]\tAcc: 0.743064\n",
      "Test Epoch: 65 [8192/10016 (82%)]\tAcc: 0.842737\n",
      "Total test acc = 0.8474363032317147\n",
      "epoch:  66\n",
      "Train Epoch: 66 [5120/50016 (10%)]\tLoss: 0.381697\n",
      "Train Epoch: 66 [10240/50016 (20%)]\tLoss: 0.509855\n",
      "Train Epoch: 66 [15360/50016 (31%)]\tLoss: 0.242641\n",
      "Train Epoch: 66 [20480/50016 (41%)]\tLoss: 0.392939\n",
      "Train Epoch: 66 [25600/50016 (51%)]\tLoss: 0.385111\n",
      "Train Epoch: 66 [30720/50016 (61%)]\tLoss: 0.332640\n",
      "Train Epoch: 66 [35840/50016 (72%)]\tLoss: 0.369068\n",
      "Train Epoch: 66 [40960/50016 (82%)]\tLoss: 0.284947\n",
      "Train Epoch: 66 [46080/50016 (92%)]\tLoss: 0.433022\n",
      "Test Epoch: 66 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 66 [2048/10016 (20%)]\tAcc: 0.933405\n",
      "Test Epoch: 66 [4096/10016 (41%)]\tAcc: 0.874903\n",
      "Test Epoch: 66 [6144/10016 (61%)]\tAcc: 0.712700\n",
      "Test Epoch: 66 [8192/10016 (82%)]\tAcc: 0.836785\n",
      "Total test acc = 0.8521813510161461\n",
      "epoch:  67\n",
      "Train Epoch: 67 [5120/50016 (10%)]\tLoss: 0.209665\n",
      "Train Epoch: 67 [10240/50016 (20%)]\tLoss: 0.247682\n",
      "Train Epoch: 67 [15360/50016 (31%)]\tLoss: 0.309790\n",
      "Train Epoch: 67 [20480/50016 (41%)]\tLoss: 0.479635\n",
      "Train Epoch: 67 [25600/50016 (51%)]\tLoss: 0.180713\n",
      "Train Epoch: 67 [30720/50016 (61%)]\tLoss: 0.290308\n",
      "Train Epoch: 67 [35840/50016 (72%)]\tLoss: 0.205676\n",
      "Train Epoch: 67 [40960/50016 (82%)]\tLoss: 0.490922\n",
      "Train Epoch: 67 [46080/50016 (92%)]\tLoss: 0.404838\n",
      "Test Epoch: 67 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 67 [2048/10016 (20%)]\tAcc: 0.962764\n",
      "Test Epoch: 67 [4096/10016 (41%)]\tAcc: 0.872763\n",
      "Test Epoch: 67 [6144/10016 (61%)]\tAcc: 0.835871\n",
      "Test Epoch: 67 [8192/10016 (82%)]\tAcc: 0.839747\n",
      "Total test acc = 0.8374295495012292\n",
      "epoch:  68\n",
      "Train Epoch: 68 [5120/50016 (10%)]\tLoss: 0.336958\n",
      "Train Epoch: 68 [10240/50016 (20%)]\tLoss: 0.177533\n",
      "Train Epoch: 68 [15360/50016 (31%)]\tLoss: 0.222409\n",
      "Train Epoch: 68 [20480/50016 (41%)]\tLoss: 0.587657\n",
      "Train Epoch: 68 [25600/50016 (51%)]\tLoss: 0.104310\n",
      "Train Epoch: 68 [30720/50016 (61%)]\tLoss: 0.341027\n",
      "Train Epoch: 68 [35840/50016 (72%)]\tLoss: 0.409502\n",
      "Train Epoch: 68 [40960/50016 (82%)]\tLoss: 0.190539\n",
      "Train Epoch: 68 [46080/50016 (92%)]\tLoss: 0.512331\n",
      "Test Epoch: 68 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 68 [2048/10016 (20%)]\tAcc: 0.868097\n",
      "Test Epoch: 68 [4096/10016 (41%)]\tAcc: 0.744960\n",
      "Test Epoch: 68 [6144/10016 (61%)]\tAcc: 0.775262\n",
      "Test Epoch: 68 [8192/10016 (82%)]\tAcc: 0.930384\n",
      "Total test acc = 0.8457909401403558\n",
      "epoch:  69\n",
      "Train Epoch: 69 [5120/50016 (10%)]\tLoss: 0.443085\n",
      "Train Epoch: 69 [10240/50016 (20%)]\tLoss: 0.576385\n",
      "Train Epoch: 69 [15360/50016 (31%)]\tLoss: 0.305901\n",
      "Train Epoch: 69 [20480/50016 (41%)]\tLoss: 0.743365\n",
      "Train Epoch: 69 [25600/50016 (51%)]\tLoss: 0.373076\n",
      "Train Epoch: 69 [30720/50016 (61%)]\tLoss: 0.686541\n",
      "Train Epoch: 69 [35840/50016 (72%)]\tLoss: 0.645772\n",
      "Train Epoch: 69 [40960/50016 (82%)]\tLoss: 0.379729\n",
      "Train Epoch: 69 [46080/50016 (92%)]\tLoss: 0.401146\n",
      "Test Epoch: 69 [0/10016 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 69 [2048/10016 (20%)]\tAcc: 0.836844\n",
      "Test Epoch: 69 [4096/10016 (41%)]\tAcc: 0.869136\n",
      "Test Epoch: 69 [6144/10016 (61%)]\tAcc: 0.774220\n",
      "Test Epoch: 69 [8192/10016 (82%)]\tAcc: 0.840603\n",
      "Total test acc = 0.8470263396360489\n",
      "epoch:  70\n",
      "Train Epoch: 70 [5120/50016 (10%)]\tLoss: 0.349260\n",
      "Train Epoch: 70 [10240/50016 (20%)]\tLoss: 0.430512\n",
      "Train Epoch: 70 [15360/50016 (31%)]\tLoss: 0.596126\n",
      "Train Epoch: 70 [20480/50016 (41%)]\tLoss: 0.354372\n",
      "Train Epoch: 70 [25600/50016 (51%)]\tLoss: 0.407955\n",
      "Train Epoch: 70 [30720/50016 (61%)]\tLoss: 0.406038\n",
      "Train Epoch: 70 [35840/50016 (72%)]\tLoss: 0.258551\n",
      "Train Epoch: 70 [40960/50016 (82%)]\tLoss: 0.469903\n",
      "Train Epoch: 70 [46080/50016 (92%)]\tLoss: 0.374814\n",
      "Test Epoch: 70 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 70 [2048/10016 (20%)]\tAcc: 0.867121\n",
      "Test Epoch: 70 [4096/10016 (41%)]\tAcc: 0.808467\n",
      "Test Epoch: 70 [6144/10016 (61%)]\tAcc: 0.743097\n",
      "Test Epoch: 70 [8192/10016 (82%)]\tAcc: 0.807525\n",
      "Total test acc = 0.8432168510408378\n",
      "epoch:  71\n",
      "Train Epoch: 71 [5120/50016 (10%)]\tLoss: 0.513434\n",
      "Train Epoch: 71 [10240/50016 (20%)]\tLoss: 0.348732\n",
      "Train Epoch: 71 [15360/50016 (31%)]\tLoss: 0.326769\n",
      "Train Epoch: 71 [20480/50016 (41%)]\tLoss: 0.079089\n",
      "Train Epoch: 71 [25600/50016 (51%)]\tLoss: 0.500728\n",
      "Train Epoch: 71 [30720/50016 (61%)]\tLoss: 0.300775\n",
      "Train Epoch: 71 [35840/50016 (72%)]\tLoss: 0.550985\n",
      "Train Epoch: 71 [40960/50016 (82%)]\tLoss: 0.289605\n",
      "Train Epoch: 71 [46080/50016 (92%)]\tLoss: 0.488487\n",
      "Test Epoch: 71 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 71 [2048/10016 (20%)]\tAcc: 0.709801\n",
      "Test Epoch: 71 [4096/10016 (41%)]\tAcc: 0.902218\n",
      "Test Epoch: 71 [6144/10016 (61%)]\tAcc: 0.746942\n",
      "Test Epoch: 71 [8192/10016 (82%)]\tAcc: 0.745051\n",
      "Total test acc = 0.850848812602139\n",
      "epoch:  72\n",
      "Train Epoch: 72 [5120/50016 (10%)]\tLoss: 0.348852\n",
      "Train Epoch: 72 [10240/50016 (20%)]\tLoss: 0.430355\n",
      "Train Epoch: 72 [15360/50016 (31%)]\tLoss: 0.110721\n",
      "Train Epoch: 72 [20480/50016 (41%)]\tLoss: 0.336653\n",
      "Train Epoch: 72 [25600/50016 (51%)]\tLoss: 0.441415\n",
      "Train Epoch: 72 [30720/50016 (61%)]\tLoss: 0.489525\n",
      "Train Epoch: 72 [35840/50016 (72%)]\tLoss: 0.134980\n",
      "Train Epoch: 72 [40960/50016 (82%)]\tLoss: 0.253774\n",
      "Train Epoch: 72 [46080/50016 (92%)]\tLoss: 0.252469\n",
      "Test Epoch: 72 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 72 [2048/10016 (20%)]\tAcc: 0.871854\n",
      "Test Epoch: 72 [4096/10016 (41%)]\tAcc: 0.775321\n",
      "Test Epoch: 72 [6144/10016 (61%)]\tAcc: 0.715722\n",
      "Test Epoch: 72 [8192/10016 (82%)]\tAcc: 0.810450\n",
      "Total test acc = 0.8473295586619856\n",
      "epoch:  73\n",
      "Train Epoch: 73 [5120/50016 (10%)]\tLoss: 0.276814\n",
      "Train Epoch: 73 [10240/50016 (20%)]\tLoss: 0.100496\n",
      "Train Epoch: 73 [15360/50016 (31%)]\tLoss: 0.298259\n",
      "Train Epoch: 73 [20480/50016 (41%)]\tLoss: 0.276301\n",
      "Train Epoch: 73 [25600/50016 (51%)]\tLoss: 0.366464\n",
      "Train Epoch: 73 [30720/50016 (61%)]\tLoss: 0.195475\n",
      "Train Epoch: 73 [35840/50016 (72%)]\tLoss: 0.281725\n",
      "Train Epoch: 73 [40960/50016 (82%)]\tLoss: 0.369926\n",
      "Train Epoch: 73 [46080/50016 (92%)]\tLoss: 0.203388\n",
      "Test Epoch: 73 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 73 [2048/10016 (20%)]\tAcc: 0.839717\n",
      "Test Epoch: 73 [4096/10016 (41%)]\tAcc: 0.870966\n",
      "Test Epoch: 73 [6144/10016 (61%)]\tAcc: 0.870050\n",
      "Test Epoch: 73 [8192/10016 (82%)]\tAcc: 0.868982\n",
      "Total test acc = 0.848877835496224\n",
      "epoch:  74\n",
      "Train Epoch: 74 [5120/50016 (10%)]\tLoss: 0.354575\n",
      "Train Epoch: 74 [10240/50016 (20%)]\tLoss: 0.081381\n",
      "Train Epoch: 74 [15360/50016 (31%)]\tLoss: 0.226534\n",
      "Train Epoch: 74 [20480/50016 (41%)]\tLoss: 0.319404\n",
      "Train Epoch: 74 [25600/50016 (51%)]\tLoss: 0.335191\n",
      "Train Epoch: 74 [30720/50016 (61%)]\tLoss: 0.445422\n",
      "Train Epoch: 74 [35840/50016 (72%)]\tLoss: 0.647574\n",
      "Train Epoch: 74 [40960/50016 (82%)]\tLoss: 0.578884\n",
      "Train Epoch: 74 [46080/50016 (92%)]\tLoss: 0.519085\n",
      "Test Epoch: 74 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 74 [2048/10016 (20%)]\tAcc: 0.928464\n",
      "Test Epoch: 74 [4096/10016 (41%)]\tAcc: 0.778102\n",
      "Test Epoch: 74 [6144/10016 (61%)]\tAcc: 0.772393\n",
      "Test Epoch: 74 [8192/10016 (82%)]\tAcc: 0.903224\n",
      "Total test acc = 0.8448621065300554\n",
      "epoch:  75\n",
      "Train Epoch: 75 [5120/50016 (10%)]\tLoss: 0.251864\n",
      "Train Epoch: 75 [10240/50016 (20%)]\tLoss: 0.235318\n",
      "Train Epoch: 75 [15360/50016 (31%)]\tLoss: 0.169376\n",
      "Train Epoch: 75 [20480/50016 (41%)]\tLoss: 0.325725\n",
      "Train Epoch: 75 [25600/50016 (51%)]\tLoss: 0.370956\n",
      "Train Epoch: 75 [30720/50016 (61%)]\tLoss: 0.202903\n",
      "Train Epoch: 75 [35840/50016 (72%)]\tLoss: 0.422382\n",
      "Train Epoch: 75 [40960/50016 (82%)]\tLoss: 0.202611\n",
      "Train Epoch: 75 [46080/50016 (92%)]\tLoss: 0.295433\n",
      "Test Epoch: 75 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 75 [2048/10016 (20%)]\tAcc: 0.836881\n",
      "Test Epoch: 75 [4096/10016 (41%)]\tAcc: 0.866993\n",
      "Test Epoch: 75 [6144/10016 (61%)]\tAcc: 0.714714\n",
      "Test Epoch: 75 [8192/10016 (82%)]\tAcc: 0.865106\n",
      "Total test acc = 0.8484712866171982\n",
      "epoch:  76\n",
      "Train Epoch: 76 [5120/50016 (10%)]\tLoss: 0.461212\n",
      "Train Epoch: 76 [10240/50016 (20%)]\tLoss: 0.412823\n",
      "Train Epoch: 76 [15360/50016 (31%)]\tLoss: 0.575484\n",
      "Train Epoch: 76 [20480/50016 (41%)]\tLoss: 0.227060\n",
      "Train Epoch: 76 [25600/50016 (51%)]\tLoss: 0.286210\n",
      "Train Epoch: 76 [30720/50016 (61%)]\tLoss: 0.428449\n",
      "Train Epoch: 76 [35840/50016 (72%)]\tLoss: 0.309132\n",
      "Train Epoch: 76 [40960/50016 (82%)]\tLoss: 0.367489\n",
      "Train Epoch: 76 [46080/50016 (92%)]\tLoss: 0.260330\n",
      "Test Epoch: 76 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 76 [2048/10016 (20%)]\tAcc: 0.899228\n",
      "Test Epoch: 76 [4096/10016 (41%)]\tAcc: 0.901179\n",
      "Test Epoch: 76 [6144/10016 (61%)]\tAcc: 0.679558\n",
      "Test Epoch: 76 [8192/10016 (82%)]\tAcc: 0.870018\n",
      "Total test acc = 0.8511554503632311\n",
      "epoch:  77\n",
      "Train Epoch: 77 [5120/50016 (10%)]\tLoss: 0.197626\n",
      "Train Epoch: 77 [10240/50016 (20%)]\tLoss: 0.463263\n",
      "Train Epoch: 77 [15360/50016 (31%)]\tLoss: 0.269445\n",
      "Train Epoch: 77 [20480/50016 (41%)]\tLoss: 0.436934\n",
      "Train Epoch: 77 [25600/50016 (51%)]\tLoss: 0.175793\n",
      "Train Epoch: 77 [30720/50016 (61%)]\tLoss: 0.282588\n",
      "Train Epoch: 77 [35840/50016 (72%)]\tLoss: 0.680455\n",
      "Train Epoch: 77 [40960/50016 (82%)]\tLoss: 0.370409\n",
      "Train Epoch: 77 [46080/50016 (92%)]\tLoss: 0.424235\n",
      "Test Epoch: 77 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 77 [2048/10016 (20%)]\tAcc: 0.866081\n",
      "Test Epoch: 77 [4096/10016 (41%)]\tAcc: 0.778101\n",
      "Test Epoch: 77 [6144/10016 (61%)]\tAcc: 0.680445\n",
      "Test Epoch: 77 [8192/10016 (82%)]\tAcc: 0.809411\n",
      "Total test acc = 0.8459940748715713\n",
      "epoch:  78\n",
      "Train Epoch: 78 [5120/50016 (10%)]\tLoss: 0.278950\n",
      "Train Epoch: 78 [10240/50016 (20%)]\tLoss: 0.335497\n",
      "Train Epoch: 78 [15360/50016 (31%)]\tLoss: 0.412620\n",
      "Train Epoch: 78 [20480/50016 (41%)]\tLoss: 0.334211\n",
      "Train Epoch: 78 [25600/50016 (51%)]\tLoss: 0.254128\n",
      "Train Epoch: 78 [30720/50016 (61%)]\tLoss: 0.284738\n",
      "Train Epoch: 78 [35840/50016 (72%)]\tLoss: 0.374092\n",
      "Train Epoch: 78 [40960/50016 (82%)]\tLoss: 0.261497\n",
      "Train Epoch: 78 [46080/50016 (92%)]\tLoss: 0.201985\n",
      "Test Epoch: 78 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 78 [2048/10016 (20%)]\tAcc: 0.900294\n",
      "Test Epoch: 78 [4096/10016 (41%)]\tAcc: 0.778192\n",
      "Test Epoch: 78 [6144/10016 (61%)]\tAcc: 0.807551\n",
      "Test Epoch: 78 [8192/10016 (82%)]\tAcc: 0.898311\n",
      "Total test acc = 0.8456841769408103\n",
      "epoch:  79\n",
      "Train Epoch: 79 [5120/50016 (10%)]\tLoss: 0.072750\n",
      "Train Epoch: 79 [10240/50016 (20%)]\tLoss: 0.615347\n",
      "Train Epoch: 79 [15360/50016 (31%)]\tLoss: 0.092877\n",
      "Train Epoch: 79 [20480/50016 (41%)]\tLoss: 0.225410\n",
      "Train Epoch: 79 [25600/50016 (51%)]\tLoss: 0.243343\n",
      "Train Epoch: 79 [30720/50016 (61%)]\tLoss: 0.144643\n",
      "Train Epoch: 79 [35840/50016 (72%)]\tLoss: 0.401270\n",
      "Train Epoch: 79 [40960/50016 (82%)]\tLoss: 0.326786\n",
      "Train Epoch: 79 [46080/50016 (92%)]\tLoss: 0.280116\n",
      "Test Epoch: 79 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 79 [2048/10016 (20%)]\tAcc: 0.809412\n",
      "Test Epoch: 79 [4096/10016 (41%)]\tAcc: 0.966668\n",
      "Test Epoch: 79 [6144/10016 (61%)]\tAcc: 0.681606\n",
      "Test Epoch: 79 [8192/10016 (82%)]\tAcc: 0.838769\n",
      "Total test acc = 0.8529101911120959\n",
      "epoch:  80\n",
      "Train Epoch: 80 [5120/50016 (10%)]\tLoss: 0.121589\n",
      "Train Epoch: 80 [10240/50016 (20%)]\tLoss: 0.322585\n",
      "Train Epoch: 80 [15360/50016 (31%)]\tLoss: 0.346179\n",
      "Train Epoch: 80 [20480/50016 (41%)]\tLoss: 0.264688\n",
      "Train Epoch: 80 [25600/50016 (51%)]\tLoss: 0.339243\n",
      "Train Epoch: 80 [30720/50016 (61%)]\tLoss: 0.480407\n",
      "Train Epoch: 80 [35840/50016 (72%)]\tLoss: 0.239724\n",
      "Train Epoch: 80 [40960/50016 (82%)]\tLoss: 0.379201\n",
      "Train Epoch: 80 [46080/50016 (92%)]\tLoss: 0.313442\n",
      "Test Epoch: 80 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 80 [2048/10016 (20%)]\tAcc: 0.837856\n",
      "Test Epoch: 80 [4096/10016 (41%)]\tAcc: 0.901179\n",
      "Test Epoch: 80 [6144/10016 (61%)]\tAcc: 0.776272\n",
      "Test Epoch: 80 [8192/10016 (82%)]\tAcc: 0.933560\n",
      "Total test acc = 0.8504296500117784\n",
      "epoch:  81\n",
      "Train Epoch: 81 [5120/50016 (10%)]\tLoss: 0.467699\n",
      "Train Epoch: 81 [10240/50016 (20%)]\tLoss: 0.095887\n",
      "Train Epoch: 81 [15360/50016 (31%)]\tLoss: 0.263416\n",
      "Train Epoch: 81 [20480/50016 (41%)]\tLoss: 0.331262\n",
      "Train Epoch: 81 [25600/50016 (51%)]\tLoss: 0.232365\n",
      "Train Epoch: 81 [30720/50016 (61%)]\tLoss: 0.154687\n",
      "Train Epoch: 81 [35840/50016 (72%)]\tLoss: 0.537893\n",
      "Train Epoch: 81 [40960/50016 (82%)]\tLoss: 0.398546\n",
      "Train Epoch: 81 [46080/50016 (92%)]\tLoss: 0.351342\n",
      "Test Epoch: 81 [0/10016 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 81 [2048/10016 (20%)]\tAcc: 0.900233\n",
      "Test Epoch: 81 [4096/10016 (41%)]\tAcc: 0.872949\n",
      "Test Epoch: 81 [6144/10016 (61%)]\tAcc: 0.739193\n",
      "Test Epoch: 81 [8192/10016 (82%)]\tAcc: 0.902096\n",
      "Total test acc = 0.8548684576409656\n",
      "epoch:  82\n",
      "Train Epoch: 82 [5120/50016 (10%)]\tLoss: 0.414029\n",
      "Train Epoch: 82 [10240/50016 (20%)]\tLoss: 0.303655\n",
      "Train Epoch: 82 [15360/50016 (31%)]\tLoss: 0.497122\n",
      "Train Epoch: 82 [20480/50016 (41%)]\tLoss: 0.405132\n",
      "Train Epoch: 82 [25600/50016 (51%)]\tLoss: 0.271442\n",
      "Train Epoch: 82 [30720/50016 (61%)]\tLoss: 0.310922\n",
      "Train Epoch: 82 [35840/50016 (72%)]\tLoss: 0.189336\n",
      "Train Epoch: 82 [40960/50016 (82%)]\tLoss: 0.118068\n",
      "Train Epoch: 82 [46080/50016 (92%)]\tLoss: 0.342904\n",
      "Test Epoch: 82 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 82 [2048/10016 (20%)]\tAcc: 0.804562\n",
      "Test Epoch: 82 [4096/10016 (41%)]\tAcc: 0.807493\n",
      "Test Epoch: 82 [6144/10016 (61%)]\tAcc: 0.712821\n",
      "Test Epoch: 82 [8192/10016 (82%)]\tAcc: 0.776177\n",
      "Total test acc = 0.8422719856727714\n",
      "epoch:  83\n",
      "Train Epoch: 83 [5120/50016 (10%)]\tLoss: 0.276525\n",
      "Train Epoch: 83 [10240/50016 (20%)]\tLoss: 0.322524\n",
      "Train Epoch: 83 [15360/50016 (31%)]\tLoss: 0.245191\n",
      "Train Epoch: 83 [20480/50016 (41%)]\tLoss: 0.319040\n",
      "Train Epoch: 83 [25600/50016 (51%)]\tLoss: 0.601081\n",
      "Train Epoch: 83 [30720/50016 (61%)]\tLoss: 0.206502\n",
      "Train Epoch: 83 [35840/50016 (72%)]\tLoss: 0.334121\n",
      "Train Epoch: 83 [40960/50016 (82%)]\tLoss: 0.272488\n",
      "Train Epoch: 83 [46080/50016 (92%)]\tLoss: 0.329317\n",
      "Test Epoch: 83 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 83 [2048/10016 (20%)]\tAcc: 0.805504\n",
      "Test Epoch: 83 [4096/10016 (41%)]\tAcc: 0.902278\n",
      "Test Epoch: 83 [6144/10016 (61%)]\tAcc: 0.807584\n",
      "Test Epoch: 83 [8192/10016 (82%)]\tAcc: 0.777246\n",
      "Total test acc = 0.8472262130622061\n",
      "epoch:  84\n",
      "Train Epoch: 84 [5120/50016 (10%)]\tLoss: 0.311384\n",
      "Train Epoch: 84 [10240/50016 (20%)]\tLoss: 0.143908\n",
      "Train Epoch: 84 [15360/50016 (31%)]\tLoss: 0.182738\n",
      "Train Epoch: 84 [20480/50016 (41%)]\tLoss: 0.353091\n",
      "Train Epoch: 84 [25600/50016 (51%)]\tLoss: 0.164052\n",
      "Train Epoch: 84 [30720/50016 (61%)]\tLoss: 0.190654\n",
      "Train Epoch: 84 [35840/50016 (72%)]\tLoss: 0.882566\n",
      "Train Epoch: 84 [40960/50016 (82%)]\tLoss: 0.328836\n",
      "Train Epoch: 84 [46080/50016 (92%)]\tLoss: 0.181187\n",
      "Test Epoch: 84 [0/10016 (0%)]\tAcc: 0.968750\n",
      "Test Epoch: 84 [2048/10016 (20%)]\tAcc: 0.899378\n",
      "Test Epoch: 84 [4096/10016 (41%)]\tAcc: 0.840692\n",
      "Test Epoch: 84 [6144/10016 (61%)]\tAcc: 0.835873\n",
      "Test Epoch: 84 [8192/10016 (82%)]\tAcc: 0.900326\n",
      "Total test acc = 0.8530101975168127\n",
      "epoch:  85\n",
      "Train Epoch: 85 [5120/50016 (10%)]\tLoss: 0.426593\n",
      "Train Epoch: 85 [10240/50016 (20%)]\tLoss: 0.368847\n",
      "Train Epoch: 85 [15360/50016 (31%)]\tLoss: 0.550185\n",
      "Train Epoch: 85 [20480/50016 (41%)]\tLoss: 0.175319\n",
      "Train Epoch: 85 [25600/50016 (51%)]\tLoss: 0.256843\n",
      "Train Epoch: 85 [30720/50016 (61%)]\tLoss: 0.344962\n",
      "Train Epoch: 85 [35840/50016 (72%)]\tLoss: 0.303460\n",
      "Train Epoch: 85 [40960/50016 (82%)]\tLoss: 0.478089\n",
      "Train Epoch: 85 [46080/50016 (92%)]\tLoss: 0.350947\n",
      "Test Epoch: 85 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 85 [2048/10016 (20%)]\tAcc: 0.866148\n",
      "Test Epoch: 85 [4096/10016 (41%)]\tAcc: 0.869960\n",
      "Test Epoch: 85 [6144/10016 (61%)]\tAcc: 0.774320\n",
      "Test Epoch: 85 [8192/10016 (82%)]\tAcc: 0.838828\n",
      "Total test acc = 0.8523940622887453\n",
      "epoch:  86\n",
      "Train Epoch: 86 [5120/50016 (10%)]\tLoss: 0.202479\n",
      "Train Epoch: 86 [10240/50016 (20%)]\tLoss: 0.196291\n",
      "Train Epoch: 86 [15360/50016 (31%)]\tLoss: 0.245712\n",
      "Train Epoch: 86 [20480/50016 (41%)]\tLoss: 0.382121\n",
      "Train Epoch: 86 [25600/50016 (51%)]\tLoss: 0.167171\n",
      "Train Epoch: 86 [30720/50016 (61%)]\tLoss: 0.250151\n",
      "Train Epoch: 86 [35840/50016 (72%)]\tLoss: 0.159069\n",
      "Train Epoch: 86 [40960/50016 (82%)]\tLoss: 0.453595\n",
      "Train Epoch: 86 [46080/50016 (92%)]\tLoss: 0.297457\n",
      "Test Epoch: 86 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 86 [2048/10016 (20%)]\tAcc: 0.807430\n",
      "Test Epoch: 86 [4096/10016 (41%)]\tAcc: 0.935357\n",
      "Test Epoch: 86 [6144/10016 (61%)]\tAcc: 0.745996\n",
      "Test Epoch: 86 [8192/10016 (82%)]\tAcc: 0.807610\n",
      "Total test acc = 0.8513617033132795\n",
      "epoch:  87\n",
      "Train Epoch: 87 [5120/50016 (10%)]\tLoss: 0.269744\n",
      "Train Epoch: 87 [10240/50016 (20%)]\tLoss: 0.413695\n",
      "Train Epoch: 87 [15360/50016 (31%)]\tLoss: 0.542479\n",
      "Train Epoch: 87 [20480/50016 (41%)]\tLoss: 0.633545\n",
      "Train Epoch: 87 [25600/50016 (51%)]\tLoss: 0.298611\n",
      "Train Epoch: 87 [30720/50016 (61%)]\tLoss: 0.136654\n",
      "Train Epoch: 87 [35840/50016 (72%)]\tLoss: 0.296916\n",
      "Train Epoch: 87 [40960/50016 (82%)]\tLoss: 0.349244\n",
      "Train Epoch: 87 [46080/50016 (92%)]\tLoss: 0.310768\n",
      "Test Epoch: 87 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 87 [2048/10016 (20%)]\tAcc: 0.837793\n",
      "Test Epoch: 87 [4096/10016 (41%)]\tAcc: 0.838712\n",
      "Test Epoch: 87 [6144/10016 (61%)]\tAcc: 0.806545\n",
      "Test Epoch: 87 [8192/10016 (82%)]\tAcc: 0.961818\n",
      "Total test acc = 0.8550815623810427\n",
      "epoch:  88\n",
      "Train Epoch: 88 [5120/50016 (10%)]\tLoss: 0.145469\n",
      "Train Epoch: 88 [10240/50016 (20%)]\tLoss: 0.410097\n",
      "Train Epoch: 88 [15360/50016 (31%)]\tLoss: 0.246548\n",
      "Train Epoch: 88 [20480/50016 (41%)]\tLoss: 0.460363\n",
      "Train Epoch: 88 [25600/50016 (51%)]\tLoss: 0.489154\n",
      "Train Epoch: 88 [30720/50016 (61%)]\tLoss: 0.273279\n",
      "Train Epoch: 88 [35840/50016 (72%)]\tLoss: 0.298374\n",
      "Train Epoch: 88 [40960/50016 (82%)]\tLoss: 0.530919\n",
      "Train Epoch: 88 [46080/50016 (92%)]\tLoss: 0.258903\n",
      "Test Epoch: 88 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 88 [2048/10016 (20%)]\tAcc: 0.806574\n",
      "Test Epoch: 88 [4096/10016 (41%)]\tAcc: 0.779197\n",
      "Test Epoch: 88 [6144/10016 (61%)]\tAcc: 0.869989\n",
      "Test Epoch: 88 [8192/10016 (82%)]\tAcc: 0.747889\n",
      "Total test acc = 0.8524972880861623\n",
      "epoch:  89\n",
      "Train Epoch: 89 [5120/50016 (10%)]\tLoss: 0.182448\n",
      "Train Epoch: 89 [10240/50016 (20%)]\tLoss: 0.438739\n",
      "Train Epoch: 89 [15360/50016 (31%)]\tLoss: 0.567401\n",
      "Train Epoch: 89 [20480/50016 (41%)]\tLoss: 0.134922\n",
      "Train Epoch: 89 [25600/50016 (51%)]\tLoss: 0.365560\n",
      "Train Epoch: 89 [30720/50016 (61%)]\tLoss: 0.265759\n",
      "Train Epoch: 89 [35840/50016 (72%)]\tLoss: 0.323488\n",
      "Train Epoch: 89 [40960/50016 (82%)]\tLoss: 0.153685\n",
      "Train Epoch: 89 [46080/50016 (92%)]\tLoss: 0.220857\n",
      "Test Epoch: 89 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 89 [2048/10016 (20%)]\tAcc: 0.835813\n",
      "Test Epoch: 89 [4096/10016 (41%)]\tAcc: 0.747950\n",
      "Test Epoch: 89 [6144/10016 (61%)]\tAcc: 0.743038\n",
      "Test Epoch: 89 [8192/10016 (82%)]\tAcc: 0.871972\n",
      "Total test acc = 0.848880960295787\n",
      "epoch:  90\n",
      "Train Epoch: 90 [5120/50016 (10%)]\tLoss: 0.239004\n",
      "Train Epoch: 90 [10240/50016 (20%)]\tLoss: 0.286393\n",
      "Train Epoch: 90 [15360/50016 (31%)]\tLoss: 0.353813\n",
      "Train Epoch: 90 [20480/50016 (41%)]\tLoss: 0.242023\n",
      "Train Epoch: 90 [25600/50016 (51%)]\tLoss: 0.343824\n",
      "Train Epoch: 90 [30720/50016 (61%)]\tLoss: 0.335110\n",
      "Train Epoch: 90 [35840/50016 (72%)]\tLoss: 0.533913\n",
      "Train Epoch: 90 [40960/50016 (82%)]\tLoss: 0.053113\n",
      "Train Epoch: 90 [46080/50016 (92%)]\tLoss: 0.221743\n",
      "Test Epoch: 90 [0/10016 (0%)]\tAcc: 0.968750\n",
      "Test Epoch: 90 [2048/10016 (20%)]\tAcc: 0.837794\n",
      "Test Epoch: 90 [4096/10016 (41%)]\tAcc: 0.867946\n",
      "Test Epoch: 90 [6144/10016 (61%)]\tAcc: 0.647272\n",
      "Test Epoch: 90 [8192/10016 (82%)]\tAcc: 0.903161\n",
      "Total test acc = 0.853939220607494\n",
      "epoch:  91\n",
      "Train Epoch: 91 [5120/50016 (10%)]\tLoss: 0.188660\n",
      "Train Epoch: 91 [10240/50016 (20%)]\tLoss: 0.275302\n",
      "Train Epoch: 91 [15360/50016 (31%)]\tLoss: 0.501161\n",
      "Train Epoch: 91 [20480/50016 (41%)]\tLoss: 0.455869\n",
      "Train Epoch: 91 [25600/50016 (51%)]\tLoss: 0.213779\n",
      "Train Epoch: 91 [30720/50016 (61%)]\tLoss: 0.748196\n",
      "Train Epoch: 91 [35840/50016 (72%)]\tLoss: 0.279170\n",
      "Train Epoch: 91 [40960/50016 (82%)]\tLoss: 0.497755\n",
      "Train Epoch: 91 [46080/50016 (92%)]\tLoss: 0.273282\n",
      "Test Epoch: 91 [0/10016 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 91 [2048/10016 (20%)]\tAcc: 0.777277\n",
      "Test Epoch: 91 [4096/10016 (41%)]\tAcc: 0.932401\n",
      "Test Epoch: 91 [6144/10016 (61%)]\tAcc: 0.772276\n",
      "Test Epoch: 91 [8192/10016 (82%)]\tAcc: 0.868099\n",
      "Total test acc = 0.8555910317796164\n",
      "epoch:  92\n",
      "Train Epoch: 92 [5120/50016 (10%)]\tLoss: 0.210568\n",
      "Train Epoch: 92 [10240/50016 (20%)]\tLoss: 0.334213\n",
      "Train Epoch: 92 [15360/50016 (31%)]\tLoss: 0.265603\n",
      "Train Epoch: 92 [20480/50016 (41%)]\tLoss: 0.226849\n",
      "Train Epoch: 92 [25600/50016 (51%)]\tLoss: 0.526088\n",
      "Train Epoch: 92 [30720/50016 (61%)]\tLoss: 0.226104\n",
      "Train Epoch: 92 [35840/50016 (72%)]\tLoss: 0.226215\n",
      "Train Epoch: 92 [40960/50016 (82%)]\tLoss: 0.468642\n",
      "Train Epoch: 92 [46080/50016 (92%)]\tLoss: 0.224194\n",
      "Test Epoch: 92 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 92 [2048/10016 (20%)]\tAcc: 0.803585\n",
      "Test Epoch: 92 [4096/10016 (41%)]\tAcc: 0.811427\n",
      "Test Epoch: 92 [6144/10016 (61%)]\tAcc: 0.778131\n",
      "Test Epoch: 92 [8192/10016 (82%)]\tAcc: 0.903167\n",
      "Total test acc = 0.8591037335956402\n",
      "epoch:  93\n",
      "Train Epoch: 93 [5120/50016 (10%)]\tLoss: 0.347425\n",
      "Train Epoch: 93 [10240/50016 (20%)]\tLoss: 0.680175\n",
      "Train Epoch: 93 [15360/50016 (31%)]\tLoss: 0.307682\n",
      "Train Epoch: 93 [20480/50016 (41%)]\tLoss: 0.240410\n",
      "Train Epoch: 93 [25600/50016 (51%)]\tLoss: 0.204756\n",
      "Train Epoch: 93 [30720/50016 (61%)]\tLoss: 0.133511\n",
      "Train Epoch: 93 [35840/50016 (72%)]\tLoss: 0.221098\n",
      "Train Epoch: 93 [40960/50016 (82%)]\tLoss: 0.275955\n",
      "Train Epoch: 93 [46080/50016 (92%)]\tLoss: 0.612754\n",
      "Test Epoch: 93 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 93 [2048/10016 (20%)]\tAcc: 0.894497\n",
      "Test Epoch: 93 [4096/10016 (41%)]\tAcc: 0.777098\n",
      "Test Epoch: 93 [6144/10016 (61%)]\tAcc: 0.902308\n",
      "Test Epoch: 93 [8192/10016 (82%)]\tAcc: 0.807517\n",
      "Total test acc = 0.8574456656307157\n",
      "epoch:  94\n",
      "Train Epoch: 94 [5120/50016 (10%)]\tLoss: 0.349153\n",
      "Train Epoch: 94 [10240/50016 (20%)]\tLoss: 0.249502\n",
      "Train Epoch: 94 [15360/50016 (31%)]\tLoss: 0.465546\n",
      "Train Epoch: 94 [20480/50016 (41%)]\tLoss: 0.406886\n",
      "Train Epoch: 94 [25600/50016 (51%)]\tLoss: 0.195233\n",
      "Train Epoch: 94 [30720/50016 (61%)]\tLoss: 0.127481\n",
      "Train Epoch: 94 [35840/50016 (72%)]\tLoss: 0.365627\n",
      "Train Epoch: 94 [40960/50016 (82%)]\tLoss: 0.410749\n",
      "Train Epoch: 94 [46080/50016 (92%)]\tLoss: 0.259810\n",
      "Test Epoch: 94 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 94 [2048/10016 (20%)]\tAcc: 0.776303\n",
      "Test Epoch: 94 [4096/10016 (41%)]\tAcc: 0.778012\n",
      "Test Epoch: 94 [6144/10016 (61%)]\tAcc: 0.745022\n",
      "Test Epoch: 94 [8192/10016 (82%)]\tAcc: 0.777153\n",
      "Total test acc = 0.8510491055648323\n",
      "epoch:  95\n",
      "Train Epoch: 95 [5120/50016 (10%)]\tLoss: 0.149133\n",
      "Train Epoch: 95 [10240/50016 (20%)]\tLoss: 0.139239\n",
      "Train Epoch: 95 [15360/50016 (31%)]\tLoss: 0.470386\n",
      "Train Epoch: 95 [20480/50016 (41%)]\tLoss: 0.297924\n",
      "Train Epoch: 95 [25600/50016 (51%)]\tLoss: 0.297646\n",
      "Train Epoch: 95 [30720/50016 (61%)]\tLoss: 0.534931\n",
      "Train Epoch: 95 [35840/50016 (72%)]\tLoss: 0.123197\n",
      "Train Epoch: 95 [40960/50016 (82%)]\tLoss: 0.216172\n",
      "Train Epoch: 95 [46080/50016 (92%)]\tLoss: 0.315469\n",
      "Test Epoch: 95 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 95 [2048/10016 (20%)]\tAcc: 0.837827\n",
      "Test Epoch: 95 [4096/10016 (41%)]\tAcc: 0.810419\n",
      "Test Epoch: 95 [6144/10016 (61%)]\tAcc: 0.868068\n",
      "Test Epoch: 95 [8192/10016 (82%)]\tAcc: 0.742944\n",
      "Total test acc = 0.8540491120680981\n",
      "epoch:  96\n",
      "Train Epoch: 96 [5120/50016 (10%)]\tLoss: 0.281179\n",
      "Train Epoch: 96 [10240/50016 (20%)]\tLoss: 0.229851\n",
      "Train Epoch: 96 [15360/50016 (31%)]\tLoss: 0.215033\n",
      "Train Epoch: 96 [20480/50016 (41%)]\tLoss: 0.347285\n",
      "Train Epoch: 96 [25600/50016 (51%)]\tLoss: 0.283954\n",
      "Train Epoch: 96 [30720/50016 (61%)]\tLoss: 0.305132\n",
      "Train Epoch: 96 [35840/50016 (72%)]\tLoss: 0.429611\n",
      "Train Epoch: 96 [40960/50016 (82%)]\tLoss: 0.491785\n",
      "Train Epoch: 96 [46080/50016 (92%)]\tLoss: 0.359545\n",
      "Test Epoch: 96 [0/10016 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 96 [2048/10016 (20%)]\tAcc: 0.932461\n",
      "Test Epoch: 96 [4096/10016 (41%)]\tAcc: 0.808467\n",
      "Test Epoch: 96 [6144/10016 (61%)]\tAcc: 0.777153\n",
      "Test Epoch: 96 [8192/10016 (82%)]\tAcc: 0.903194\n",
      "Total test acc = 0.860961696711396\n",
      "epoch:  97\n",
      "Train Epoch: 97 [5120/50016 (10%)]\tLoss: 0.112423\n",
      "Train Epoch: 97 [10240/50016 (20%)]\tLoss: 0.242140\n",
      "Train Epoch: 97 [15360/50016 (31%)]\tLoss: 0.472673\n",
      "Train Epoch: 97 [20480/50016 (41%)]\tLoss: 0.233702\n",
      "Train Epoch: 97 [25600/50016 (51%)]\tLoss: 0.361730\n",
      "Train Epoch: 97 [30720/50016 (61%)]\tLoss: 0.498089\n",
      "Train Epoch: 97 [35840/50016 (72%)]\tLoss: 0.545346\n",
      "Train Epoch: 97 [40960/50016 (82%)]\tLoss: 0.574099\n",
      "Train Epoch: 97 [46080/50016 (92%)]\tLoss: 0.224503\n",
      "Test Epoch: 97 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 97 [2048/10016 (20%)]\tAcc: 0.930659\n",
      "Test Epoch: 97 [4096/10016 (41%)]\tAcc: 0.870970\n",
      "Test Epoch: 97 [6144/10016 (61%)]\tAcc: 0.714777\n",
      "Test Epoch: 97 [8192/10016 (82%)]\tAcc: 0.870020\n",
      "Total test acc = 0.8498102858182041\n",
      "epoch:  98\n",
      "Train Epoch: 98 [5120/50016 (10%)]\tLoss: 0.189858\n",
      "Train Epoch: 98 [10240/50016 (20%)]\tLoss: 0.268946\n",
      "Train Epoch: 98 [15360/50016 (31%)]\tLoss: 0.158642\n",
      "Train Epoch: 98 [20480/50016 (41%)]\tLoss: 0.149152\n",
      "Train Epoch: 98 [25600/50016 (51%)]\tLoss: 0.175468\n",
      "Train Epoch: 98 [30720/50016 (61%)]\tLoss: 0.422694\n",
      "Train Epoch: 98 [35840/50016 (72%)]\tLoss: 0.261564\n",
      "Train Epoch: 98 [40960/50016 (82%)]\tLoss: 0.243639\n",
      "Train Epoch: 98 [46080/50016 (92%)]\tLoss: 0.419457\n",
      "Test Epoch: 98 [0/10016 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 98 [2048/10016 (20%)]\tAcc: 0.805631\n",
      "Test Epoch: 98 [4096/10016 (41%)]\tAcc: 0.872950\n",
      "Test Epoch: 98 [6144/10016 (61%)]\tAcc: 0.900293\n",
      "Test Epoch: 98 [8192/10016 (82%)]\tAcc: 0.898399\n",
      "Total test acc = 0.859826222494361\n",
      "epoch:  99\n",
      "Train Epoch: 99 [5120/50016 (10%)]\tLoss: 0.285053\n",
      "Train Epoch: 99 [10240/50016 (20%)]\tLoss: 0.388723\n",
      "Train Epoch: 99 [15360/50016 (31%)]\tLoss: 0.487084\n",
      "Train Epoch: 99 [20480/50016 (41%)]\tLoss: 0.352004\n",
      "Train Epoch: 99 [25600/50016 (51%)]\tLoss: 0.169225\n",
      "Train Epoch: 99 [30720/50016 (61%)]\tLoss: 0.236170\n",
      "Train Epoch: 99 [35840/50016 (72%)]\tLoss: 0.252818\n",
      "Train Epoch: 99 [40960/50016 (82%)]\tLoss: 0.236826\n",
      "Train Epoch: 99 [46080/50016 (92%)]\tLoss: 0.373657\n",
      "Test Epoch: 99 [0/10016 (0%)]\tAcc: 0.812500\n",
      "Test Epoch: 99 [2048/10016 (20%)]\tAcc: 0.803585\n",
      "Test Epoch: 99 [4096/10016 (41%)]\tAcc: 0.871821\n",
      "Test Epoch: 99 [6144/10016 (61%)]\tAcc: 0.869989\n",
      "Test Epoch: 99 [8192/10016 (82%)]\tAcc: 0.837826\n",
      "Total test acc = 0.8567264308933494\n",
      "epoch:  100\n",
      "Train Epoch: 100 [5120/50016 (10%)]\tLoss: 0.151237\n",
      "Train Epoch: 100 [10240/50016 (20%)]\tLoss: 0.093336\n",
      "Train Epoch: 100 [15360/50016 (31%)]\tLoss: 0.322158\n",
      "Train Epoch: 100 [20480/50016 (41%)]\tLoss: 0.351623\n",
      "Train Epoch: 100 [25600/50016 (51%)]\tLoss: 0.313735\n",
      "Train Epoch: 100 [30720/50016 (61%)]\tLoss: 0.333479\n",
      "Train Epoch: 100 [35840/50016 (72%)]\tLoss: 0.259163\n",
      "Train Epoch: 100 [40960/50016 (82%)]\tLoss: 0.221015\n",
      "Train Epoch: 100 [46080/50016 (92%)]\tLoss: 0.261385\n",
      "Test Epoch: 100 [0/10016 (0%)]\tAcc: 0.937500\n",
      "Test Epoch: 100 [2048/10016 (20%)]\tAcc: 0.833798\n",
      "Test Epoch: 100 [4096/10016 (41%)]\tAcc: 0.868952\n",
      "Test Epoch: 100 [6144/10016 (61%)]\tAcc: 0.869045\n",
      "Test Epoch: 100 [8192/10016 (82%)]\tAcc: 0.899319\n",
      "Total test acc = 0.8563165358240654\n"
     ]
    }
   ],
   "source": [
    "while epoch < epochs:\n",
    "    #checkpoint=torch.load(path)\n",
    "    #epoch=checkpoint['epoch']\n",
    "    #if epoch > 0:\n",
    "    #    #loss=checkpoint['loss']\n",
    "    #    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])   # load data from checkpoint if has trained.\n",
    "    print(\"epoch: \",epoch+1)\n",
    "    train_loss.append(train(net,train_data_loader,optimizer,epoch+1,device))\n",
    "    test_acc.append(test(net,test_data_loader,epoch+1,device))\n",
    "    scheduler.step()\n",
    "    #params=net.state_dict()\n",
    "    epoch+=1\n",
    "    #torch.save({'epoch': epoch,'model_state_dict': net.state_dict(),'optimizer_state_dict': optimizer.state_dict()},path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.860961696711396"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(test_acc,'r',label=\"test_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "max(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
