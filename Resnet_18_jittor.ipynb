{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0407 21:26:19.027983 20 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:19.033725 20 compiler.py:956] Jittor(1.3.8.5) src: /root/miniconda3/lib/python3.10/site-packages/jittor\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:19.037624 20 compiler.py:957] g++ at /usr/bin/g++(11.4.0)\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:19.038409 20 compiler.py:958] cache_path: /root/.cache/jittor/jt1.3.8/g++11.4.0/py3.10.8/Linux-5.15.0-9x14/IntelRXeonRGolx95/default\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:19.042383 20 __init__.py:411] Found nvcc(12.1.105) at /usr/local/cuda/bin/nvcc.\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:19.045575 20 __init__.py:411] Found addr2line(2.38) at /usr/bin/addr2line.\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:19.192012 20 compiler.py:1011] cuda key:cu12.1.105_sm_89\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:19.709901 20 __init__.py:227] Total mem: 1007.52GB, using 16 procs for compiling.\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:19.890676 20 jit_compiler.cc:28] Load cc_path: /usr/bin/g++\u001b[m\n",
      "\u001b[38;5;2m[i 0407 21:26:20.073458 20 init.cc:62] Found cuda archs: [89,]\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "from jittor.optim import Optimizer\n",
    "from jittor import nn\n",
    "from jittor import Module\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from jittor.dataset.cifar import CIFAR10\n",
    "from jittor.dataset import DataLoader\n",
    "import jittor.transform as trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0407 21:26:25.527148 20 cuda_flags.cc:49] CUDA enabled.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "jt.flags.use_cuda=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def execute(self, x):\n",
    "        out = nn.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def execute(self, x):\n",
    "        out = nn.relu(self.bn1(self.conv1(x)))\n",
    "        out = nn.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def execute(self, x):\n",
    "        out = nn.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = nn.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_data_loader,optimizer,total_train_step,epoch,compose):\n",
    "    net.train()\n",
    "    for data in train_data_loader:\n",
    "        imgs,targets=data\n",
    "        imgs,targets=imgs.float32(),targets.float32()\n",
    "        imgs=imgs.permute(0,3,1,2)\n",
    "        imgs=compose(imgs)\n",
    "        outputs=net(imgs)\n",
    "        loss=nn.cross_entropy_loss(outputs,targets)\n",
    "        optimizer.step(loss)\n",
    "        total_train_step+=1\n",
    "        if total_train_step %160 ==0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, total_train_step*32, len(train_data_loader),\n",
    "                    100. * total_train_step*32 / len(train_data_loader), loss.data[0]))\n",
    "            #format_text='Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\n'.format(\n",
    "                    #epoch, total_train_step*32, len(train_data_loader),\n",
    "                    #100. * total_train_step*32 / len(train_data_loader), loss.data[0])\n",
    "            #file.write(format_text)\n",
    "\n",
    "def test(net,test_data_loader,epoch,compose):\n",
    "    net.eval()\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_data_loader):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs,targets=inputs.permute(0,3,1,2).float32(),targets.float32()\n",
    "        inputs=compose(inputs)\n",
    "        outputs = net(inputs)\n",
    "        pred = np.argmax(outputs.data, axis=1)\n",
    "        acc = np.sum(targets.data==pred)\n",
    "        total_acc += acc\n",
    "        total_num += batch_size\n",
    "        acc = acc / batch_size\n",
    "        if batch_idx % 64 == 0:\n",
    "            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tAcc: {:.6f}'.format(epoch, \\\n",
    "                    batch_idx*batch_size, len(test_data_loader),100. * float(batch_idx)*batch_size / len(test_data_loader), acc))\n",
    "    print ('Total test acc =', total_acc / total_num)\n",
    "    #format_text=f'Total test acc ={total_acc / total_num}\\n'\n",
    "    #file.write(format_text)\n",
    "    return total_acc/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "epochs:1\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 1.869271\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.865712\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.208960\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.450563\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.887372\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.283290\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.073344\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 0.967505\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 0.918199\n",
      "Test Epoch: 1 [0/10000 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 1 [2048/10000 (20%)]\tAcc: 0.531250\n",
      "Test Epoch: 1 [4096/10000 (41%)]\tAcc: 0.531250\n",
      "Test Epoch: 1 [6144/10000 (61%)]\tAcc: 0.687500\n",
      "Test Epoch: 1 [8192/10000 (82%)]\tAcc: 0.687500\n",
      "Total test acc = 0.6627\n",
      "epochs:2\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.333475\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 0.823460\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 0.714010\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.014778\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.663175\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.000837\n",
      "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 0.610464\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 0.547369\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 0.830053\n",
      "Test Epoch: 2 [0/10000 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 2 [2048/10000 (20%)]\tAcc: 0.750000\n",
      "Test Epoch: 2 [4096/10000 (41%)]\tAcc: 0.531250\n",
      "Test Epoch: 2 [6144/10000 (61%)]\tAcc: 0.750000\n",
      "Test Epoch: 2 [8192/10000 (82%)]\tAcc: 0.718750\n",
      "Total test acc = 0.7404\n",
      "epochs:3\n",
      "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 0.891302\n",
      "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 0.507427\n",
      "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 0.535452\n",
      "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 0.679006\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.581787\n",
      "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 0.697852\n",
      "Train Epoch: 3 [35840/50000 (72%)]\tLoss: 0.412046\n",
      "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 0.389372\n",
      "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 0.348207\n",
      "Test Epoch: 3 [0/10000 (0%)]\tAcc: 0.750000\n",
      "Test Epoch: 3 [2048/10000 (20%)]\tAcc: 0.718750\n",
      "Test Epoch: 3 [4096/10000 (41%)]\tAcc: 0.625000\n",
      "Test Epoch: 3 [6144/10000 (61%)]\tAcc: 0.750000\n",
      "Test Epoch: 3 [8192/10000 (82%)]\tAcc: 0.718750\n",
      "Total test acc = 0.7267\n",
      "epochs:4\n",
      "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 0.713164\n",
      "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 0.248537\n",
      "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 0.249269\n",
      "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 0.338195\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.477700\n",
      "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 0.398049\n",
      "Train Epoch: 4 [35840/50000 (72%)]\tLoss: 0.223215\n",
      "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 0.317116\n",
      "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 0.134307\n",
      "Test Epoch: 4 [0/10000 (0%)]\tAcc: 0.781250\n",
      "Test Epoch: 4 [2048/10000 (20%)]\tAcc: 0.750000\n",
      "Test Epoch: 4 [4096/10000 (41%)]\tAcc: 0.656250\n",
      "Test Epoch: 4 [6144/10000 (61%)]\tAcc: 0.750000\n",
      "Test Epoch: 4 [8192/10000 (82%)]\tAcc: 0.718750\n",
      "Total test acc = 0.7157\n",
      "epochs:5\n",
      "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 0.309299\n",
      "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 0.099243\n",
      "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 0.228087\n",
      "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 0.233710\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.373380\n",
      "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 0.224865\n",
      "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 0.235743\n",
      "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 0.162016\n",
      "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 0.228011\n",
      "Test Epoch: 5 [0/10000 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 5 [2048/10000 (20%)]\tAcc: 0.781250\n",
      "Test Epoch: 5 [4096/10000 (41%)]\tAcc: 0.718750\n",
      "Test Epoch: 5 [6144/10000 (61%)]\tAcc: 0.718750\n",
      "Test Epoch: 5 [8192/10000 (82%)]\tAcc: 0.812500\n",
      "Total test acc = 0.7493\n",
      "epochs:6\n",
      "Train Epoch: 6 [5120/50000 (10%)]\tLoss: 0.185304\n",
      "Train Epoch: 6 [10240/50000 (20%)]\tLoss: 0.254176\n",
      "Train Epoch: 6 [15360/50000 (31%)]\tLoss: 0.201006\n",
      "Train Epoch: 6 [20480/50000 (41%)]\tLoss: 0.162764\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.192180\n",
      "Train Epoch: 6 [30720/50000 (61%)]\tLoss: 0.111267\n",
      "Train Epoch: 6 [35840/50000 (72%)]\tLoss: 0.192235\n",
      "Train Epoch: 6 [40960/50000 (82%)]\tLoss: 0.139911\n",
      "Train Epoch: 6 [46080/50000 (92%)]\tLoss: 0.033618\n",
      "Test Epoch: 6 [0/10000 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 6 [2048/10000 (20%)]\tAcc: 0.750000\n",
      "Test Epoch: 6 [4096/10000 (41%)]\tAcc: 0.687500\n",
      "Test Epoch: 6 [6144/10000 (61%)]\tAcc: 0.656250\n",
      "Test Epoch: 6 [8192/10000 (82%)]\tAcc: 0.750000\n",
      "Total test acc = 0.7504\n",
      "epochs:7\n",
      "Train Epoch: 7 [5120/50000 (10%)]\tLoss: 0.380606\n",
      "Train Epoch: 7 [10240/50000 (20%)]\tLoss: 0.064027\n",
      "Train Epoch: 7 [15360/50000 (31%)]\tLoss: 0.088190\n",
      "Train Epoch: 7 [20480/50000 (41%)]\tLoss: 0.081456\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.167575\n",
      "Train Epoch: 7 [30720/50000 (61%)]\tLoss: 0.069200\n",
      "Train Epoch: 7 [35840/50000 (72%)]\tLoss: 0.031305\n",
      "Train Epoch: 7 [40960/50000 (82%)]\tLoss: 0.090034\n",
      "Train Epoch: 7 [46080/50000 (92%)]\tLoss: 0.061182\n",
      "Test Epoch: 7 [0/10000 (0%)]\tAcc: 0.843750\n",
      "Test Epoch: 7 [2048/10000 (20%)]\tAcc: 0.718750\n",
      "Test Epoch: 7 [4096/10000 (41%)]\tAcc: 0.718750\n",
      "Test Epoch: 7 [6144/10000 (61%)]\tAcc: 0.656250\n",
      "Test Epoch: 7 [8192/10000 (82%)]\tAcc: 0.781250\n",
      "Total test acc = 0.7653\n",
      "epochs:8\n",
      "Train Epoch: 8 [5120/50000 (10%)]\tLoss: 0.027424\n",
      "Train Epoch: 8 [10240/50000 (20%)]\tLoss: 0.021382\n",
      "Train Epoch: 8 [15360/50000 (31%)]\tLoss: 0.121595\n",
      "Train Epoch: 8 [20480/50000 (41%)]\tLoss: 0.096564\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.133047\n",
      "Train Epoch: 8 [30720/50000 (61%)]\tLoss: 0.132189\n",
      "Train Epoch: 8 [35840/50000 (72%)]\tLoss: 0.081531\n",
      "Train Epoch: 8 [40960/50000 (82%)]\tLoss: 0.077060\n",
      "Train Epoch: 8 [46080/50000 (92%)]\tLoss: 0.093629\n",
      "Test Epoch: 8 [0/10000 (0%)]\tAcc: 0.875000\n",
      "Test Epoch: 8 [2048/10000 (20%)]\tAcc: 0.812500\n",
      "Test Epoch: 8 [4096/10000 (41%)]\tAcc: 0.812500\n",
      "Test Epoch: 8 [6144/10000 (61%)]\tAcc: 0.781250\n",
      "Test Epoch: 8 [8192/10000 (82%)]\tAcc: 0.812500\n",
      "Total test acc = 0.7951\n",
      "epochs:9\n",
      "Train Epoch: 9 [5120/50000 (10%)]\tLoss: 0.072963\n",
      "Train Epoch: 9 [10240/50000 (20%)]\tLoss: 0.020576\n",
      "Train Epoch: 9 [15360/50000 (31%)]\tLoss: 0.022179\n",
      "Train Epoch: 9 [20480/50000 (41%)]\tLoss: 0.016381\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.025208\n",
      "Train Epoch: 9 [30720/50000 (61%)]\tLoss: 0.060910\n",
      "Train Epoch: 9 [35840/50000 (72%)]\tLoss: 0.016359\n",
      "Train Epoch: 9 [40960/50000 (82%)]\tLoss: 0.037652\n",
      "Train Epoch: 9 [46080/50000 (92%)]\tLoss: 0.028943\n",
      "Test Epoch: 9 [0/10000 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 9 [2048/10000 (20%)]\tAcc: 0.781250\n",
      "Test Epoch: 9 [4096/10000 (41%)]\tAcc: 0.750000\n",
      "Test Epoch: 9 [6144/10000 (61%)]\tAcc: 0.875000\n",
      "Test Epoch: 9 [8192/10000 (82%)]\tAcc: 0.750000\n",
      "Total test acc = 0.792\n",
      "epochs:10\n",
      "Train Epoch: 10 [5120/50000 (10%)]\tLoss: 0.037691\n",
      "Train Epoch: 10 [10240/50000 (20%)]\tLoss: 0.010180\n",
      "Train Epoch: 10 [15360/50000 (31%)]\tLoss: 0.005435\n",
      "Train Epoch: 10 [20480/50000 (41%)]\tLoss: 0.028324\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.004502\n",
      "Train Epoch: 10 [30720/50000 (61%)]\tLoss: 0.008058\n",
      "Train Epoch: 10 [35840/50000 (72%)]\tLoss: 0.022545\n",
      "Train Epoch: 10 [40960/50000 (82%)]\tLoss: 0.005793\n",
      "Train Epoch: 10 [46080/50000 (92%)]\tLoss: 0.007122\n",
      "Test Epoch: 10 [0/10000 (0%)]\tAcc: 0.906250\n",
      "Test Epoch: 10 [2048/10000 (20%)]\tAcc: 0.812500\n",
      "Test Epoch: 10 [4096/10000 (41%)]\tAcc: 0.750000\n",
      "Test Epoch: 10 [6144/10000 (61%)]\tAcc: 0.812500\n",
      "Test Epoch: 10 [8192/10000 (82%)]\tAcc: 0.750000\n",
      "Total test acc = 0.8031\n",
      "epochs:11\n",
      "Train Epoch: 11 [5120/50000 (10%)]\tLoss: 0.044707\n",
      "Train Epoch: 11 [10240/50000 (20%)]\tLoss: 0.018488\n",
      "Train Epoch: 11 [15360/50000 (31%)]\tLoss: 0.001383\n",
      "Train Epoch: 11 [20480/50000 (41%)]\tLoss: 0.001319\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.013091\n",
      "Train Epoch: 11 [30720/50000 (61%)]\tLoss: 0.024025\n",
      "Train Epoch: 11 [35840/50000 (72%)]\tLoss: 0.026248\n",
      "Train Epoch: 11 [40960/50000 (82%)]\tLoss: 0.003692\n",
      "Train Epoch: 11 [46080/50000 (92%)]\tLoss: 0.008218\n",
      "Test Epoch: 11 [0/10000 (0%)]\tAcc: 0.968750\n",
      "Test Epoch: 11 [2048/10000 (20%)]\tAcc: 0.781250\n",
      "Test Epoch: 11 [4096/10000 (41%)]\tAcc: 0.843750\n",
      "Test Epoch: 11 [6144/10000 (61%)]\tAcc: 0.781250\n",
      "Test Epoch: 11 [8192/10000 (82%)]\tAcc: 0.843750\n",
      "Total test acc = 0.8164\n",
      "epochs:12\n",
      "Train Epoch: 12 [5120/50000 (10%)]\tLoss: 0.009210\n",
      "Train Epoch: 12 [10240/50000 (20%)]\tLoss: 0.004957\n",
      "Train Epoch: 12 [15360/50000 (31%)]\tLoss: 0.001995\n",
      "Train Epoch: 12 [20480/50000 (41%)]\tLoss: 0.001380\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.001876\n",
      "Train Epoch: 12 [30720/50000 (61%)]\tLoss: 0.004961\n",
      "Train Epoch: 12 [35840/50000 (72%)]\tLoss: 0.003449\n",
      "Train Epoch: 12 [40960/50000 (82%)]\tLoss: 0.013148\n"
     ]
    }
   ],
   "source": [
    "net=ResNet18()\n",
    "train_data=CIFAR10(train=True)\n",
    "test_data=CIFAR10(train=False)\n",
    "compose=trans.Compose([trans.ImageNormalize((0.485,0.456,0.406),(0.229, 0.224, 0.225))])\n",
    "train_data_loader=DataLoader(train_data,batch_size=32)\n",
    "test_data_loader=DataLoader(test_data,batch_size=32)\n",
    "learning_rate=1e-3\n",
    "optimizer=nn.SGD(net.parameters(),lr=learning_rate,momentum=0.9,weight_decay=5e-4)\n",
    "scheduler = jt.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "total_train_step=0\n",
    "total_test_step=0\n",
    "test_acc=[]\n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    print(\"epochs:{}\".format(epoch+1))\n",
    "    train(net,train_data_loader,optimizer,total_train_step,epoch+1,compose)\n",
    "    test_acc.append(test(net, test_data_loader, epoch+1,compose))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_acc,'r',label=\"test_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
