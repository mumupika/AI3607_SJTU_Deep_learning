{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jittor as jt\n",
    "import pygmtools as pygm\n",
    "from jittor.optim import Optimizer\n",
    "from jittor import nn\n",
    "from jittor import Module\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from jittor.dataset.cifar import CIFAR10\n",
    "from jittor.dataset import DataLoader\n",
    "import jittor.transform as trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0402 19:32:27.163587 00 cuda_flags.cc:49] CUDA enabled.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "jt.flags.use_cuda=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Resnet definition'''\n",
    "'''Todo'''\n",
    "class Resnet(Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet,self).__init__()\n",
    "        self.conv1=nn.Conv(3,8,3,1,1)\n",
    "        self.bn1=nn.BatchNorm(8)\n",
    "        self.relu=nn.Relu()\n",
    "        self.maxpool=nn.MaxPool2d(2)\n",
    "        self.conv2=nn.Conv(8,16,3,1,1)\n",
    "        self.bn2=nn.BatchNorm(16)\n",
    "        #relu\n",
    "        #maxpool\n",
    "        self.conv3=nn.Conv(16,32,3,1,1)\n",
    "        self.bn3=nn.BatchNorm(32)\n",
    "        #relu\n",
    "        #maxpool\n",
    "        self.conv4=nn.Conv(32,64,3,1,1)\n",
    "        self.bn4=nn.BatchNorm(64)\n",
    "        #relu\n",
    "        #maxpool\n",
    "        self.conv5=nn.Conv(64,128,3,1,1)\n",
    "        self.bn5=nn.BatchNorm(128)\n",
    "        #relu\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.l1=nn.Linear(128*4*4,512)\n",
    "        self.bn6=nn.BatchNorm(512)\n",
    "        #relu\n",
    "        self.fc=nn.Linear(512,16)\n",
    "        #softmax\n",
    "    def execute(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.bn2(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        x=self.conv3(x)\n",
    "        x=self.bn3(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        x=self.conv4(x)\n",
    "        x=self.bn4(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        x=self.conv5(x)\n",
    "        x=self.bn5(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.l1(x)\n",
    "        x=self.bn6(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.fc(x)\n",
    "        x=jt.reshape(x,(-1,4,4))\n",
    "        x=pygm.linear_solvers.sinkhorn(x)\n",
    "        #x=nn.softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image_load(train_data):\n",
    "    batch_size=0\n",
    "    imgs=[]# for image to all images set\n",
    "    for data in train_data:\n",
    "        img,target=data\n",
    "        batch_size+=1\n",
    "        for i in range(len(img)):\n",
    "            image=[]\n",
    "            '''Containing all 4 parts.'''\n",
    "            image.append(img[i].permute(2,1,0)[:,0:32,0:32])\n",
    "            image.append(img[i].permute(2,1,0)[:,32:32*2,0:32])\n",
    "            image.append(img[i].permute(2,1,0)[:,0:32,32:32*2])\n",
    "            image.append(img[i].permute(2,1,0)[:,32:32*2,32:32*2])\n",
    "            imgs.append(image)\n",
    "        if batch_size%4==0: #here change batch_size\n",
    "            imgs=np.array(imgs)\n",
    "            imgs= jt.Var(imgs).float32()\n",
    "            yield imgs\n",
    "            imgs=[]\n",
    "    #imgs=jt.Var(imgs).float32()\n",
    "    #return imgs\n",
    "    \n",
    "\n",
    "\n",
    "def target_generation(images):\n",
    "    '''Randomly shuffle permutation of image,Generate target'''\n",
    "    rearranged_images=[]\n",
    "    targets=[]\n",
    "    for i in range(len(images)):\n",
    "        permute=np.random.permutation(4)[:4]\n",
    "        rearranged_img=[]\n",
    "        target=np.zeros((4,4))\n",
    "        for j in range(len(images[i])):\n",
    "            rearranged_img.append(images[i][permute[j]])\n",
    "            target[j][permute[j]]=1\n",
    "        rearranged_img=jt.Var(rearranged_img).permute(1,0,2,3)\n",
    "        rearranged_img=np.reshape(rearranged_img,(3,64,64))\n",
    "        rearranged_images.append(rearranged_img)\n",
    "        targets.append(target)\n",
    "    \n",
    "    rearranged_images,targets=np.array(rearranged_images),np.array(targets)\n",
    "    rearranged_images,targets=jt.Var(rearranged_images).float32(),jt.Var(targets).float32()\n",
    "    return rearranged_images,targets\n",
    "\n",
    "        \n",
    "        \n",
    "def train(net,optimizer,train_data_loader,epoch):\n",
    "    net.train()\n",
    "    train_step=0\n",
    "    total_loss=0\n",
    "    for image in train_image_load(train_data_loader):\n",
    "        inputs,targets=target_generation(image)     #(64,3,64,64) vs (64,4,4)\n",
    "        for i in range(4):\n",
    "            outputs=net(inputs)\n",
    "            loss=pygm.utils.permutation_loss(outputs,targets)\n",
    "            optimizer.step(loss)\n",
    "            train_step+=1\n",
    "            total_loss+=loss\n",
    "            if train_step%500==0:\n",
    "                print(f'epoch:{epoch},Step:{train_step},Loss:{loss}')\n",
    "                #format_text=f\"epoch:{epoch},Step:{train_step},Loss:{loss}\\n\"\n",
    "                #file.write(format_text)\n",
    "    return total_loss/train_step\n",
    "\n",
    "\n",
    "def test_image_load(test_data):\n",
    "    batch_size=0\n",
    "    imgs=[]\n",
    "    for data in test_data:\n",
    "        img,target=data\n",
    "        batch_size+=1\n",
    "        for i in range(len(img)):\n",
    "            image=[]\n",
    "            '''Containing all 4 parts.'''\n",
    "            image.append(img[i].permute(2,1,0)[:,0:32,0:32])\n",
    "            image.append(img[i].permute(2,1,0)[:,32:32*2,0:32])\n",
    "            image.append(img[i].permute(2,1,0)[:,0:32,32:32*2])\n",
    "            image.append(img[i].permute(2,1,0)[:,32:32*2,32:32*2])\n",
    "            imgs.append(image)\n",
    "            \n",
    "        imgs=np.array(imgs)\n",
    "        imgs=jt.Var(imgs).float32()\n",
    "        yield imgs\n",
    "        imgs=[]\n",
    "\n",
    "\n",
    "def test_target_generation(images):\n",
    "    '''Randomly shuffle permutation of image,Generate target'''\n",
    "    rearranged_images=[]\n",
    "    targets=[]\n",
    "    for i in range(len(images)):\n",
    "        permute=np.random.permutation(4)[:4]\n",
    "        rearranged_img=[]\n",
    "        target=np.zeros((4,4))\n",
    "        for j in range(len(images[i])):\n",
    "            rearranged_img.append(images[i][permute[j]])\n",
    "            target[j][permute[j]]=1\n",
    "        rearranged_img=jt.Var(rearranged_img).permute(1,0,2,3)\n",
    "        rearranged_img=np.reshape(rearranged_img,(3,64,64))\n",
    "        rearranged_images.append(rearranged_img)\n",
    "        targets.append(target)\n",
    "    rearranged_images,targets=np.array(rearranged_images),np.array(targets)\n",
    "    rearranged_images,targets=jt.Var(rearranged_images).float32(),jt.Var(targets).float32()\n",
    "    return rearranged_images,targets\n",
    "\n",
    "def eval(outputs,target_i):\n",
    "    acc=0\n",
    "    for i in range(len(outputs)):\n",
    "        pred=jt.argmax(outputs[i],1)\n",
    "        real=jt.argmax(target_i[i],1)\n",
    "        for j in range(len(pred[0])):\n",
    "            if pred[0][j]==real[0][j]:\n",
    "                acc+=1\n",
    "    return acc\n",
    "\n",
    "def test(net,optimizer,test_data_loader,epoch):\n",
    "    test_step=0\n",
    "    total_acc=0\n",
    "    net.eval()\n",
    "    for image in test_image_load(test_data_loader):\n",
    "        inputs,targets=test_target_generation(image)\n",
    "        outputs=net(inputs) # output(64,4),target(64,4,4)\n",
    "        acc=eval(outputs,targets)\n",
    "        total_acc+=acc/64.0\n",
    "        test_step+=1\n",
    "        if test_step%100==0:\n",
    "            print(f'epoch:{epoch},Step:{test_step},Accuracy:{acc/64.0*100}%')\n",
    "            #format_text=f'epoch:{epoch},Step:{test_step},Accuracy:{total_acc/(len(outputs)*4)*100}%\\n'\n",
    "            #file.write(format_text)\n",
    "    \n",
    "    print(f'\\n epoch:{epoch},Accuracy:{total_acc/test_step*100}%\\n')\n",
    "    #format_text=f'\\n epoch:{epoch},Accuracy:{overall_acc/test_step*100}%\\n'\n",
    "    #file.write(format_text)\n",
    "    return total_acc/test_step*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "'''The target and the full set of images have completed'''\n",
    "net=Resnet()\n",
    "learning_rate=1e-5\n",
    "optimizer=nn.SGD(net.parameters(),lr=learning_rate,momentum=0.9)\n",
    "scheduler = jt.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "# 1. Get the train data\n",
    "train_data=CIFAR10(train=True,transform=trans.RandomResizedCrop((32*2,32*2)))\n",
    "train_data_loader=DataLoader(train_data,batch_size=16)\n",
    "test_data=CIFAR10(train=False,transform=trans.RandomResizedCrop((32*2,32*2)))\n",
    "test_data_loader=DataLoader(test_data,batch_size=16)\n",
    "epochs=int(1e5)\n",
    "train_loss=[]\n",
    "test_acc=[]\n",
    "pygm.set_backend('jittor')\n",
    "epoch=0                         # change here after the kernel break.\n",
    "path_pkl=\"/root/perm_jittor.pkl\"\n",
    "path_p=\"/root/epoch_jittor.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not run here anymore.\n",
    "epoch_dict={\"epoch\":epoch}\n",
    "jt.save(epoch_dict,path_p)\n",
    "net.save(path_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  85\n",
      "epoch:85,Step:500,Loss:0.8513048887252808\n",
      "epoch:85,Step:1000,Loss:0.9510497450828552\n",
      "epoch:85,Step:1500,Loss:0.9607932567596436\n",
      "epoch:85,Step:2000,Loss:0.914334774017334\n",
      "epoch:85,Step:2500,Loss:0.9032962322235107\n",
      "epoch:85,Step:3000,Loss:1.005419373512268\n",
      "epoch:85,Step:100,Accuracy:71.875%\n",
      "epoch:85,Step:200,Accuracy:90.625%\n",
      "epoch:85,Step:300,Accuracy:92.1875%\n",
      "epoch:85,Step:400,Accuracy:79.6875%\n",
      "epoch:85,Step:500,Accuracy:89.0625%\n",
      "epoch:85,Step:600,Accuracy:82.8125%\n",
      "\n",
      " epoch:85,Accuracy:80.595%\n",
      "\n",
      "epoch:  86\n",
      "epoch:86,Step:500,Loss:0.9541069269180298\n",
      "epoch:86,Step:1000,Loss:0.9439809322357178\n",
      "epoch:86,Step:1500,Loss:0.9454079866409302\n",
      "epoch:86,Step:2000,Loss:0.9303105473518372\n",
      "epoch:86,Step:2500,Loss:0.8985252976417542\n",
      "epoch:86,Step:3000,Loss:0.8568313717842102\n",
      "epoch:86,Step:100,Accuracy:81.25%\n",
      "epoch:86,Step:200,Accuracy:82.8125%\n",
      "epoch:86,Step:300,Accuracy:85.9375%\n",
      "epoch:86,Step:400,Accuracy:76.5625%\n",
      "epoch:86,Step:500,Accuracy:85.9375%\n",
      "epoch:86,Step:600,Accuracy:84.375%\n",
      "\n",
      " epoch:86,Accuracy:80.87%\n",
      "\n",
      "epoch:  87\n",
      "epoch:87,Step:500,Loss:0.7493227124214172\n",
      "epoch:87,Step:1000,Loss:0.9634714722633362\n",
      "epoch:87,Step:1500,Loss:1.079095721244812\n",
      "epoch:87,Step:2000,Loss:0.9382933378219604\n",
      "epoch:87,Step:2500,Loss:0.889168918132782\n",
      "epoch:87,Step:3000,Loss:0.8598160743713379\n",
      "epoch:87,Step:100,Accuracy:75.0%\n",
      "epoch:87,Step:200,Accuracy:84.375%\n",
      "epoch:87,Step:300,Accuracy:81.25%\n",
      "epoch:87,Step:400,Accuracy:89.0625%\n",
      "epoch:87,Step:500,Accuracy:79.6875%\n",
      "epoch:87,Step:600,Accuracy:68.75%\n",
      "\n",
      " epoch:87,Accuracy:80.9875%\n",
      "\n",
      "epoch:  88\n",
      "epoch:88,Step:500,Loss:0.8268046379089355\n",
      "epoch:88,Step:1000,Loss:0.8799924254417419\n",
      "epoch:88,Step:1500,Loss:0.7780762314796448\n",
      "epoch:88,Step:2000,Loss:0.8555307984352112\n",
      "epoch:88,Step:2500,Loss:0.7926141023635864\n",
      "epoch:88,Step:3000,Loss:0.8182438611984253\n",
      "epoch:88,Step:100,Accuracy:84.375%\n",
      "epoch:88,Step:200,Accuracy:79.6875%\n",
      "epoch:88,Step:300,Accuracy:76.5625%\n",
      "epoch:88,Step:400,Accuracy:81.25%\n",
      "epoch:88,Step:500,Accuracy:75.0%\n",
      "epoch:88,Step:600,Accuracy:70.3125%\n",
      "\n",
      " epoch:88,Accuracy:78.49249999999999%\n",
      "\n",
      "epoch:  89\n",
      "epoch:89,Step:500,Loss:0.8584204316139221\n",
      "epoch:89,Step:1000,Loss:0.873556911945343\n",
      "epoch:89,Step:1500,Loss:0.8353119492530823\n",
      "epoch:89,Step:2000,Loss:0.9159390926361084\n",
      "epoch:89,Step:2500,Loss:0.864866316318512\n",
      "epoch:89,Step:3000,Loss:0.820855438709259\n",
      "epoch:89,Step:100,Accuracy:75.0%\n",
      "epoch:89,Step:200,Accuracy:84.375%\n",
      "epoch:89,Step:300,Accuracy:60.9375%\n",
      "epoch:89,Step:400,Accuracy:82.8125%\n",
      "epoch:89,Step:500,Accuracy:75.0%\n",
      "epoch:89,Step:600,Accuracy:82.8125%\n",
      "\n",
      " epoch:89,Accuracy:81.9375%\n",
      "\n",
      "epoch:  90\n",
      "epoch:90,Step:500,Loss:0.7525190711021423\n",
      "epoch:90,Step:1000,Loss:0.9360120296478271\n",
      "epoch:90,Step:1500,Loss:0.9624311923980713\n",
      "epoch:90,Step:2000,Loss:0.8033839464187622\n",
      "epoch:90,Step:2500,Loss:0.9227698445320129\n",
      "epoch:90,Step:3000,Loss:0.953691303730011\n",
      "epoch:90,Step:100,Accuracy:73.4375%\n",
      "epoch:90,Step:200,Accuracy:87.5%\n",
      "epoch:90,Step:300,Accuracy:70.3125%\n",
      "epoch:90,Step:400,Accuracy:84.375%\n",
      "epoch:90,Step:500,Accuracy:67.1875%\n",
      "epoch:90,Step:600,Accuracy:87.5%\n",
      "\n",
      " epoch:90,Accuracy:81.2975%\n",
      "\n",
      "epoch:  91\n",
      "epoch:91,Step:500,Loss:0.8456125259399414\n",
      "epoch:91,Step:1000,Loss:0.9189924597740173\n",
      "epoch:91,Step:1500,Loss:0.8193286657333374\n",
      "epoch:91,Step:2000,Loss:0.9600816965103149\n",
      "epoch:91,Step:2500,Loss:0.8664266467094421\n",
      "epoch:91,Step:3000,Loss:0.8903879523277283\n",
      "epoch:91,Step:100,Accuracy:90.625%\n",
      "epoch:91,Step:200,Accuracy:82.8125%\n",
      "epoch:91,Step:300,Accuracy:84.375%\n",
      "epoch:91,Step:400,Accuracy:89.0625%\n",
      "epoch:91,Step:500,Accuracy:85.9375%\n",
      "epoch:91,Step:600,Accuracy:87.5%\n",
      "\n",
      " epoch:91,Accuracy:82.8475%\n",
      "\n",
      "epoch:  92\n",
      "epoch:92,Step:500,Loss:0.8973007202148438\n",
      "epoch:92,Step:1000,Loss:0.9203137755393982\n",
      "epoch:92,Step:1500,Loss:0.9427374005317688\n",
      "epoch:92,Step:2000,Loss:0.971883237361908\n",
      "epoch:92,Step:2500,Loss:0.8365336060523987\n",
      "epoch:92,Step:3000,Loss:0.8599393367767334\n",
      "epoch:92,Step:100,Accuracy:89.0625%\n",
      "epoch:92,Step:200,Accuracy:73.4375%\n",
      "epoch:92,Step:300,Accuracy:85.9375%\n",
      "epoch:92,Step:400,Accuracy:84.375%\n",
      "epoch:92,Step:500,Accuracy:81.25%\n",
      "epoch:92,Step:600,Accuracy:82.8125%\n",
      "\n",
      " epoch:92,Accuracy:82.17%\n",
      "\n",
      "epoch:  93\n",
      "epoch:93,Step:500,Loss:0.8332640528678894\n",
      "epoch:93,Step:1000,Loss:1.0189357995986938\n",
      "epoch:93,Step:1500,Loss:0.9047911167144775\n",
      "epoch:93,Step:2000,Loss:0.8718385100364685\n",
      "epoch:93,Step:2500,Loss:0.7878329157829285\n",
      "epoch:93,Step:3000,Loss:0.7494153380393982\n",
      "epoch:93,Step:100,Accuracy:81.25%\n",
      "epoch:93,Step:200,Accuracy:89.0625%\n",
      "epoch:93,Step:300,Accuracy:92.1875%\n",
      "epoch:93,Step:400,Accuracy:81.25%\n",
      "epoch:93,Step:500,Accuracy:95.3125%\n",
      "epoch:93,Step:600,Accuracy:78.125%\n",
      "\n",
      " epoch:93,Accuracy:83.6275%\n",
      "\n",
      "epoch:  94\n",
      "epoch:94,Step:500,Loss:0.8269040584564209\n",
      "epoch:94,Step:1000,Loss:0.7092223167419434\n",
      "epoch:94,Step:1500,Loss:0.9026699066162109\n",
      "epoch:94,Step:2000,Loss:0.8843876123428345\n",
      "epoch:94,Step:2500,Loss:0.7309737205505371\n",
      "epoch:94,Step:3000,Loss:1.0131447315216064\n",
      "epoch:94,Step:100,Accuracy:76.5625%\n",
      "epoch:94,Step:200,Accuracy:81.25%\n",
      "epoch:94,Step:300,Accuracy:89.0625%\n",
      "epoch:94,Step:400,Accuracy:68.75%\n",
      "epoch:94,Step:500,Accuracy:87.5%\n",
      "epoch:94,Step:600,Accuracy:70.3125%\n",
      "\n",
      " epoch:94,Accuracy:83.2525%\n",
      "\n",
      "epoch:  95\n",
      "epoch:95,Step:500,Loss:0.8038646578788757\n",
      "epoch:95,Step:1000,Loss:0.8149751424789429\n",
      "epoch:95,Step:1500,Loss:0.8426170945167542\n",
      "epoch:95,Step:2000,Loss:0.819804310798645\n",
      "epoch:95,Step:2500,Loss:0.7198276519775391\n",
      "epoch:95,Step:3000,Loss:0.6945213675498962\n",
      "epoch:95,Step:100,Accuracy:79.6875%\n",
      "epoch:95,Step:200,Accuracy:79.6875%\n",
      "epoch:95,Step:300,Accuracy:78.125%\n",
      "epoch:95,Step:400,Accuracy:89.0625%\n",
      "epoch:95,Step:500,Accuracy:87.5%\n",
      "epoch:95,Step:600,Accuracy:73.4375%\n",
      "\n",
      " epoch:95,Accuracy:82.52000000000001%\n",
      "\n",
      "epoch:  96\n",
      "epoch:96,Step:500,Loss:0.7449252605438232\n",
      "epoch:96,Step:1000,Loss:0.8789156079292297\n",
      "epoch:96,Step:1500,Loss:0.8032769560813904\n",
      "epoch:96,Step:2000,Loss:0.9905824065208435\n",
      "epoch:96,Step:2500,Loss:0.7731950879096985\n",
      "epoch:96,Step:3000,Loss:0.7200210094451904\n",
      "epoch:96,Step:100,Accuracy:79.6875%\n",
      "epoch:96,Step:200,Accuracy:81.25%\n",
      "epoch:96,Step:300,Accuracy:93.75%\n",
      "epoch:96,Step:400,Accuracy:87.5%\n",
      "epoch:96,Step:500,Accuracy:75.0%\n",
      "epoch:96,Step:600,Accuracy:85.9375%\n",
      "\n",
      " epoch:96,Accuracy:82.695%\n",
      "\n",
      "epoch:  97\n",
      "epoch:97,Step:500,Loss:0.6872929334640503\n",
      "epoch:97,Step:1000,Loss:0.815836489200592\n",
      "epoch:97,Step:1500,Loss:0.8349337577819824\n",
      "epoch:97,Step:2000,Loss:0.8123695850372314\n",
      "epoch:97,Step:2500,Loss:0.947760820388794\n",
      "epoch:97,Step:3000,Loss:0.7754138112068176\n",
      "epoch:97,Step:100,Accuracy:82.8125%\n",
      "epoch:97,Step:200,Accuracy:93.75%\n",
      "epoch:97,Step:300,Accuracy:81.25%\n",
      "epoch:97,Step:400,Accuracy:82.8125%\n",
      "epoch:97,Step:500,Accuracy:85.9375%\n",
      "epoch:97,Step:600,Accuracy:93.75%\n",
      "\n",
      " epoch:97,Accuracy:83.67999999999999%\n",
      "\n",
      "epoch:  98\n",
      "epoch:98,Step:500,Loss:0.7088087797164917\n",
      "epoch:98,Step:1000,Loss:0.7750835418701172\n",
      "epoch:98,Step:1500,Loss:0.846442461013794\n",
      "epoch:98,Step:2000,Loss:0.965319812297821\n",
      "epoch:98,Step:2500,Loss:0.8868321776390076\n",
      "epoch:98,Step:3000,Loss:0.8300464749336243\n",
      "epoch:98,Step:100,Accuracy:78.125%\n",
      "epoch:98,Step:200,Accuracy:85.9375%\n",
      "epoch:98,Step:300,Accuracy:89.0625%\n",
      "epoch:98,Step:400,Accuracy:96.875%\n",
      "epoch:98,Step:500,Accuracy:90.625%\n",
      "epoch:98,Step:600,Accuracy:87.5%\n",
      "\n",
      " epoch:98,Accuracy:84.74249999999999%\n",
      "\n",
      "epoch:  99\n",
      "epoch:99,Step:500,Loss:0.8631347417831421\n",
      "epoch:99,Step:1000,Loss:0.8547860383987427\n",
      "epoch:99,Step:1500,Loss:0.6968128681182861\n",
      "epoch:99,Step:2000,Loss:0.6946377754211426\n",
      "epoch:99,Step:2500,Loss:0.7507264018058777\n",
      "epoch:99,Step:3000,Loss:0.6699924468994141\n",
      "epoch:99,Step:100,Accuracy:89.0625%\n",
      "epoch:99,Step:200,Accuracy:78.125%\n",
      "epoch:99,Step:300,Accuracy:82.8125%\n",
      "epoch:99,Step:400,Accuracy:82.8125%\n",
      "epoch:99,Step:500,Accuracy:87.5%\n",
      "epoch:99,Step:600,Accuracy:84.375%\n",
      "\n",
      " epoch:99,Accuracy:84.2975%\n",
      "\n",
      "epoch:  100\n",
      "epoch:100,Step:500,Loss:0.8089234232902527\n",
      "epoch:100,Step:1000,Loss:0.9220643639564514\n",
      "epoch:100,Step:1500,Loss:0.6772241592407227\n",
      "epoch:100,Step:2000,Loss:0.9490756392478943\n",
      "epoch:100,Step:2500,Loss:0.8833193778991699\n",
      "epoch:100,Step:3000,Loss:0.7171071767807007\n",
      "epoch:100,Step:100,Accuracy:85.9375%\n",
      "epoch:100,Step:200,Accuracy:92.1875%\n",
      "epoch:100,Step:300,Accuracy:100.0%\n",
      "epoch:100,Step:400,Accuracy:81.25%\n",
      "epoch:100,Step:500,Accuracy:82.8125%\n",
      "epoch:100,Step:600,Accuracy:75.0%\n",
      "\n",
      " epoch:100,Accuracy:84.3925%\n",
      "\n",
      "epoch:  101\n",
      "epoch:101,Step:500,Loss:0.7108680009841919\n",
      "epoch:101,Step:1000,Loss:0.8379808664321899\n",
      "epoch:101,Step:1500,Loss:0.6999235153198242\n",
      "epoch:101,Step:2000,Loss:0.7608649134635925\n",
      "epoch:101,Step:2500,Loss:0.6274776458740234\n",
      "epoch:101,Step:3000,Loss:0.7093202471733093\n",
      "epoch:101,Step:100,Accuracy:92.1875%\n",
      "epoch:101,Step:200,Accuracy:85.9375%\n",
      "epoch:101,Step:300,Accuracy:96.875%\n",
      "epoch:101,Step:400,Accuracy:100.0%\n",
      "epoch:101,Step:500,Accuracy:73.4375%\n",
      "epoch:101,Step:600,Accuracy:71.875%\n",
      "\n",
      " epoch:101,Accuracy:85.1975%\n",
      "\n",
      "epoch:  102\n",
      "epoch:102,Step:500,Loss:0.7098858952522278\n",
      "epoch:102,Step:1000,Loss:0.8566522598266602\n",
      "epoch:102,Step:1500,Loss:0.706468939781189\n",
      "epoch:102,Step:2000,Loss:0.8956374526023865\n",
      "epoch:102,Step:2500,Loss:0.8037429451942444\n",
      "epoch:102,Step:3000,Loss:0.803354024887085\n",
      "epoch:102,Step:100,Accuracy:82.8125%\n",
      "epoch:102,Step:200,Accuracy:81.25%\n",
      "epoch:102,Step:300,Accuracy:76.5625%\n",
      "epoch:102,Step:400,Accuracy:82.8125%\n",
      "epoch:102,Step:500,Accuracy:87.5%\n",
      "epoch:102,Step:600,Accuracy:78.125%\n",
      "\n",
      " epoch:102,Accuracy:85.7525%\n",
      "\n",
      "epoch:  103\n",
      "epoch:103,Step:500,Loss:0.6559921503067017\n",
      "epoch:103,Step:1000,Loss:0.9479371309280396\n",
      "epoch:103,Step:1500,Loss:0.8613288998603821\n",
      "epoch:103,Step:2000,Loss:0.7841613292694092\n",
      "epoch:103,Step:2500,Loss:0.9243384599685669\n",
      "epoch:103,Step:3000,Loss:0.7529851198196411\n",
      "epoch:103,Step:100,Accuracy:82.8125%\n",
      "epoch:103,Step:200,Accuracy:71.875%\n",
      "epoch:103,Step:300,Accuracy:81.25%\n",
      "epoch:103,Step:400,Accuracy:87.5%\n",
      "epoch:103,Step:500,Accuracy:81.25%\n",
      "epoch:103,Step:600,Accuracy:84.375%\n",
      "\n",
      " epoch:103,Accuracy:85.1825%\n",
      "\n",
      "epoch:  104\n",
      "epoch:104,Step:500,Loss:0.7955425381660461\n",
      "epoch:104,Step:1000,Loss:0.7215260863304138\n",
      "epoch:104,Step:1500,Loss:0.8051949143409729\n",
      "epoch:104,Step:2000,Loss:0.983483612537384\n",
      "epoch:104,Step:2500,Loss:0.7832488417625427\n",
      "epoch:104,Step:3000,Loss:0.6585076451301575\n",
      "epoch:104,Step:100,Accuracy:93.75%\n",
      "epoch:104,Step:200,Accuracy:84.375%\n",
      "epoch:104,Step:300,Accuracy:82.8125%\n",
      "epoch:104,Step:400,Accuracy:79.6875%\n",
      "epoch:104,Step:500,Accuracy:76.5625%\n",
      "epoch:104,Step:600,Accuracy:92.1875%\n",
      "\n",
      " epoch:104,Accuracy:85.965%\n",
      "\n",
      "epoch:  105\n",
      "epoch:105,Step:500,Loss:0.6830252408981323\n",
      "epoch:105,Step:1000,Loss:0.7926362752914429\n",
      "epoch:105,Step:1500,Loss:0.6767323613166809\n",
      "epoch:105,Step:2000,Loss:0.7181667685508728\n",
      "epoch:105,Step:2500,Loss:0.7971651554107666\n",
      "epoch:105,Step:3000,Loss:0.6606535315513611\n",
      "epoch:105,Step:100,Accuracy:87.5%\n",
      "epoch:105,Step:200,Accuracy:89.0625%\n",
      "epoch:105,Step:300,Accuracy:71.875%\n",
      "epoch:105,Step:400,Accuracy:84.375%\n",
      "epoch:105,Step:500,Accuracy:90.625%\n",
      "epoch:105,Step:600,Accuracy:78.125%\n",
      "\n",
      " epoch:105,Accuracy:85.8775%\n",
      "\n",
      "epoch:  106\n",
      "epoch:106,Step:500,Loss:0.8077703714370728\n",
      "epoch:106,Step:1000,Loss:0.6844603419303894\n",
      "epoch:106,Step:1500,Loss:0.7194927334785461\n",
      "epoch:106,Step:2000,Loss:0.7379977703094482\n",
      "epoch:106,Step:2500,Loss:0.6545462012290955\n",
      "epoch:106,Step:3000,Loss:0.7110297679901123\n",
      "epoch:106,Step:100,Accuracy:78.125%\n",
      "epoch:106,Step:200,Accuracy:81.25%\n",
      "epoch:106,Step:300,Accuracy:84.375%\n",
      "epoch:106,Step:400,Accuracy:84.375%\n",
      "epoch:106,Step:500,Accuracy:78.125%\n",
      "epoch:106,Step:600,Accuracy:96.875%\n",
      "\n",
      " epoch:106,Accuracy:85.91499999999999%\n",
      "\n",
      "epoch:  107\n",
      "epoch:107,Step:500,Loss:0.7970823049545288\n",
      "epoch:107,Step:1000,Loss:0.7093261480331421\n",
      "epoch:107,Step:1500,Loss:0.7491860389709473\n",
      "epoch:107,Step:2000,Loss:0.8953105211257935\n",
      "epoch:107,Step:2500,Loss:0.7669932246208191\n",
      "epoch:107,Step:3000,Loss:0.7270171046257019\n",
      "epoch:107,Step:100,Accuracy:81.25%\n",
      "epoch:107,Step:200,Accuracy:90.625%\n",
      "epoch:107,Step:300,Accuracy:81.25%\n",
      "epoch:107,Step:400,Accuracy:93.75%\n",
      "epoch:107,Step:500,Accuracy:85.9375%\n",
      "epoch:107,Step:600,Accuracy:75.0%\n",
      "\n",
      " epoch:107,Accuracy:86.8325%\n",
      "\n",
      "epoch:  108\n",
      "epoch:108,Step:500,Loss:0.6061679124832153\n",
      "epoch:108,Step:1000,Loss:0.7316818833351135\n",
      "epoch:108,Step:1500,Loss:0.6555542945861816\n",
      "epoch:108,Step:2000,Loss:0.7911333441734314\n",
      "epoch:108,Step:2500,Loss:0.7523417472839355\n",
      "epoch:108,Step:3000,Loss:0.774165689945221\n",
      "epoch:108,Step:100,Accuracy:81.25%\n",
      "epoch:108,Step:200,Accuracy:78.125%\n",
      "epoch:108,Step:300,Accuracy:89.0625%\n",
      "epoch:108,Step:400,Accuracy:87.5%\n",
      "epoch:108,Step:500,Accuracy:82.8125%\n",
      "epoch:108,Step:600,Accuracy:89.0625%\n",
      "\n",
      " epoch:108,Accuracy:86.52499999999999%\n",
      "\n",
      "epoch:  109\n",
      "epoch:109,Step:500,Loss:0.8177644610404968\n",
      "epoch:109,Step:1000,Loss:0.826566755771637\n",
      "epoch:109,Step:1500,Loss:0.7561990022659302\n",
      "epoch:109,Step:2000,Loss:0.751589298248291\n",
      "epoch:109,Step:2500,Loss:0.6921505331993103\n",
      "epoch:109,Step:3000,Loss:0.6988634467124939\n",
      "epoch:109,Step:100,Accuracy:67.1875%\n",
      "epoch:109,Step:200,Accuracy:84.375%\n",
      "epoch:109,Step:300,Accuracy:81.25%\n",
      "epoch:109,Step:400,Accuracy:93.75%\n",
      "epoch:109,Step:500,Accuracy:84.375%\n",
      "epoch:109,Step:600,Accuracy:92.1875%\n",
      "\n",
      " epoch:109,Accuracy:85.8475%\n",
      "\n",
      "epoch:  110\n",
      "epoch:110,Step:500,Loss:0.671572208404541\n",
      "epoch:110,Step:1000,Loss:0.711115300655365\n",
      "epoch:110,Step:1500,Loss:0.6694474220275879\n",
      "epoch:110,Step:2000,Loss:0.6876018643379211\n",
      "epoch:110,Step:2500,Loss:0.7022126317024231\n",
      "epoch:110,Step:3000,Loss:0.661169171333313\n",
      "epoch:110,Step:100,Accuracy:95.3125%\n",
      "epoch:110,Step:200,Accuracy:84.375%\n",
      "epoch:110,Step:300,Accuracy:93.75%\n",
      "epoch:110,Step:400,Accuracy:76.5625%\n",
      "epoch:110,Step:500,Accuracy:85.9375%\n",
      "epoch:110,Step:600,Accuracy:84.375%\n",
      "\n",
      " epoch:110,Accuracy:86.9875%\n",
      "\n",
      "epoch:  111\n",
      "epoch:111,Step:500,Loss:0.50029456615448\n",
      "epoch:111,Step:1000,Loss:0.6120525002479553\n",
      "epoch:111,Step:1500,Loss:0.798147439956665\n",
      "epoch:111,Step:2000,Loss:0.6566978096961975\n",
      "epoch:111,Step:2500,Loss:0.5541685819625854\n",
      "epoch:111,Step:3000,Loss:0.639604389667511\n",
      "epoch:111,Step:100,Accuracy:90.625%\n",
      "epoch:111,Step:200,Accuracy:90.625%\n",
      "epoch:111,Step:300,Accuracy:87.5%\n",
      "epoch:111,Step:400,Accuracy:90.625%\n",
      "epoch:111,Step:500,Accuracy:90.625%\n",
      "epoch:111,Step:600,Accuracy:87.5%\n",
      "\n",
      " epoch:111,Accuracy:87.1925%\n",
      "\n",
      "epoch:  112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m net\u001b[38;5;241m.\u001b[39mload(path_pkl)\n\u001b[0;32m----> 7\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m test_acc\u001b[38;5;241m.\u001b[39mappend(test(net,optimizer,test_data_loader,epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      9\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, optimizer, train_data_loader, epoch)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m     54\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mnet(inputs)\n\u001b[0;32m---> 55\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[43mpygm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermutation_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(loss)\n\u001b[1;32m     57\u001b[0m     train_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pygmtools/utils.py:1118\u001b[0m, in \u001b[0;36mpermutation_loss\u001b[0;34m(pred_dsmat, gt_perm, n1, n2, backend)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1115\u001b[0m         NOT_IMPLEMENTED_MSG\u001b[38;5;241m.\u001b[39mformat(backend)\n\u001b[1;32m   1116\u001b[0m     )\n\u001b[0;32m-> 1118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pygmtools/jittor_backend.py:1321\u001b[0m, in \u001b[0;36mpermutation_loss\u001b[0;34m(pred_dsmat, gt_perm, n1, n2)\u001b[0m\n\u001b[1;32m   1317\u001b[0m batch_num \u001b[38;5;241m=\u001b[39m pred_dsmat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1319\u001b[0m pred_dsmat \u001b[38;5;241m=\u001b[39m pred_dsmat\u001b[38;5;241m.\u001b[39mfloat32()\n\u001b[0;32m-> 1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jt\u001b[38;5;241m.\u001b[39mall((pred_dsmat \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (pred_dsmat \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_dsmat contains invalid numerical entries.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jt\u001b[38;5;241m.\u001b[39mall((gt_perm \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (gt_perm \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/jittor/__init__.py:2026\u001b[0m, in \u001b[0;36mto_bool\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(v):\n\u001b[1;32m   2025\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_int() \u001b[38;5;129;01mor\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_bool()\n\u001b[0;32m-> 2026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ori_bool(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < epochs:\n",
    "    checkpoint=jt.load(path_p)\n",
    "    if checkpoint['epoch'] > 0:\n",
    "        epoch=checkpoint['epoch']\n",
    "    print(\"epoch: \",epoch+1)\n",
    "    net.load(path_pkl)\n",
    "    train_loss.append(train(net,optimizer,train_data_loader,epoch+1))\n",
    "    test_acc.append(test(net,optimizer,test_data_loader,epoch+1))\n",
    "    scheduler.step()\n",
    "    epoch+=1\n",
    "    epoch_dict={\"epoch\":epoch}\n",
    "    jt.save(epoch_dict,path_p)\n",
    "    net.save(path_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_acc,'r',label=\"test_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss,'g',label=\"train_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jittor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
