{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0401 08:40:59.948649 00 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 0401 08:41:00.002757 00 compiler.py:956] Jittor(1.3.8.5) src: /Users/mumujun/anaconda3/envs/jittor/lib/python3.12/site-packages/jittor\u001b[m\n",
      "\u001b[38;5;2m[i 0401 08:41:00.023921 00 compiler.py:957] clang at /usr/bin/clang++(15.0.0)\u001b[m\n",
      "\u001b[38;5;2m[i 0401 08:41:00.024857 00 compiler.py:958] cache_path: /Users/mumujun/.cache/jittor/jt1.3.8/clang15.0.0/py3.12.2/macOS-13.6.3-ax71/AppleM1Pro/default\u001b[m\n",
      "\u001b[38;5;2m[i 0401 08:41:00.363540 00 __init__.py:227] Total mem: 16.00GB, using 5 procs for compiling.\u001b[m\n",
      "\u001b[38;5;2m[i 0401 08:41:00.495459 00 jit_compiler.cc:28] Load cc_path: /usr/bin/clang++\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "from jittor.optim import Optimizer\n",
    "from jittor import nn\n",
    "from jittor import Module\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from jittor.dataset.cifar import CIFAR10\n",
    "from jittor.dataset import DataLoader\n",
    "import jittor.transform as trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNnet(Module):\n",
    "    def __init__(self):\n",
    "        super(CNNnet,self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Conv(3,8,3,1,1),\n",
    "            nn.BatchNorm(8),\n",
    "            nn.Relu(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv(8,16,3,1,1),\n",
    "            nn.BatchNorm(16),\n",
    "            nn.Relu(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv(16,32,3,1,1),\n",
    "            nn.BatchNorm(32),\n",
    "            nn.Relu(),\n",
    "            nn.Conv(32,64,3,1,1),\n",
    "            nn.BatchNorm(64),\n",
    "            nn.Relu(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv(64,128,3,1,1),\n",
    "            nn.BatchNorm(128),\n",
    "            nn.Relu(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4,512),\n",
    "            nn.BatchNorm(512),\n",
    "            nn.Relu(),\n",
    "            nn.Linear(512,10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def execute(self,x):\n",
    "        x=self.model(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def train(net,train_data_loader,optimizer,total_train_step,epoch,compose):\n",
    "    net.train()\n",
    "    for data in train_data_loader:\n",
    "        imgs,targets=data\n",
    "        imgs,targets=imgs.float32(),targets.float32()\n",
    "        imgs=imgs.permute(0,3,1,2)\n",
    "        imgs=compose(imgs)\n",
    "        outputs=net(imgs)\n",
    "        loss=nn.cross_entropy_loss(outputs,targets)\n",
    "        optimizer.step(loss)\n",
    "        total_train_step+=1\n",
    "        if total_train_step %50 ==0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, total_train_step*32, len(train_data_loader),\n",
    "                    100. * total_train_step*32 / len(train_data_loader), loss.data[0]))\n",
    "\n",
    "def test(net,test_data_loader,epoch,compose):\n",
    "    net.eval()\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_data_loader):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs,targets=inputs.permute(0,3,1,2).float32(),targets.float32()\n",
    "        inputs=compose(inputs)\n",
    "        outputs = net(inputs)\n",
    "        pred = np.argmax(outputs.data, axis=1)\n",
    "        acc = np.sum(targets.data==pred)\n",
    "        total_acc += acc\n",
    "        total_num += batch_size\n",
    "        acc = acc / batch_size\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tAcc: {:.6f}'.format(epoch, \\\n",
    "                    batch_idx*batch_size, len(test_data_loader),100. * float(batch_idx)*batch_size / len(test_data_loader), acc))\n",
    "            format_text='Test Epoch: {} [{}/{} ({:.0f}%)]\\tAcc: {:.6f}\\n'.format(epoch, \\\n",
    "                    batch_idx*batch_size, len(test_data_loader),100. * float(batch_idx)*batch_size / len(test_data_loader), acc)\n",
    "    print ('Total test acc =', total_acc / total_num)\n",
    "    return total_acc/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "net=CNNnet()\n",
    "train_data=CIFAR10(train=True)\n",
    "test_data=CIFAR10(train=False)\n",
    "compose=trans.Compose([trans.ImageNormalize((0.485,0.456,0.406),(0.229, 0.224, 0.225))])\n",
    "train_data_loader=DataLoader(train_data,batch_size=32)\n",
    "test_data_loader=DataLoader(test_data,batch_size=32)\n",
    "learning_rate=1e-5\n",
    "optimizer=nn.SGD(net.parameters(),lr=learning_rate,momentum=0.9,weight_decay=5e-4)\n",
    "scheduler = jt.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "total_train_step=0\n",
    "total_test_step=0\n",
    "test_acc=[]\n",
    "epochs=1000\n",
    "epoch=0                        # change here after the kernel break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_dict={\"epoch_CNN\":epoch}\n",
    "jt.save(epoch_dict,\"epoch_CNN_jittor.p\")\n",
    "net.save(\"CNN_jittor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:1\n",
      "Train Epoch: 1 [1600/50000 (3%)]\tLoss: 2.300014\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.297851\n",
      "Train Epoch: 1 [4800/50000 (10%)]\tLoss: 2.305547\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.287434\n",
      "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2.295432\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 2.292613\n",
      "Train Epoch: 1 [11200/50000 (22%)]\tLoss: 2.294497\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.303261\n",
      "Train Epoch: 1 [14400/50000 (29%)]\tLoss: 2.296516\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.312906\n",
      "Train Epoch: 1 [17600/50000 (35%)]\tLoss: 2.284385\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.297136\n",
      "Train Epoch: 1 [20800/50000 (42%)]\tLoss: 2.288284\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 2.295592\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 2.287389\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.291272\n",
      "Train Epoch: 1 [27200/50000 (54%)]\tLoss: 2.271903\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 2.290718\n",
      "Train Epoch: 1 [30400/50000 (61%)]\tLoss: 2.306842\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.293559\n",
      "Train Epoch: 1 [33600/50000 (67%)]\tLoss: 2.302564\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 2.278021\n",
      "Train Epoch: 1 [36800/50000 (74%)]\tLoss: 2.294721\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.284885\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 2.299648\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 2.293645\n",
      "Train Epoch: 1 [43200/50000 (86%)]\tLoss: 2.278424\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.306003\n",
      "Train Epoch: 1 [46400/50000 (93%)]\tLoss: 2.305164\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 2.293763\n",
      "Train Epoch: 1 [49600/50000 (99%)]\tLoss: 2.288251\n",
      "Test Epoch: 1 [0/10000 (0%)]\tAcc: 0.187500\n",
      "Test Epoch: 1 [320/10000 (3%)]\tAcc: 0.125000\n",
      "Test Epoch: 1 [640/10000 (6%)]\tAcc: 0.093750\n",
      "Test Epoch: 1 [960/10000 (10%)]\tAcc: 0.125000\n",
      "Test Epoch: 1 [1280/10000 (13%)]\tAcc: 0.093750\n",
      "Test Epoch: 1 [1600/10000 (16%)]\tAcc: 0.218750\n",
      "Test Epoch: 1 [1920/10000 (19%)]\tAcc: 0.156250\n",
      "Test Epoch: 1 [2240/10000 (22%)]\tAcc: 0.000000\n",
      "Test Epoch: 1 [2560/10000 (26%)]\tAcc: 0.062500\n",
      "Test Epoch: 1 [2880/10000 (29%)]\tAcc: 0.125000\n",
      "Test Epoch: 1 [3200/10000 (32%)]\tAcc: 0.218750\n",
      "Test Epoch: 1 [3520/10000 (35%)]\tAcc: 0.218750\n",
      "Test Epoch: 1 [3840/10000 (38%)]\tAcc: 0.093750\n",
      "Test Epoch: 1 [4160/10000 (42%)]\tAcc: 0.062500\n",
      "Test Epoch: 1 [4480/10000 (45%)]\tAcc: 0.281250\n",
      "Test Epoch: 1 [4800/10000 (48%)]\tAcc: 0.093750\n",
      "Test Epoch: 1 [5120/10000 (51%)]\tAcc: 0.250000\n",
      "Test Epoch: 1 [5440/10000 (54%)]\tAcc: 0.062500\n",
      "Test Epoch: 1 [5760/10000 (58%)]\tAcc: 0.250000\n",
      "Test Epoch: 1 [6080/10000 (61%)]\tAcc: 0.218750\n",
      "Test Epoch: 1 [6400/10000 (64%)]\tAcc: 0.125000\n",
      "Test Epoch: 1 [6720/10000 (67%)]\tAcc: 0.218750\n",
      "Test Epoch: 1 [7040/10000 (70%)]\tAcc: 0.218750\n",
      "Test Epoch: 1 [7360/10000 (74%)]\tAcc: 0.125000\n",
      "Test Epoch: 1 [7680/10000 (77%)]\tAcc: 0.125000\n",
      "Test Epoch: 1 [8000/10000 (80%)]\tAcc: 0.281250\n",
      "Test Epoch: 1 [8320/10000 (83%)]\tAcc: 0.093750\n",
      "Test Epoch: 1 [8640/10000 (86%)]\tAcc: 0.125000\n",
      "Test Epoch: 1 [8960/10000 (90%)]\tAcc: 0.218750\n",
      "Test Epoch: 1 [9280/10000 (93%)]\tAcc: 0.125000\n",
      "Test Epoch: 1 [9600/10000 (96%)]\tAcc: 0.281250\n",
      "Test Epoch: 1 [9920/10000 (99%)]\tAcc: 0.093750\n",
      "Total test acc = 0.1635\n",
      "epochs:2\n",
      "Train Epoch: 2 [1600/50000 (3%)]\tLoss: 2.289236\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 2.290523\n",
      "Train Epoch: 2 [4800/50000 (10%)]\tLoss: 2.298097\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 2.279879\n",
      "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 2.285969\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 2.282767\n",
      "Train Epoch: 2 [11200/50000 (22%)]\tLoss: 2.287117\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.298023\n",
      "Train Epoch: 2 [14400/50000 (29%)]\tLoss: 2.289412\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 2.308489\n",
      "Train Epoch: 2 [17600/50000 (35%)]\tLoss: 2.274010\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 2.288889\n",
      "Train Epoch: 2 [20800/50000 (42%)]\tLoss: 2.276616\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 2.288058\n",
      "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 2.279510\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.285372\n",
      "Train Epoch: 2 [27200/50000 (54%)]\tLoss: 2.260857\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 2.279408\n",
      "Train Epoch: 2 [30400/50000 (61%)]\tLoss: 2.298975\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.290668\n",
      "Train Epoch: 2 [33600/50000 (67%)]\tLoss: 2.295395\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 2.266166\n",
      "Train Epoch: 2 [36800/50000 (74%)]\tLoss: 2.286957\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 2.280020\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 2.293216\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 2.285667\n",
      "Train Epoch: 2 [43200/50000 (86%)]\tLoss: 2.268455\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 2.300157\n",
      "Train Epoch: 2 [46400/50000 (93%)]\tLoss: 2.299929\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 2.287330\n",
      "Train Epoch: 2 [49600/50000 (99%)]\tLoss: 2.280047\n",
      "Test Epoch: 2 [0/10000 (0%)]\tAcc: 0.218750\n",
      "Test Epoch: 2 [320/10000 (3%)]\tAcc: 0.125000\n",
      "Test Epoch: 2 [640/10000 (6%)]\tAcc: 0.187500\n",
      "Test Epoch: 2 [960/10000 (10%)]\tAcc: 0.187500\n",
      "Test Epoch: 2 [1280/10000 (13%)]\tAcc: 0.093750\n",
      "Test Epoch: 2 [1600/10000 (16%)]\tAcc: 0.218750\n",
      "Test Epoch: 2 [1920/10000 (19%)]\tAcc: 0.156250\n",
      "Test Epoch: 2 [2240/10000 (22%)]\tAcc: 0.031250\n",
      "Test Epoch: 2 [2560/10000 (26%)]\tAcc: 0.093750\n",
      "Test Epoch: 2 [2880/10000 (29%)]\tAcc: 0.187500\n",
      "Test Epoch: 2 [3200/10000 (32%)]\tAcc: 0.218750\n",
      "Test Epoch: 2 [3520/10000 (35%)]\tAcc: 0.250000\n",
      "Test Epoch: 2 [3840/10000 (38%)]\tAcc: 0.093750\n",
      "Test Epoch: 2 [4160/10000 (42%)]\tAcc: 0.062500\n",
      "Test Epoch: 2 [4480/10000 (45%)]\tAcc: 0.312500\n",
      "Test Epoch: 2 [4800/10000 (48%)]\tAcc: 0.093750\n",
      "Test Epoch: 2 [5120/10000 (51%)]\tAcc: 0.312500\n",
      "Test Epoch: 2 [5440/10000 (54%)]\tAcc: 0.125000\n",
      "Test Epoch: 2 [5760/10000 (58%)]\tAcc: 0.250000\n",
      "Test Epoch: 2 [6080/10000 (61%)]\tAcc: 0.312500\n",
      "Test Epoch: 2 [6400/10000 (64%)]\tAcc: 0.125000\n",
      "Test Epoch: 2 [6720/10000 (67%)]\tAcc: 0.250000\n",
      "Test Epoch: 2 [7040/10000 (70%)]\tAcc: 0.218750\n",
      "Test Epoch: 2 [7360/10000 (74%)]\tAcc: 0.125000\n",
      "Test Epoch: 2 [7680/10000 (77%)]\tAcc: 0.156250\n",
      "Test Epoch: 2 [8000/10000 (80%)]\tAcc: 0.281250\n",
      "Test Epoch: 2 [8320/10000 (83%)]\tAcc: 0.187500\n",
      "Test Epoch: 2 [8640/10000 (86%)]\tAcc: 0.187500\n",
      "Test Epoch: 2 [8960/10000 (90%)]\tAcc: 0.187500\n",
      "Test Epoch: 2 [9280/10000 (93%)]\tAcc: 0.187500\n",
      "Test Epoch: 2 [9600/10000 (96%)]\tAcc: 0.312500\n",
      "Test Epoch: 2 [9920/10000 (99%)]\tAcc: 0.125000\n",
      "Total test acc = 0.1911\n",
      "epochs:2\n",
      "Train Epoch: 2 [1600/50000 (3%)]\tLoss: 2.276901\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 2.282359\n",
      "Train Epoch: 2 [4800/50000 (10%)]\tLoss: 2.288839\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 2.270903\n",
      "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 2.274552\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 2.272426\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m net\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN_CIFAR10.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtotal_train_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcompose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m test_acc\u001b[38;5;241m.\u001b[39mappend(test(net, test_data_loader, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,compose))\n\u001b[1;32m      9\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_data_loader, optimizer, total_train_step, epoch, compose)\u001b[0m\n\u001b[1;32m     43\u001b[0m outputs\u001b[38;5;241m=\u001b[39mnet(imgs)\n\u001b[1;32m     44\u001b[0m loss\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(outputs,targets)\n\u001b[0;32m---> 45\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m total_train_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_train_step \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jittor/lib/python3.12/site-packages/jittor/optim.py:305\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     jt\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mnode_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;66;03m# get arguments from each param_groups\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jittor/lib/python3.12/site-packages/jittor/optim.py:220\u001b[0m, in \u001b[0;36mOptimizer.pre_step\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" something should be done before step, such as calc gradients, mpi sync, and so on.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03mExample::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m            self.post_step()\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m jt\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mnode_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/jittor/lib/python3.12/site-packages/jittor/optim.py:170\u001b[0m, in \u001b[0;36mOptimizer.backward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m    167\u001b[0m             params_has_grad\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# sync prev params\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[43mjt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_has_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# get gradient\u001b[39;00m\n\u001b[1;32m    173\u001b[0m grads \u001b[38;5;241m=\u001b[39m jt\u001b[38;5;241m.\u001b[39mgrad(loss, params_has_grad, retain_graph)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < epochs:\n",
    "    checkpoint=jt.load(\"epoch_CNN_jittor.p\")\n",
    "    if checkpoint['epoch_CNN'] > 0:\n",
    "        epoch=checkpoint['epoch_CNN']\n",
    "    net.load(\"CNN_CIFAR10.pkl\")\n",
    "    print(\"epochs:{}\".format(epoch+1))\n",
    "    train(net,train_data_loader,optimizer,total_train_step,epoch+1,compose)\n",
    "    test_acc.append(test(net, test_data_loader, epoch+1,compose))\n",
    "    scheduler.step()\n",
    "    epoch+=1\n",
    "    net.save(\"CNN_CIFAR10.pkl\")\n",
    "    epoch_dict={\"epoch_CNN\":epoch}\n",
    "    jt.save(epoch_dict,\"epoch_CNN_jittor.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_acc,'r',label=\"test_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jittor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
